Introduction The application of mature imaging hardware, software, and standards from earth‐sensing, astronomical, medical, national security, and consumer applications is decreasing risk in collecting and managing digital images and data in libraries, archives, and museums. Early navigators, astronomers, and surveyors used both established and pioneering scientific techniques during early maritime and terrestrial exploration of new worlds, stars, and planets. Centuries later, advanced imaging techniques provided mankind with the first opportunity to clearly see and survey vast regions of earth and space. Just as mankind is now able to survey and study the planet and universe with a range of digital imaging technologies, imaging and preservation scientists are turning the same digital imaging techniques to the study of historic and cultural treasures. Imaging science has expanded the historical record of works of art, historic documents, and national treasures. Digital imaging in discrete spectral bands to support cultural heritage institutions has progressed to the point where it is now a key laboratory tool. There are very real challenges and consequences with the application of new technologies to libraries, archives, and museums. All three have significant collections of fragile items, and while digital imaging increases access, the adoption of techniques for analysis requires a high level of accountability to develop a truly nondestructive, noninvasive, and enduring methodology. The successful integration of spectral imaging for cultural heritage is underway simultaneously in a number of countries throughout the world and in collaboration. As C. Fischer and I. Kakoulli noted in 2006: “Primarily used for scientific investigations of paintings, it has also been successfully applied to the study of documents, the evaluation of conservation treatments and digital imaging for documentation” [1, p. 3]. While many developments depend upon advances in technology, others reflect the ability to build upon the efforts of other researchers [2, 3]. The analysis for transcription and translation of deteriorated ancient texts requires collaboration between conservators and scholars for effective translation, while accurate determination of parameters associated with substrates and media without sampling is critical to the assessment and preservation of many international items of cultural heritage [4]. The successful transfer of spectral imaging to cultural heritage institutions depends on how effectively the new technology can be integrated into work processes, study methods, and current systems. The real challenge for successful application of new technologies with information technology and data management is the integration of people and processes with the technology. The convergence of advanced digital imaging and lighting systems with data standards and effective management has improved access to and allowed efficient handling of large volumes of data for libraries, archives, and museums. The Implementation of a Digital Imaging System at the Library of Congress Like other libraries and museums, the Library of Congress has worked with a range of basic spectral imaging systems to study objects in the collection, including ultraviolet (UV) and infrared (IR) light and a broadband low‐resolution multispectral system with two broad, distinct bands in the UV, one each in red, green, and blue regions of the visible spectrum, and three bands in the IR. These lighting systems were used to assess UV florescence of various compounds and to increase visibility of background or low‐visibility information, mainly for conservation assessment and treatment decisions. Researchers—scientific, conservation, and scholarly—in cultural heritage organizations worldwide often utilize UV and IR lighting to enhance features not clearly visible to the naked eye. Advances over the past decade have imparted the capability to utilize a robust spectral imaging system that provides large format, high‐quality images and standardized data output with commercial off‐the‐shelf components, including integrated collection and data storage software. The reduced costs and increased capabilities take spectral imaging from an exotic activity to a nondestructive working tool. Building on the pioneering research and development efforts in systems design, spectral illumination and imaging, and data management in the Archimedes Palimpsest Program [5], the Library contracted for a hyperspectral imaging study of a key object in the collection to assess the capability and versatility of a fully integrated hyperspectral imaging system. Hyperspectral imaging—taking a series of digital images in nanometer bandwidths of the visible and nonvisible spectrum, from ultraviolet through visible to infrared, often in contiguous wavelengths—can detect variances in components or material composition of the object being imaged at any wavelength or combination of wavelengths. The resulting images may be digitally combined with or subtracted from each other to form images for scientific or scholarly analysis. These hyperspectral images contain a wealth of information but require significant interpretation to process and analyze the collected data. Advances in technology have greatly impacted the field of spectral imaging, which have been further enhanced through improvements in the process of image capture techniques and specific postimage processing (see fig. 1), offering libraries, as well as museums and archives, a useful and efficient tool for the study of cultural objects [6]. Fig. 1.— Development of Library of Congress imaging system, hyperspectral imaging Hyperspectral imaging of cultural objects requires well‐developed metadata to capture object identification, scientific, and spatial information to ensure long‐term access, management, and use of the objects. The special nature of hyperspectral imaging is the creation of multiple images of the same cultural object that are related to one another along multiple axes, among them content identification, spatial registration, and illumination parameters. This convergence of scientific techniques and the creation of large digital data sets with library and museum objects requires a metadata standard that encompasses multiple domains and goes beyond the provisions of any single existing metadata standard. Data acquisition and format must anticipate the needs of libraries, archives, scholars, scientists, and other potential users of the data. As both an international library and the de facto national library of the United States, the Library of Congress is a major repository and archive of knowledge and creativity in the United States. Its extensive collection includes three types of media that reflect the chronology of their technological introduction: historic and ancient traditional materials (e.g., stone, papyrus, parchment, and paper), more modern nineteenth‐ and twentieth‐century analog audiovisual materials (e.g., photographic negatives, magnetic tape, motion pictures), and contemporary digital materials (optical discs, storage devices, and hardware for machine readability). It is the host of an increasing number of permanent and temporary exhibits that allows public access to a rich array of significant items of cultural heritage. The hyperspectral imaging study was in part related to the latter role, since the Waldseemüller 1507 “World Map” (the first map to use the term “America”) must always be available for public viewing as required by the terms of its purchase by the Library of Congress. As outreach, a permanent exhibit was created in the Jefferson building to house this map as part of the exhibit Exploring the Early Americas [7]. As a response to the requirements of a permanent exhibit, the map was housed in an environmentally controlled, aluminum encasement in an anoxic environment to minimize the potential degradation caused by the presence of oxygen. Maintaining control of relative humidity and oxygen demanded a permanent hermetic seal that restricted access to the actual map. The high‐resolution hyperspectral imaging was undertaken to provide baseline information about the map and its condition, as well as to create an extensive library of images that could be used for research into printing and construction techniques; scholarly interpretations; to enhance lost information; and to characterize colorants, inks, and other media and substrates present. This challenge was exacerbated with the requirements to complete the work within a limited time frame prior to the installation of the exhibit in December 2007. In November 2007, the Library’s Preservation, Research and Testing Division (PRTD) conducted the hyperspectral digital imaging studies of the Waldseemüller 1507 paper map and other maps from the Geography and Maps Division, including the Carta Marina paper map and the parchment “Map with Ship.” A joint imaging team imaged the twelve sheets of 18 × 24–inch paper on which the Waldseemüller map was printed and other items. Regular meetings were held with Library of Congress personnel before, during, and after the imaging process to define the Library’s study requirements and optimize the imaging studies to meet these requirements. In a convergence of library and museum requirements, the Library of Congress chose to utilize the same illumination system pioneered during the Archimedes Palimpsest Program at the Walters Art Museum, in order to minimize project risk. Illumination was provided by the latest Equipoise Imaging EurekaLight LED illuminators and SideLong raking light illuminators (see fig. 2). Raking illumination was provided in two spectral bands (470 and 910 nanometers) to illuminate from either side of the object at a low, oblique angle. While this technique is commonly used to reveal impressions and topographic features in the paper or parchment, only the current imaging allows the potential to combine this information with other spectral regions of interest. Cool‐light LED sources provided illumination at 12 specific wavelengths from two illumination panels: UV (365 nanometers), Visible (445, 470, 505, 530, 570, 617, 625 nanometers), and IR (700, 735, 780, 870 nanometers). Fig. 2.— Imaging setup with LED light panel The glass camera lens did not allow imaging of reflected UV light but allowed the capture of UV fluorescence from the excitation of elements in the object. Use of UV light was limited to the specific image capture to minimize damage to the object. Reflected and fluorescing light was captured in individual registered images from each spectral band, yielding an image cube of multiple images for processing. Registration of the images—aligning all features on an image so that they match perfectly—ensures processing yields that are sharp and crisp when overlaid or combined. To minimize the impact of light on fragile items, the length of exposure was carefully controlled and the image histogram maximized individually for each exposure. The cool‐light LED panels reduced another concern with the imaging of fragile artifacts: the increase in temperature from lights that reduce the relative humidity or moisture content of the materials, causing dimensional changes that ultimately lead to microfractures and points of weakness in the organic materials. One of the concerns with historic documents is minimizing the effects of photodegradation, since photography and imaging systems often expose the item to high levels of light required to obtain a high‐quality image. The graph (see fig. 3) clearly illustrates the reduction of light exposure from the normal room lights (which are usually enhanced by floodlights to increase this up to four times) at 350 lux to approximately 3.5 lux. To make sense of these numbers, it should be noted that recommended lighting levels for display of light‐sensitive materials are not to exceed 50 lux, with lower levels preferred for very fragile, light‐sensitive items and long‐term displays. All light damage is cumulative, so lower levels of light employed in analyses as well as in exhibition greatly increase the lifetime of the item and its component materials. Fig. 3.— Comparison of room lighting with the hyperspectral imaging lighting system The LED illumination system was integrated with a monochrome camera to minimize problems encountered with the Bayer pattern of small colored filters used on color cameras. Color cameras that collect red‐green‐blue (RGB) images require the use of a color filter array (a Bayer filter) that uses twice as many green elements as red or blue to mimic the peak response of the human eye to green wavelengths. With this array, each pixel is then filtered to record only one of three colors; therefore, only one‐third of the color data are collected at each pixel. To obtain a full‐color image, demosaicing algorithms are required to interpolate the full range of color for each point, thereby resulting in varying quality final images. Utilizing a monochrome camera allows all spectral information to be captured by the camera from the object being imaged. By offering imaging in distinct spectral bands or combinations of spectral bands without filters between the camera and the image, this system does not limit the light reaching the camera elements or change registration of the images. Monochrome images offered the following advantages for artifact studies: (1) no residual artifacts are left from the color sensor’s Bayer pattern filtration; (2) without the Bayer array filtering light, the monochrome camera utilizes over twice the light from the scene, resulting in over twice the signal‐to‐noise ratio and less light on the cultural object; (3) the need for postprocessing and the anti‐aliasing filter common to Bayer pattern digital single lens reflex devices is eliminated. The imaging team used a MegaVision digital imaging system that provided a digital image size of about 40 megapixels, offering good resolution and file size for efficient processing and analysis. It utilized an integrated MegaVision Monochrome E6 39 megapixel back and a camera with a Schneider Apo Macro 120 millimeter f5.6 lens that provided a dynamic range of 12 bits per channel with a CCD Array of 7216 × 5412 pixels and 6.8 micron pixel size. Each sheet of the map was imaged verso and recto at each of the twelve wavelengths to obtain a total of twenty‐four macro images per sheet, as well as four raking illumination images (verso and recto). The imaging team produced 300 dots per inch (dpi) 8‐bit TIFF images for all illuminations with associated metadata for each image. Higher resolution 600 dpi images were also taken and stitched together to create twenty‐four stitched, full‐page, high‐resolution images per sheet. The imaging also captured spatial information for each image, an approach built on concepts established during the Archimedes Palimpsest project, in what was dubbed “scriptospatial imaging.” In the same manner and following the same standards, the Library of Congress is establishing the ability to spatially link data derived from the images to specific points on the images themselves. This includes preservation and conservation annotations and data about specific regions on the document, visible and “hidden” text, and data from other studies. Prior to imaging the Waldseemüller map at the Library of Congress, the data and program manager established a coordinate system for the images. With this standardized system, standard geospatial software can be used to graphically link the images and text for data integration and access. To image the Waldseemüller map at 600 dpi resolution, the imaging team had to take six overlapping images of each 18 × 24–inch sheet of the map and stitch them together to create a single, large image (see fig. 4). This was accomplished by positioning the map on a computer controlled x,y table that translated on the x and y axes to precisely move each sheet through the six positions. The sixteen spectral and raking images were captured at each position along with the spatial coordinates for that position. Following the imaging and image processing, a geospatial scientist digitally stitched the 600 dpi images together to create single images of each sheet. Fig. 4.— Stitching process for overlapping images of large documents Following the initial imaging session, the imaging team met with the Library of Congress principals to identify the major key areas of interest for preservation and scholarly studies. This included experimental image processing to provide insights into the details that could be provided by various types of processing. The imaging team then took the individual spectral images for each map sheet or sheet section and processed them to reveal features of interest to the Library of Congress. The processed images were used for a range of library studies: identifying the response of specific characteristics of the map, including printers' inks, pigments, iron gall, effects of treatments, and other aspects as identified by conservators/scientists prior to the imaging session, and using the multispectral raking incidence illumination to enhance the visibility of any “topographic” features relating to the woodblock printing and any other mechanical contact. In October 2008, PRTD conducted studies of L’Enfant’s 1791 “Plan of Washington D.C.” and some daguerreotypes with an enhanced MegaVision‐Equipoise Imaging system. This was similar to the imaging system used in the earlier studies, with the addition of an additional 1,050 nanometers spectral band in the infrared range, as well as enhanced software to allow additional control of the camera and illumination. Building on these studies and imaging systems, the Library is now integrating digital imaging systems in the PRTD Optical Laboratory capable of producing numerous large images. This new technology will be used for preservation studies to support cultural heritage institutions worldwide that have already expressed interest in furthering their capacities for nondestructive enhanced spectral imaging. Digital imaging studies of parchment and paper, colorants and inks, daguerreotypes, and encasement materials are currently underway. A data management system to integrate, access, store, retrieve, and process images is critical to developing this capacity as a standard analytical tool for libraries and museums. Image Processing The composite of the twelve discrete spectral imaging wavelengths and four raking combinations creates the image cube (see fig. 5). This illustrates the spectral response of the colorant utilized to create the red grid lines on the Waldseemüller map (sheets 6 and 7), where the images captured in the data cube show the change in response relative to the specific wavelength illuminations. Once the spectral regions that best reveal a given feature are identified and utilized in the image processing, they can then be used to enhance this feature for scholarly and preservation research. Fig. 5.— Spectral imaging cube for red grid line feature For some scholarly, scientific, and preservation studies, digitally imaging an object in specific spectra isolated with either filters or narrow bands of illumination will suffice in revealing underlying areas of interest [8]. However, for many studies, advanced digital processing of images from multiple spectral bands is required to improve visibility of regions of interest by suppressing some features and enhancing others, as well as ascribing certain colors to certain features for better visibility [9, 10]. The collection of multiple registered images across a range of spectral bands offers tremendous opportunities for digital processing of the images to reveal information not readily discernible in the original images. The true added value of imaging in multiple spectral bands lies in the ability to combine images and enhance features, details, and underlying information that are not visible to the naked eye or in a single spectral band. This is accomplished by mathematically combining and suppressing the output from multiple spectral bands to combine the spectral signatures of specific wavelengths of light that offer an enhanced image of specific elements of the image. One of the leading processing techniques used to reveal areas of interest is pseudocolor processing, which was used effectively in the Archimedes Palimpsest Program to reveal the undertext of Archimedes' work and to separate or suppress the overtext from the Greek book of prayer. At the Library of Congress the imaging team created pseudocolor images using advanced processing techniques to map specific components and areas of interest on the Waldseemüller map and other maps. Without going into the advanced mathematics, suffice it to note that the imaging scientists developed and refined mathematical algorithms that could be applied to each 300 dpi or 600 dpi image cube to yield processed images, including pseudocolor and “embossed” images, the latter presenting a representation of what the original woodblock would have looked like. Utilizing differences in the spectral signatures of the reflected light, the imaging scientists processed the images to highlight different classes of response and produced images with enhanced visibility of areas of interest [11]. This included enhanced pseudocolor images of red grid lines on two of the Waldseemüller map sheets (see fig. 6) and various annotations and historical information of the maps. The scientists also processed multiple spectral bands to reveal printing and construction techniques on the Waldseemüller map in “embossed” images (see fig. 7). Fig. 6.— Series of imaged sheet with spectral changes in red grid line response Fig. 7.— Comparison of “embossed” woodblock and pseudocolor processing Additional 1200 dpi high‐resolution images were taken for processing of specific areas of interest to reveal hidden text. The team also captured transmitted light images to develop a technique for capturing watermarks from the sheets (see fig. 8). This is required for analysis of papers by illuminating not only the watermark itself to trace the paper to a specific location of its production but also by revealing the paper‘s construction. Prior to about 1800, all paper was made by hand, and laid paper was made on a mold consisting of a frame that had a fine mesh of wires that ran parallel to the long sides of the mold. These were intersected by thicker wires—the chain lines—that ran parallel to the shorter sides of the mold. After the water with suspended paper fibers was evenly distributed over the wire mesh, the mold would then be inverted, creating two distinct sides to the paper: the mold side carrying the impression of the wire and chain lines. The identification and study of watermarks is critical to analysis and establishment of the provenance of library and archive documents, and this development is being used to establish a technique that meets the current safety requirements of the Library of Congress. Fig. 8.— Transmitted light image of watermark, paper chain, and laid lines The hyperspectral imaging of L’Enfant’s 1791 “Plan of Washington D.C.” clearly illustrates the convergence of the utilization of this new technology, not only for libraries but also to address the needs of archives and museums to enhance and reveal lost information. Figure 9 shows the comparison of what is currently visible to the naked eye and the same section of the map as revealed under infrared. The neatly penciled street grid and other features stand out in stark contrast. An understanding of treatments or damage that occurred in previous centuries begins to be revealed through other spectral combinations (see fig. 10), which is critical for conservation professionals’ work with the documents and artifacts to extend their lifetimes and prevent further deterioration relating to specific chemical reactions that may be occurring. Fig. 9.— Visible and infrared images of L’Enfant 1791 “Plan of Washington D.C.” Fig. 10.— L’Enfant “Plan of Washington D.C.” combined wavelengths: 450 nanometers, 535 nanometers, 735 nanometers (blue, green, red), and fine detail of “the White House.” The capacity to reveal hidden and lost text is of immense value to cultural heritage institutions, providing the ability to confirm provenance, recover lost information, and allow researchers to confirm or disprove theories of the techniques of artists, underpaintings, overwritten text, and nondestructive identification of inks and colorants (see fig. 11). Researchers are continually searching for nondestructive methods of analysis to provide increasing levels of information, and this technique ably demonstrates the challenges faced in the convergence of acquiring information, making it accessible and managing the large volumes of information generated. Fig. 11.— Hidden text on “Carta Marina” 1516 map—pastedown on sheet 9; left: visible light (530 nanometers) right: IR (910 nanometers). Further work with principle component analysis and with topographic and other processing techniques available in open‐source software are currently being pursued for image processing. Further development work on other artifact formats, such as early photographic processes, are proving useful in documenting condition and identifying deterioration mechanisms in daguerreotypes, with the utilization of special imaging procedures providing additional protection for fragile, ungilded images. Figure 12 illustrates the clear images obtained at 638 nanometers of a daguerreotype not fully visible to the naked eye, as well as other deterioration processes apparent on the plate at other spectral wavelengths. Fig. 12.— Daguerreotype image features revealed at different spectral wavelengths Data Management Installation of an effective imaging system in a library, as well as archives and museums, requires not just the installation of good imaging equipment, training, and efficient operation. The Library has learned through experience and work with other imaging programs that simply imaging objects does not address the key issues in providing digital products for users. A full imaging system that offers useful products for a major library must include robust data management capabilities and information technology infrastructure to ensure that users and systems are not swamped with data and can readily access needed images. The study, preservation, storage, and display of historic paper, parchment, and other artifacts requires the integration of large amounts of data to provide information of value to key users in the Library of Congress. Retrieving data and information about parchment, paper, and other objects; preserving the data for current and future use; and contributing to the creation of knowledge from the retrieved information and digital data requires effective systems development that includes the technology, work processes, and trained personnel. There has been a shift over the past few decades from cultural heritage organizations as repositories of objects to repositories of knowledge [12], so there is a real need to understand the potential impact of these new imaging capabilities on libraries, archives, and museums, including the value added for researchers, as well as public outreach and keeping all systems current with technology and standards. The Library has implemented a range of integration and management activities to effectively use these advanced imaging systems based on current and future requirements. This includes identification of user needs for data and information at the Library, identification and integration of the technologies and work processes needed to address these needs, and specification of the data and metadata standards and requirements to ensure data portability and access. Members of the preservation, technical, scholarly, and scientific disciplines all contribute to the collection and study of digital information on a range of parchment, paper, and other objects in the Library. Experience in the Library has shown that the following developments must be addressed in support of future imaging studies to collect and fully exploit imaging digital data for the preservation of cultural objects and with a standard imaging analysis tool: 1. Standardized methods and procedures for hyperspectral imaging and collection of metadata elements for cultural objects. This includes digital imaging studies with the MegaVision‐Equipoise system as well as data from other instruments. Appropriate standardized metadata are being defined to ensure that they meet the Library’s standards. Collection methodologies are being established with optimized equipment configurations and aligned with developments in digital data storage and retrieval standards. This information is being integrated with technical and processing information and documented for Library reference and use. 2. Image processing and data management techniques that offer the best information to address the Library studies utilizing hyperspectral imaging. Experts are working with Library personnel in the development of full life cycle, digital image processing capabilities. This includes addressing the application and preservation of metadata and data management techniques, following accepted standard practices. 3. Advanced digital image processing techniques with standard commercial off‐the‐shelf software (e.g., Photoshop and ImageJ) for digitally enhancing hyperspectral images [13]. This is in response to processing challenges for accessing software capable of carrying out the imaging manipulations required for preservation research and the training required to develop these capabilities for personnel acquiring skills in advanced image processing. Metadata The Library is integrating broad imaging and metadata standards, work processes, and technical systems with those in other libraries, archives, and museums. With collaboration across the library community, advanced digital imaging and information systems make major contributions to artifact studies and the information needs of a range of cultural heritage organizations, including museums and archives. For the Waldseemüller imaging project, the Library employed the metadata scheme developed for the Archimedes Palimpsest project. The Archimedes Palimpsest Metadata Standard (APMS) [14] is based on Dublin Core and the Content Standard for Digital Geospatial Metadata [15]. It also incorporates extensions to the standard where they are needed to complete the scientific description of an image’s capture or creation. The standard is derived from existing standards including the Dublin Core Metadata Element Set 1.1 [16] and the Dublin Core Metadata Initiative Terms [17]. There are six types of information defined in the standard: (1) identification, (2) spatial data reference, (3) imaging and spectral data reference, (4) data type, (5) data content, and (6) metadata reference. The Archimedes Palimpsest Metadata Standard uses the Dublin Core elements and terms to identify the image. The elements based on the geospatial standard, those under items 2, 3, and 4, also describe the image itself. The content of the imaged object—the manuscript page or map—is provided in item 5. The standard holds, first, that its focus is the description of the digital object—from the Metadata Standard’s introduction: “1. Objectives. This standard is intended to provide a common set of terminology and definitions for the archival documentation of digital multispectral imagery data” [14]. As such, the Dublin Core data describes the digital object: each image is assigned its own identifier (the Dublin Core dc:identifier); the creators of the object (dc:creator) are the imagers, not Archimedes, Martin Waldseemüller, or Pierre Charles L’Enfant; and the keywords (dc:subject) describe the image. In this last case, the subject entries, a value will be “Map image” and not “Map.” The distinction is a subtle one, perhaps, but it is critical. Second, the APMS adds elements that relate scientific parameters of the imaging, apply geospatial imaging data elements to document imaging, and add content elements that allow multiple images of the same object to be related to one another. Sample element descriptions are shown in the appendixes. Appendixes A and B show sample spatial metadata for an image of the Waldseemüller map, with the metadata based on both Dublin Core [16] and the Content Standard for Digital Geospatial Metadata [18]. The standard defines the spatial reference in terms of the dimensions of the object. The pixel dimensions of the image can be determined by multiplying the resolution by the lower‐right x and y coordinates. The folio position and count elements locate the image within a grid for high‐resolution images of objects too large to image in a single shot and coordinate one image with other images of the same tile of the same object. Appendix C shows the metadata fields that implement the Imaging and spectral data information elements include the wavelength of the light or lights used, their wattage, number, and angles of incidence at the four azimuthal points around the imaged object. Appendix D shows a fragment of APMS section 5, content description information. The content description information defines, through the foliation scheme and foliation numbers, the identity of the image subject and relates images of a single subject together. The APMS data elements provide other critical metadata, but these examples demonstrate how the multiple elements work together to coordinate select images from a larger data set based on imaging, spatial, and content parameters. The metadata standard meets the specific needs of metadata for hyperspectral imaging of paper and parchment documents. Key to identifying related images, discovering their spectral and spatial characteristics, and performing computational work using them is a standard that describes spatial, scientific, and content information as the APMS does. For the Waldseemüller project, the metadata are stored in each image file’s ImageDescription TIFF header. Through the ID_File_Name metadata element, the metadata are connected to the image. As it happens, the image file names include descriptions according to a standardized naming convention. File names for 300 dpi images of the recto side of sheet 8 of the Waldseemüller map, for example, include 300_8_r_0_365_pack8.tif, 300_8_r_0_445_pack8.tif, 300_8_r_0_470_pack8.tif, 300_8_r_0_embossed.tif, 300_8_r_0_rkLft445_pack8.tif, and so forth. The names provide adequate metadata to distinguish multiple image of a sheet side from the others. The metadata are provided merely as a convenience to the user, and, while it would be possible to use simple serial numbers for each image, this schema provides quick reference information. The definitive descriptive and scientific metadata are provided in the TIFF header. The complete set of images of a single side of a sheet forms an image cube (see fig. 5). While the Dublin Core metadata elements identify the individual digital object, the other metadata elements (spatial, imaging and illumination, and data content) describe the matrix of relationships between the images and locate a single image within that matrix. Standard library and museum cataloging and metadata standards are not designed to store hyperspectral imaging metadata or other types of specialized scientific data. Dublin Core, a very common and popular metadata scheme designed for the World Wide Web, provides elements for identifying objects using a simplified core set of elements. It is intended to be extremely general and applicable to all types of resources [19]. While Dublin Core allows users to locate resources on the Web, MAchine Readable Cataloging (MARC) is a more detailed standard designed specifically for cataloging documents [20]. The Museum System (TMS) is focused on cataloging objects in museum collections. None of these systems is designed to accommodate or enable discovery of the type of scientific and geospatial information stored in hyperspectral imaging, nor should they be. Even a system like TMS, which allows considerable customization of user fields, should not be adapted to the needs of any single sort of scientific data. The Metadata for Images in XML schema (MIX) [21] provide a rich set of metadata elements for digital images but do not cover the special needs of hyperspectral imaging. There is a special distinction that can be made between MARC and the APMS. MARC is intended to refer to a document. If hyperspectral images were simple surrogates of the imaged document, they could be grouped together and treated as an alternate edition of the document, and this is possible if the Library chooses to add hyperspectral data sets to its collections. However, as pointed out above, the APMS describes the individual digital object. The individual digital object is not the same as the document but is a scientific sample of the object. The metadata that describe the physical object, its MARC record (see appendix E), have some similarities to the metadata of the digital object, but their purposes and structure are different. No one system of cataloging or metadata will be able to address the needs of this special data set nor will any one of them be able to address the needs of the range of existing or forthcoming types of scientific and other structured data and metadata that libraries may store in digital repositories. As the Library of Congress PRTD continues its use of hyperspectral imaging to extend its knowledge of the condition and content of documents and other objects in the Library’s collection, it has the problem of preserving and accessing the digital data through its metadata and maintaining links between the new data and catalog records for the objects supplemented with new digital data. For example, the staff of PRTD should be able to locate hyperspectral images of the Waldseemüller map based on geospatial or illumination information, move easily from the digital object metadata to the Library’s catalog, and vice versa. Or, preservation staff should be able to locate all digital objects created via hyperspectral imaging or a subset of the objects created using a certain wavelength of light. This repository would not be limited to hyperspectral imaging and would include other scientific analytical and scholarly linked data. For example, the Archimedes Palimpsest project used X‐ray fluorescence (XRF) to image a number of folios where text was obscured by twentieth‐century forgeries. For these images a special extension of the metadata standard was created to handle some fifty‐plus extra data elements required to record the parameters of XRF imaging [22]. The Library’s repository of scientific data will be able to accommodate a range of types of scientific metadata with metadata that complies with a core metadata component and requisite extension data. Though the data stored in the repository will be of different types and formats, they are all collected and used for the purpose of scientific preservation. A storage system that complies with PREMIS (Preservation Metadata: Implementation Strategies) guidelines may be adapted to support a range of digital data and metadata. The PREMIS system provides structures for ingesting, preserving, and disseminating packets of information [23]. The PREMIS data model and data dictionary are based on the Reference Model for an Open Archival Information System (OAIS) produced by the Consultative Committee for Space Data Systems [24]. The PREMIS system defines metadata for the preservation of digital objects, such as identifiers, checksums, bit structure, and the relationship between objects. It allows for storing additional metadata relating to objects. A common method for including additional descriptive and structural metadata is to use METS XML files [25]. The Metadata Encoding and Transmission Standard (METS) is “a standard for encoding descriptive, administrative, and structural metadata regarding objects within a digital library, expressed using the XML schema language of the World Wide Web Consortium” [26]. A PREMIS‐compliant system is concerned with preserving the digital data and maintaining it over time. The PREMIS system does not make prescriptions for descriptive metadata—metadata that describe the content of digital objects, as opposed to their bits—and what in OAIS is called Preservation Description Information (PDI). Alternate metadata belonging to the domain of the data content can accompany the digital object, and this is often done using METS, as depicted in figure 13. As there is some overlap between METS and PREMIS, it is necessary to define how administrative and other data are stored in a system employing METS and PREMIS [27]. For example, both METS and PREMIS provide fields for checksum data, and a common approach is to use redundancy and place checksum information in both schemas. Fig. 13.— Structural and descriptive metadata The integration of a PREMIS digital repository of scientific information into a library setting creates special problems for linking systems and making decisions about the division of responsibilities between those systems. Scientific images are not mere surrogates of documents in a collection. They provide specialized information for a user group within the library community that may want to locate collection items either through existing library catalogs or based on scientific criteria that are discoverable only through systems built on scientific metadata, the descriptive metadata mentioned above. In each scenario, users must be able to navigate from one system to another. This is a different problem from PREMIS systems that store digital representations (e.g., digital publications) or surrogates (e.g., images of special collections items) of documents from a library’s collection. Scientific images are not “another view” of an object but data in their own right. As discussed above, catalog tools and standards are not designed to store scientific data and should not. The different foci of the library catalog and the proposed repository raise questions about the division of responsibilities. It must be decided what overlap there should be between the card catalog and the descriptive metadata stored with scientific images of library documents. There is value and convenience in having some of the same content information found in a library catalog stored in a repository’s metadata. A common use for this would be to export Dublin Core metadata that included, among several others, dc:subject values identifying an image as a hyperspectral image and as an image of a woodblock map. For this purpose, it is convenient to have content subject keywords stored in the object’s descriptive metadata. However, it can be argued that object content information, like authorship, provenance, and date of origin already cataloged in another library system, does not belong in descriptive metadata stored in a PREMIS system. This is seen most clearly in the case of objects that are not yet well understood, as with a map of disputed date and provenance. A library will maintain the data in its catalog, updating an object’s record as data change. Without a clear mechanism or process for updating repository metadata, overlapping data in the repository become out of sync with catalog data that may be incomplete or completely incorrect. The repository should have clearly defined boundaries between all systems that provide access to the data it stores. A repository that may hold multiple types of scientific data, with variant metadata formats, cannot integrate techniques for discovering data based on the content of any one metadata scheme. The repository has the responsibility of maintaining the integrity of the data it contains but can expose metadata that allow data that are heterogeneous (within certain limits) to be searched and browsed by multiple external tools (see fig. 14). Fig. 14.— Waldseemüller metadata application Systems developed by the Library of Congress PRTD to preserve and maintain hyperspectral imaging and other digital scientific data are being developed with these challenges in mind. As important as the preservation of the bits of the data collected in digital form by the PRTD is the preservation of the metadata that enables user understanding of the content of those objects. The systems and guidelines being developed will provide connections between existing and new Library tools and will define the boundaries and relationships between the data stored in these separate systems. The success of the convergence of new technology with the preservation of the cultural treasures of the Library depends on the successful management of the data collected: the integration of effective data management and work processes, the development of data discovery systems based on user needs, and effective training of personnel to work with the system (see fig. 15). Fig. 15.— Iterative integration of IT, imaging, and convergence of user information needs Data Infrastructure Effective use of images, data, metadata, and associated studies requires not only effective data management but also an infrastructure to handle the large files, access the metadata, and allow processing and manipulation of the images. This requires capabilities that may or may not be available from the information services of a cultural institution, including (1) data storage: sufficient storage devices, either local or enterprise‐wide, to store multiple large image and data files for timely access; (2) dissemination and access: systems that allow multiple users to access and disseminate data and images for processing, studies and data integration; and (3) processing: capabilities with sufficient speed and memory to digitally process large image and data files. Depending on resources available for the funding, operation, and maintenance of the information technology infrastructure, a key question that must be addressed is the number of individuals or groups who require access to the images and data. Once the size of the user population is defined, the institution can then determine the appropriate capability to provide with consideration of cost and efficiency. These include an (1) enterprise capability, (2) limited research and study capability, or (3) individual capabilities. An enterprise approach is required to provide large servers, high‐bandwidth connectivity, and powerful workstations across a large organization. A more limited storage and dissemination capability can be utilized in the establishment of a smaller but robust “research network” to allow the movement and processing of images and data among a smaller cadre with more powerful workstations involved in the research and study. An even more limited capability can be provided for just a few individuals in a smaller organization or group, with powerful workstations for processing but dissemination and access limited to a “sneaker‐net,” provided by transferring hard drives loaded with data and images. To maintain the integrity of the imaging data and information, a “gold standard” data set is maintained offline with the original TIFF images and metadata. These data sets are used to create duplicates that are accessed by researchers and to which research analyses and processed data, as well as other data, are added. The Library of Congress is continuing to take an evolutionary approach to hyperspectral imaging and data management as funding and resources allow, starting with individual hard drives and workstations, with plans in place for a research Intranet across the core research team in PRTD. Ultimately, this could lead to a broader enterprise approach for data storage, processing, and access across multiple users within and outside the Library of Congress. A critical component of this development is the integration of like‐minded cultural heritage institutions to the effort, and collaborators from international and national libraries, archives, and museums, have all indicated interest in pursuing this convergence. Work Processes and Training A key step in implementing an advanced, integrated digital imaging capability is defining the work processes. As part of developing and integrating the technologies associated with a digital imaging system, the key work processes and work flow required to successfully provide images for research and study are being defined and validated. This includes discussion of new information and imagery technology for the collection, storage, and access to increasing amounts of information. Imaging technologies and capabilities have a wide‐reaching impact on library personnel, researchers, users, and the existing work processes, as these capabilities are integrated into existing systems. This includes the development of required work flow processes, staff training, and information management structures. There is often resistance to modifying existing methods with new methodologies and unrealistic expectations. Implementing new technologies can involve steep learning curves for staff and management, as well as the very real challenge of coordinating and managing large volumes of data that require new methods of interpretation. To support this developing capability, databases of reference materials are needed to understand the new information being generated. Effective data management for advanced imaging must also address the personnel requirements by creating professionals with highly developed skills who can navigate the traditional boundaries between disciplines and institutions to achieve the integration required to meet user needs. The potential impact on the existing personnel and work processes can be significant. This needs to be balanced with the explosion of new information and potential capabilities of the cultural heritage institution to advance current knowledge and support the preservation of historic documents and manuscripts. Understanding the impact of new capabilities on cultural heritage organizations includes successful implementation, assessment, and understanding of the value of the technology and keeping up‐to‐date with requirements for the transfer of the technology and the needs of the institution. Work processes for parchment, paper, and other object image studies in the Library include the following work tasks to collect and fully exploit scientific and scholarly analysis: • Conservation and management—preparing the objects for imaging and managing the overall work flow and data management. • Data and image collection—gathering various data elements for an object and digital images from various optical and electronic sources, including metadata about the images, image collection processes, and the object under study. • Development of a standardized methodology for imaging setup, image acquisition processes, notation of changes in resolution, lighting exposure, spatial location, and file and folder nomenclature. • Image and data processing—digitally manipulating the various spectral images to reveal information not available from any one image, as well as formatting and linking additional information about the data to the collected data set, according to agreed vocabulary, data, and content standards. • Development of a standardized format for collection of processing metadata and linking with associated scientific analyses. • Personnel training—training in data acquisition and processing requires an innate knowledge and understanding of the document or object under study to ensure that processed images accurately reflect features in the object itself. This requires skilled interpretation and processing to generate accurate and useful information. • Data management—assessing the data elements and associated metadata to ensure they meet the system standards and entering or linking the data according to agreed digital storage and retrieval standards. • Integrated research—study of the images and data, including linking data elements and establishing relationships between data elements by knowledgeable users to derive additional information and knowledge. • Development of a spectral database allowing matching of spectral responses of inks, colorants, other media, and treatments on specific substrates for nondestructive identification and analysis. • Standardized terminology and vocabulary for imaging processes. • Information storage and access—storing and retrieving data and information electronically by remote end users on the Library Intranet or on the Web by means of electronic public access, while maintaining the integrity of the data. The integration of preservation and scientific techniques with advanced imaging and processing techniques in the Library required effective systems and information management to ensure that the large amounts of digital data could be readily collected, processed, stored, and accessed. Digital Imaging and the Convergence of Cultural Heritage Organizations Digital imaging has a major impact on the user experience and data access across all types of cultural institutions. With the growing convergence of cultural heritage organizations, the interest in both the artifact as an object and digital images of the artifact brings to the forefront the roles of institutions with a traditional focus as repositories of objects versus the current trend of being seen as repositories of knowledge. While many researchers lean toward the latter perspective, the focus on the original object is apparent not only from the continued presence and requests of researchers to see original documents but from the large crowds at exhibits of significant cultural artifacts where the original item is on limited display. The actual object itself plays a critical role for a range of users—researchers and public alike [28], as was evident with the six‐month display of the handwritten copy of the rough draft of the Declaration of Independence in the Library of Congress exhibit Creating the United States. Record numbers of visitors to the exhibit increased in the final few weeks when it was advertised that the document was about to be taken off exhibit. Visitors enjoyed the interactive experience of being able to navigate their way around the virtual version of the document on display, but it was the object itself that provided the impetus for the emotional connection of their visit. This reality is documented in “InterConnections, the IMLS National Study on the Use of Libraries, Museums and the Internet” [29], which notes that over 95 percent of visitors to libraries and museums continue to visit in person, even when they frequently visit the institutional Web site on the Internet, indicating that online visits are not replacing active visits to the institution. Conversely, visitors are using the online connection to gain knowledge and view images prior to their visit so that they can gain maximum benefit from their visit. The report notes that people trust information from libraries and museums more than any other sources of information, including government, commercial, and private individual Web sites. This imposes a high level of responsibility on cultural heritage institutions to accurately document whether an object is an original, a reproduction, or a facsimile. The Library of Congress Web site, http://www.myloc.gov, actively encourages repeat visits by providing planning information prior to visiting the Library and posting interactive images and experiences through downloads linked to the scanned personal “passport” provided in the exhibits. This positive complementary relationship between cultural heritage institutions and their digital images on Web sites is important for establishing a connection between visitors both online and in person [30]. Exhibitions containing both originals and facsimiles carefully note those items that are reproductions; imaging cannot replace the original document. However, the knowledge gained from the enhanced scientific and scholarly information contained in a hyperspectral data cube adds literally another dimension to the artifact itself. The ubiquity of interactive displays as part of exhibits and information references in cultural heritage institutions is evidence of the growing interest by a range of users to do more than simply view the item. The use of persuasive technology and imaging has promoted interest through the media, movies, and TV series, where entire chapters of history or crimes can be discovered and analyzed in a weekly segment [31]. This enhanced knowledge of the capacity of technology has led to a greater demand from both researchers and the public, and this convergence is evident as libraries, archives, and museums integrate increasingly sophisticated software and hardware to provide a seamless user interface with their artifacts in person and online. Providing access is a common point of reference for all cultural institutions, with the growing use of the Internet offering the first access point for researchers, the public, and other users. The convergence of objects in libraries, archives, and museums with digital images and data shared by these institutions and private service providers requires all organizations providing digital images, information, and access to utilize the available cyberinfrastructure and effective management of data to allow greater access. Deanna Marcum, associate librarian for library services at the Library of Congress, noted “making much more content much more accessible is a great and worthy goal.” Advances in technologies allow the customization of information resources for users, with the goal of future digital libraries being one of enabling a comprehensive collection of resources for scholarship, teaching, and learning with easy access for a range of users. She notes that access to this collection is “managed and maintained by professionals who see their role as stewards of the intellectual and cultural heritages of the world” [32]. The increased usage and demand for digital resources in cultural heritage institutions is demonstrated from Web site statistics, with users utilizing available search engines to access data that may be linked or accessed from a range of sources, not necessarily always the home institution. Linkages between Web sites encourages users to visit associated sites to meet their information requirements. A key challenge for effective stewardship is determining how to manage digital resources, to relate them to the objects they document, and to develop tools that connect and maintain as discreet the data about the digital and the physical objects. The cataloging of digital data, whether archival surrogate images, scientific images, or other data, is a task in its own right that must be coordinated with existing cataloging systems. However, the metadata problem manifested in the difference between the MARC or TMS catalog of physical holdings and the METS or Dublin Core record attached to a digital object must be addressed by institutions in the course of determining how they will use digital collections in conjunction with their traditional ones. While the successful development and preservation of digital repositories relies on a management program based on planning for long‐term use and preservation of the data, the requirements of use and long‐term storage must shape the methods and processes of digital collection and capture. The fundamental change that has driven this convergence across institutions with technology relates to the previously limited access to information. In the past, power was derived from the control of information and data. In the changing global and collaborative environment, power is now (and should be) derived from how well information is used and value added to data and information that are made accessible to many. While the changing role of libraries is to provide the best access possible, it must also create a structure that enables the users to readily access, relate, and utilize this information. The National Science Foundation coined the term “data scientist,” and professionals with these integrated skills will be critical in establishing effective access and networks. Conclusions Convergences between information technology and data and information management in advanced imaging systems illustrate common challenges and opportunities across cultural heritage institutions. Preservation professionals, researchers, and scholars in libraries, archives, and museums, share common needs for access to the original object, research images, integrated data, and information structures and systems. These are best supported by the integration of imaging and information management systems and advanced image collection and processing capabilities. An effective imaging system requires not just the installation of imaging equipment, imaging objects, and storing digital products for users. It also requires that well‐developed metadata and data management, storage, and access are effectively integrated across institutions following broadly accepted international consensus standards and protocols. The development of systems that ensure flexibility, accessibility, and integrity of scientific images and data provides a strong preservation tool for all cultural heritage institutions. Building on previous efforts in the field in other institutions, the Library of Congress is collaborating to develop a standardized model for handling large, complex spectral image files and associated data. The Library is integrating advanced spectral imaging capabilities to collect images and a range of scientific information about its manuscripts, documents, and other objects; to process the images; and to store and disseminate the digital product for end users. Over time, standardization of data and metadata structures, work processes, and personnel capabilities will provide improved system efficiency, limited budget impacts, and broader availability of and increased confidence in the imaging study products. Developing standardized methodologies will allow the technology to be adopted by a wider range of users. Establishment of a spectral database of reference colorants, inks, pigments, and substrates will help enhance the application of this capability to objects beyond the specific institutional holdings. Linking specific spectral responses with identified materials allows a truly nondestructive technique to advance the knowledge base and to preserve an extensive collection of increasingly fragile documents and artifacts. A continued focus on collaboration between people, data, and work processes, and the tools and technologies for efficient access, is critical to promoting the free flow of information across cultural institutions and service providers. Improved access allows greater management of information collections, achieving the goals of libraries to allow expanded access to collections globally, while also addressing the quality of the information and interaction. Data collections and the algorithms developed to process advanced images need to be considered as part of the “product”—rather than just as a tool to create a final digital object. This will allow a greater focus on and understanding of the informatics required to structure and organize large volumes of data. Enabling the user to effectively use and integrate the results of new technology will generate new capabilities with greater utilization of the cyberinfrastructure, information and personnel capabilities, and technical tools. The maintenance and integrity of image and data collections reinforce the enhanced capability these integrated information systems can provide over time. Effectively managing this integration will maximize the capacity of systems to meet the needs of a wider, changing, and growing generation of information users. Collaboration of information, scientific, and data professionals, and convergence across their disciplines, will assist in the integration of the technical, organizational, and social processes required to implement them. The data deluge of the past decade has highlighted the need for data management, with inherent risks from a lack of focus on the requirement for data access for generations to come. The first and greatest impact of this explosion of digital data is on the institution. The risk is that terabytes of data will be collected that exist for years without being accessible to users in a structured way—available only to people who were there when the data were collected and are able to comb through directories on a hard drive to retrieve files manually as needed. Clearly, in this situation, the ability to use and access data deteriorates over time, as personnel move on to new projects and interests, and human recollection of older data begins to fade. A first step is to standardize data collection practices to integrate metadata and to standardize interim storage practices and archive structures, but this is just a first step. It assumes the existence or development of systems to store and manage the data and anticipates something more than external hard drives or folders on server(s). It anticipates trained personnel with special skills, frameworks, and dedicated computer hardware to store, manage, preserve, and locate the data. Because most data sets have special needs, standards must be selected and implementation plans created. Even off‐the‐shelf software packages for data management must be customized by personnel with considerable technical expertise. Typically, multiple processes are occurring at the same time. The Library of Congress PRTD now has proven standards for the collection of hyperspectral images and is working on processes for collecting and maintaining the data. At the same time, research and planning is taking place for a repository for long‐term storage of the data and interfaces for access. Data that are being collected now must be collected and stored for entry into a repository that has not yet been built and yet must be usable for PRTD personnel before the repository is in place. While for decades government and industrial programs have converged for the integration of complex technical capabilities and processes, integration of new technologies into the converging library, museum, and preservation environments requires the application of a new set of requirements not only for the technologies but also for the data structure and work processes. As always, the risk is that the institutional challenges will not be met because of a lack of time or will or personnel. Once the commitment has been made to undertake such a project, the typical risks associated with any project apply. Costs, schedules, and expectations must be managed. In other words, technical projects require project management. The integration and standardization of processes, terminology, ontologies, and metadata to support integrated imaging and data management systems and the cyberinfrastrstructure of accessible repositories is critical to advancing this undertaking, with the Library of Congress PRTD working to integrate broad imaging and metadata standards, work processes, and technical systems with those in other libraries and cultural heritage institutions. For the Library, the development of systems for storing and sharing data will be conducted in collaboration with other institutions to construct standards for the exchange of preservation data. As a part of meeting the challenge of coping with a growing collection of digital data, institutions are working to make the systems they build fit the needs of users. For this Library project, the users include a range of institutions around the world, with volumes of data and commensurate access and integration issues, all converging toward a common goal of preserving object and preservation data and knowledge. With collaborative input and leadership, the combination of advanced preservation spectral imaging and integrated data information management systems can make major contributions to artifact studies and the information needs of a range of cultural heritage organizations. The changing roles of staff and other professionals in libraries, archives, and museums demand significant attention to the information needs of external users, as well as those benefiting internal staff requirements and the needs of collections. This can be attained through management and maintenance of quality data, while allowing access to an increasing volume of integrated images and data within a structured metadata scheme. By maintaining focus on the content of the original artifact or document, the new digital object created from advanced spectral images and data for scientific, scholarly, and preservation knowledge will allow access, interpretation, and preservation of fragile items of significant cultural heritage. This will allow libraries, archives, and museums to converge with information technology and data management in supporting their common role as effective stewards of these artifacts for generations to come.