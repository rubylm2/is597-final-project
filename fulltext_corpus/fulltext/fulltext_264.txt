TECHNOLOGICAL FOUNDATIONS FOR BIBLIOGRAPHIC CONTROL SYSTEMS Ronald L. Wigington and Charles N. Costakos Introduction A wide range of information technology has been available for several years from which to build information handling systems for bibliographic control. That technology, continuing to outrun the ability of designers to build com- plex systems and administrators to agree on the institutional aspects of run- ning them, has been improving rapidly both in performance and cost. The improvements in information technology show every sign of continuing, and they will provide those institutions which have had the foresight to commit themselves to use those technologies with a distinct advantage in the struggles for survival of our library and information institutions. Particularly beneficial have been cooperative ventures made possible by those technologies where shared work reduces the redundant activities in each institution, or at least provides a shared facility for use by several institutions. This report will review recent progress, current status, and continuing trends in the availability and use of information technology in building sys- tems for bibliographic control. Although we are mainly interested in looking ahead to what can be done, a comparison of today's position with the expec- tations of five to six years ago will provide additional insight into the rate of progress in developing information technology. We must also understand that we are dealing with only a brief instant of time in a very long evolution. While we are concerned with continuity of the growth of knowledge since antiquity, we are also concerned with the continu- ity of today's knowledge into the twenty-first century. By now, the reality and inevitability of the use of highly developed technology in library and infor- mation systems should be a foregone conclusion. Our strategy should be to design and operate systems for continuity and not be trapped into optimizing for the immediate future. Technology Assessment: Five Years Ago In 1970 and 1971, an assessment by a panel of the National Academy of Sciences, sponsored by the Council on Library Resources, was made of the [Library Quarterly, vol. 47, no. 3, pp. 285-3071 01977 by The University of Chicago. All right reserved. 285
286 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS state of computer and related technologies for application to libraries and information systems. The results were published in 1972 in the report, Librar- ies and Information Technology. A National System Challenge [1]. At that time, the long-promised payoffs through the use of various automation technologies in library operations and services and in the information industry in general had not yet occurred. Moreover, not enough practical experience on a signifi- cant operational scale had been demonstrated to support the promises. Large systems had been slow and expensive in development, and had fewer capabil- ities than desired. There were questions as to the shortcomings in the technol- ogy that had prevented faster progress. This study found that the trend of technological development showed every sign of being able to produce the necessary "nuts and bolts" from which to put together large-scale computer-based information systems of national ex- tent. Not only was the basic technological feasibility assured, but the cost trends of specific equipment were favorable. While there were problems in 1970-71 in bringing complex systems into reality, these were attributed to institutional defense mechanisms, organiza- tional problems, and the tremendously difficult task of managing the devel- opment and introduction of complex systems. The computer applications industry was poorly prepared for the task, lacking a stable and effective body of doctrine and practice to guide work on such systems, regardless of whether they were bibliographic, business, banking, or other systems. At the same time, the intellectual boundaries between the bibliographic disciplines and those involved in the production and application of technology were difficult to bridge. The Nature and Context of Bibliographic Control Systems Guidance in the design of large-scale information systems, and especially in the representation and structuring of the information they contain, may be had from a comparison of computation systems and information systems, both of which are implemented with the same basic technology. As pointed out by Endicott and Huyck, "Computation systems can be thought of in relation to information systems in the following way: a computation system has data to be found in the future, embedded in its logic prepared in the past; an information system has logic to be found in the future embedded in its data prepared in the past." [2, p. 36]. If not understood, this fundamental difference in approach can lead to a failure to understand what the important requirements are for information systems, especially the requirement for flexibility to extend the system for use outside the local and current context for which it was originally constructed. Information representation and file design thus take on an importance that extends beyond their relationship to the specific processes which had operat-
TECHNOLOGICAL FOUNDATIONS 287 ed on them at the time the system had been installed. There will be new processes, not known at the time of installation, which will be introduced and used on data recorded in the past. Conversion and upgrading of files for new purposes may not be economically feasible if the original designs for the files were too limiting and did not provide for the recording of essential detail and for authority control. A fundamental problem of storing information for future use is finding practical compromises between current economy and future value, a value which cannot fully be quantified. Fortunately, with the rapid improvements of information technology, the cost of information storage and computer pro- cessing is decreasing. While the human effort necessary for information anal- ysis and data entry is becoming more expensive, the benefit of an increasing level of mechanization is increasing. While this conference is focused on library systems for bibliographic con- trol, we must also be aware of how the systems relate to a national-scale information system involving public and private institutions. The general functions of a national-scale information system include: Selection: processes for choosing what should be obtained and stored. Storage: processes for appropriately organizing the information to be stored, for storing the information, and for maintaining and managing the storehouses. Access: processes for identifying and finding desired information in the storehouse. Distribution: processes for transferring information from producer to storehouse, among storehouses, and from storehouses to users. Control: processes for orchestrating the selection, storage, access, and distribution functions. [1, p. 3] Bibliographic data are used throughout all of these functions and the pro- cesses of bibliographic control require essentially the same technology as the total information system. Moreover, since aspects of bibliographic control extend well beyond the boundaries of the library community into the pub- lishing, abstracting and indexing, and user environments, the needs and in- fluences of these other environments and institutions must not be neglected. Thus, as we examine the technological foundations for bibliographic control, we encounter all facets of information technology. Also, although the total volume of bibliographic data to be processed, exchanged, and used nationally and internationally is very large, it is only a fraction of the size of the total body of information to which it refers. Often applications of new technology for information handling first become economically practical on the smaller volume of data; thus in the systems we build today for bibliographic control we will see precursors of methods for handling primary information in the future. This, in turn, needs to be taken into account in designing systems for bibliographic control.
288 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS Aspects of Information Technology The general term "information technology" has been used to cover a variety of electronic and photographic digital and image-handling technologies that are being applied to all phases of information handling from original genera- tion through use. Chief among these technologies is computer technology, which has re- ceived widespread attention for many years. The computer hardware for information processing and storage and the software that makes it work are central in modern information technology because effective computer-based information systems are not limited to transformation, storage, and retrieval functions but also include the mechanism for control of all other technologi- cally based components of the information system. Because such a large fraction of the total human effort required in today's computer-based information systems occurs at the information input stage, data entry technology is singled out for special review in this discussion. Data entry technology includes many kinds of keyboarding equipment and termi- nals as well as OCR equipment, bar code reading equipment, and so-called word-processing systems. The second aspect of these systems is communication technology, which is intimately involved in any terminal-oriented system and which provides a linkage mechanism for geographically dispersed networks. A third aspect of the technology is reprography. This includes methods for production not only of printed pages but also of microforms and other image replication. It also includes distribution media and means for the use of non- conventional media. Progress, Status, and Trends Comparing the status and trends of now (1976) with those of five to six years ago we see that the optimism expressed in the expectations in 1970-71 has been well supported in most instances in the development of the basic tech- nology and in the increasingly successful application of it. Computers There have been two divergent trends in the development of the technology of central processing units. One is represented by the very fast supercompu- ters which execute hundreds of millions of instructions per second through "pipeline" designs that perform many computations in parallel. Such exam- ples as the CDC-STAR, SDC-PEPE, and CRAY-1 have been under devel- opment for a long time and are now emerging for use on very large-scale computational problems [3]. The second trend is toward much cheaper and more capable minicomputers that can be used for decentralized computa-
TECHNOLOGICAL FOUNDATIONS 289 tional functions, and microcomputers that allow even further distribution of control and interaction functions into intelligent terminals and other periph- eral equipment. Both development trends were established about fifteen years ago, but the economic and performance improvements of large-scale integrat- ed (LSI) circuits have been dramatic over the past few years, as was expected, and have made it possible to bring those ideas to fruition. In addition, the widespread emergence of low cost LSI solid state digital electronics in the telecommunications and office equipment market and in the general consum- er market for entertainment, appliance controls, automotive controls, calcu- lators, and watches will keep a strong downward pressure on prices for an increasing level of electronic capability [4-6]. The increasing quantity of production and the competition for those markets should be very beneficial for the buyer. Increase in market volume always has a beneficial effect on unit prices as long as shortages of supply can be avoided. The industry annual volume for minicomputers (hardware, service, and software) has grown from well under $1 billion in 1973 to approximately $2 billion in 1976, and an increase to $5 billion is expected in 1980 [7]. Supercomputers might be applicable and attractive for very large informa- tion processing centers if parallel processing strategies, similar to those ap- plied to large-scale array computations, could be used. However, the trends of much more importance for bibliographic control systems arise from mini- and microcomputer developments. Although there are software costs that potentially far exceed hardware costs, and no less care must be taken in total system design, the basic hardware is getting so inexpensive that one barrier to system acquisition is almost gone. It was predicted in 1971 that by 1975 a 4K-word computer of respectable performance would be available for incorporation into intelligent terminals and would cost between $1,000 and $2,000. Currently the price of sixteen-bit minicomputers with 4K words (8K bytes) of memory starts at about $2,000. For use in intelligent terminals and other controllers, microcomputers range from a few hundred dollars up to $1,500 depending upon the amount of memory and other features. For example, a popular microcomputer with 16K bytes of memory, forty-eight input/output lines, and other necessary interfacing and control circuitry costs about $1,500. A basic minicomputer system of medium computational capacity, including 64K bytes of memory, two disk cartridges of 2.5M bytes each, a real-time clock, and a teletype-class terminal can be purchased for around $30,000. A key factor in making these minicomputers effective for information sys- tems is the low cost, high performance semiconductor memory that in 1976 typically costs around 1? per bit for 8-16K bytes, as compared with 1 1? per bit in 1971. The availability of cheap memory to hold the large volume of text and data structures which characterize these applications has particular- ly aided nonnumeric information processing and interactive systems.
290 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS For computer systems employing virtual memory techniques to extend the effective size of real memory, the paging medium and the page rate it can support are extremely important [8, 9]. They were important five years ago, but the increasing spread of virtual memory machines has increased that importance. Paging media are still drums or, with somewhat lesser perfor- mance, standard disks. There has long been an "access gap" in capacity and speed between these rotating media and the primary memory of computers. Three technologies are striving to close that gap: BEAMOS (beam addressed metal oxide semi- conductor memory), CCD (charge coupled devices), and magnetic bubbles. Utilization of all are claimed to be imminent [10, 11]. System designers may find that these "secondary memory" devices further improve the price/per- formance of both large-scale and small computers, although the same im- provements may also come directly from the continued improvement of the price/performance of solid state primary random access memory [12]. By any measure one wishes to examine, the computational power available for information systems has become very much cheaper in the last five or six years and is no longer a major obstacle to system development. Direct Access Memory More important for information systems than computational power is second- ary direct access storage, now available in various fixed and demountable forms. The price/performance improvement in this field over the last several years has also been remarkable. The hardware capital cost of disk memory including disk controller has already improved since 1970 by the factor of ten which had been predicted to occur by 1980-81 [1, p. 69], and now costs less than 10-3? per bit. Typical examples of disk memory unit costs are: (1) for large central systems (for example, IBM 370/168): 1.27 X 109 byte fixed media installation, 1.4 X 10-3?/bit; (2) for medium-scale minicomputers (for example, PDP- 11/34): 1.26 x 199 byte demountable media installation, 0.7 X 103?/bit. These improvements have come about by increasing the storage density on magnetic media and from high-volume competitive marketing. As demon- strated in the moderate capacity IBM Winchester and Memorex's Data Mark, there is another factor of five or six in density of storage on rotational magnetic media that may become available in larger units [13, p. 44]. This does, however, appear to approach the limits of material and mechanical tolerances. In addition to limits on growth of capacity due to physical limits, there are practical limits to the effectiveness of very large capacity disks because so much information is reachable only through a single reading arm (on each unit). High-volume random access traffic will be impeded by such bottle- necks, and the ability to store a quantity of information becomes less impor- tant than the ability to access it. Careful balancing of the placement of the
TECHNOLOGICAL FOUNDATIONS 291 information in a multi-unit assembly may be needed. Typically, and fortu- nately, bibliographic files have very skewed access patterns, a small fraction of the material being accessed most of the time. This high-activity portion can be placed to minimize access conflicts by using file access software that allows physical placement optimization. These recent advances in disk technology have brought file storage capaci- ties up to sizes that are suitable for large on-line bibliographic systems at hardware costs that are now acceptable but still continuing to drop. This cost drop, however, should be expected to level off as the limits of mechanical tolerances are reached. The availability of large disk units for minicomputer systems at costs per bit much less than units formerly available only for medium to large computers is particularly important for information systems. Mass Storage Large-scale mass memory (with storage capacities of 1012 bits and more) has not advanced as rapidly to the level of performance expected, or to that needed for very large information systems. The major advances in high-densi- ty disk systems have satisfied the needs of users of moderate-sized data bases, but the demand for very large installations has not been very strong. High- density magnetic tapes are still the major medium for truly large files of multibillion byte size and larger. Some major on-line systems have multibil- lion byte storage capacities implemented with disks, and a few are still oper- ating with Data Cells. The two mass storage units most likely to be routinely used, the IBM 3850 and the CDC 38500, are essentially magnetic tape media segmented for me- chanical handling. The storage cost is of the order of 0.0001? per bit, which is the same as the cost of large-scale storage five years ago. The basic mechan- ical segmentation into cells moves data from storage position to read/write station in several seconds in quantities of 8 million bytes (CDC) or 50 million bytes (IBM). Because of the way in which the data are divided into cells which are mechanically moved, such units are well suited to the automation of tape libraries containing many small files which are fetched, loaded, and then accessed as disk files. The file sizes handled best are those within an order of magnitude of the basic physical segmentation. However, because of mechanical queuing delays, such devices are not well suited for very large multibillion byte files which have frequent and widely dispersed random access. Nor are they suited for very large serial files, which are better handled by high-speed, high-density magnetic tape. A mix of high- capacity disks and a mass storage unit may be useful for the mix of active and relatively inactive portions of very large files such as will be found in a large bibliographic system. The promises of massive, cheap, rapid access storage using holography or other novel technologies have not yet been fulfilled and system designers must design around the limits of access dynamics now imposed by the mechanical
292 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS handling of segmented high-density media. Progress since the introduction of the IBM Photostore in mid-1960s has not been impressive [14]. Other approaches, such as using large numbers of multiple disk assemblies (for example, the STC-8800, an 800-million-byte assembly) for the relatively inactive portion of local files, will become competitive with the low end of the size range for mass storage units. The "video disk," now being considered for home playback of television programs and movies [15], is one version of the new media receiving recent attention that have potential application for digital mass storage. One of these, for example, called DISCOVISION, stores 54,000 TV frames on one side of a twelve-inch circular surface and uses a laser readout technique [16]. Even in this image-handling form, such a medium may be useful in a library environment for storage and use of large compilations of catalogs, indexes, and reference materials that now require many feet of shelf space. Format- ting of the material to be stored, consistent with the image resolution of standard TV equipment, would be required. Success of the technology in the entertainment market will improve the economics of its use for other purpos- es. Adaptation for digital storage and use for low-cost read-only digital infor- mation distribution is likely and, depending upon the amount of redundancy encoding necessary to combat bit errors due to various reasons, a potential exists for storing tens of billions of bits on each twelve-inch disk. Playback system cost is expected to be low and thus this storage medium could be very useful in minicomputer-based systems. For implementation of massive stores of information, however, mechanical handling of media or the simultaneous use of many playback units would be required. Terminals and Data Entry Recent advances in semiconductor technology along with related trends to- ward geographically distributed use of computer systems have fostered a pro- liferation of terminals for data entry and data communications. Estimates by Arthur D. Little, Inc. [17], indicate that total annual sales of all types of terminals, including the special purpose terminals for banks and point-of-sale applications, will more than double from 1975 to 1980, to a total value of $2.8 billion. The number of installed terminals of all types will increase from 1,233,000 (worth $5.78 billion) to 2,965,000 (worth $15.315 billion) during this period. The percentage of total computing hardware expenditures allo- cated to terminals will also increase from 12 percent in 1974 to about 24 percent by 1980. A much wider range of capabilities has become available in terrninals over the past few years, and the trend toward increased capabilities along with slowly declining prices is expected to continue. Growth for these more power- ful terminals, those with more functional capabilities, will be much stronger during the next five to ten years than that for the simpler keyboard-printer terminals which had become so widespread by the early 1970s. The market
TECHNOLOGICAL FOUNDATIONS 293 for these simpler text terminals, those compatible with the Teletype Model 33, will decline slowly over the next five years, becoming essentially a re- placement market. Aggressive marketing by Teletype of its more powerful Model 40 terminal should assure growth in the market for Teletype-compati- ble asynchronous text terminals. Substantial growth is also forecast for the IBM 3270-compatible synchronous display terminals. The largest growth is anticipated for CRT text terminals, with some hard copy attachments be- coming more readily available. What the growth for graphics terminals will be is still uncertain at this point, as it has been for many years. The market for the less expensive and more powerful storage tube graphics terminals, like the Tektronix 4000 and the Hughes Conographic series, will expand quickly relative to a small in- stalled base, and will continue to be used to fill special system requirements. Wide general use does not, however, seem likely very soon. Although the cost of these terminals has declined substantially in recent years, they are still in the $10,000-$20,000 range and, in some cases, higher. The introduction of the so-called intelligent terminals, those with memory- and microprocessor-based computing functions available at the terminal it- self, provides new opportunities for distributing to the terminal some of those tasks which have normally been handled by the main computer or minicom- puter in on-line systems. This aids the development of larger, more responsive. on-line interactive systems. An example of this type of terminal is the recently introduced Beehive 500. This terminal has 4K bytes of display memory, and supports scrolling of up to 256 display lines. It is user programmable, with up to 48K bytes of random access memory available; the contents of this memo- ry can be loaded from the computer to which the terminal is attached, so that multiple applications can be handled by the same terminal hardware config- uration. Standard functions include the ability to display up to 224 different custom-specified characters and to insert or delete characters from the text displayed on the screen. It can be programmed to interact in complex ways with the operator and to insert or delete words, sentences, and paragraphs. The terminal can operate asynchronously at 9,600 bits per second or synchro- nously at 19,200 bits per second. It is all the more remarkable because its base purchase price is in the $3,000-$4,000 range, which makes it very attractive indeed for large on-line systems where large amounts of text are processed at the terminal and where responsiveness is important. Many more terminals of this type can be expected to enter the market. The prospects for success in using such terminals lie in synthesizing the appropriate design for the appli- cation system-that is, in an appropriate selection of functions to be per- formed and the distribution of those functions among the main computer, the terminal, and the minicomputer which may support those terminals. For data entry applications, the use of "keyboard clusters," a group of full display or limited display terminals controlled by a single shared minicompu- ter, has spread rapidly and will continue to do so for the next few years as
294 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS users continue to replace the keypunch, paper tape, and key-to-tape devices with these systems. The keyboard clusters are competitive in price/through- put with the more conventional stand-alone data entry machines, and pro- vide clear advantages in data editing, reformatting, and system management and control. The future for optical character recognition (OCR) equipment as an im- portant alternative for data entry has been somewhat uncertain, in spite of occasional glowing predictions to the contrary. Recent surveys [181 indicate that the large majority of users who are planning to change data entry equip- ment will change to keyboard clusters; only a few plan to switch to OCR. The reasons that this industry has so far failed to fulfill its earlier promise include: high equipment cost, continuing high error rates, limited font recog- nition capability, forms design problems, document positioning and transport difficulties, and inadequate handprint reading capability. Specialized system requirements, like turnaround documents,' are being and will continue to be met by OCR devices, but use for general text input will probably grow only very slowly and be dwarfed by expanded use of keyboard clusters. It is risky to predict the impact of improving cost/performance of semicon- ductor technology on OCR equipment. Many of the key components are electronic-scanners, recognition units, and controllers-and there is increas- ing use of software instead of hardware to implement more complex capabili- ties in these functions. These developments have recently led to improved prospects for highly sophisticated OCR systems which can read multiple fonts intermixed on a page. The Information International Grafix I, a multimil- lion-dollar system, reads characters on microfilm (which minimizes document positioning and transport problems). It has already been used by the U.S. Navy to republish over a million pages of technical manuals containing text, illustrations, and tables [19], and to convert some Canadian court records to digital form [20]. A much less expensive multifont system, which might be expected to be in the $100-thousand range, is now being introduced by Kurz- weil Computer Products, Inc. [21]. The Kurzweil machine was originally conceived as a method of converting ordinary printed information in a wide variety of character fonts to synthesized speech for the use of the blind. The character recognition portion of it, implemented through software for feature analysis, should provide the basis for a conversion system that is significantly more economical than normal keyboarding. These products illustrate the availability of reasonable solutions to special system problems and offer hope for a practical solution to the general print-reading problem that has been elusive in previous years. 1. A turnaround document is a medium sent to a customer. It can be in such forms as punched cards, OCR cards, or magnetically readable symbols, and it is returned to the processing center to be read, at least in part, mechanically. Turnaround documents are used in many billing systems.
TECHNOLOGICAL FOUNDATIONS 295 One class of optical reading devices for data input which is gaining wide acceptance is the bar code reader [22]. From a beginning usually traced to labeling of railroad cars for control and "inventory" functions in large rail yards, bar codes have become very widespread, particularly in the last few years. With the adoption of bar coding for the Universal Product Code, a large market for bar code labelers and readers is virtually assured [231. The availability of relatively inexpensive and reliable equipment has led to the adoption of bar codes for an incredible variety of data entry applications, includirng library circulation and control systems. A few such systems are available commercially for libraries, and a number of systems based on bar codes are in operation or nearing completion in libraries now, including the University of Chicago Library. For monitoring and controlling the move- ment of physical objects, identification techniques based on bar codes offer an attractive solution. "Word-processing systems" in part provide yet another alternative for data entry sometimes called "source data automation," referring to the recording of data in machine-readable form at its source. This eliminates the need to type manually or write the data for later transcription to machine-readable form. It seems clear that the labor-intensive office, the last big holdout to automation in businesses, will "soon" fall to revolutionary new information- processing systems based on advances in the technologies covered in this paper [241. It is not clear how soon is "soon." Some feel that by 1990 most handling of business records and correspondence will be electronic, but this is one of the more optimistic predictions. There were an estimated 250,000- 300,000 editing typewriters installed at the end of 1975, and this number is expected to triple within the next five years. A related development is the possibility that electronic mail handling systems will become available through the U.S. Postal Service. A recent study [25] for the U.S. Postal Ser- vice confirms that current technology is adequate to develop such a system within ten years, if the appropriately careful systems design were done. These trends presage the growth of a broad market for this new class of electronic data entry equipment. This will have an impact on library systems through the availability of lower priced system components, and by the appearance of some computer-readable primary documents. The requirement for use of large character sets, including nonroman and special characters, has been a continuing problem in bibliographic control systems, as well as for publishers and abstracting and indexing services. With the advent of display terminals and nonimpact printers, which use dot ma- trixes to form a wide range of different characters, this problem is now tracta- ble. The Beehive 500 intelligent terminal, for example, can display up to 224 different characters, and nonimpact printers driven by appropriate format- ting programs are routinely used to compose text with several hundred differ- ent characters. The adoption by the American National Standards Institute and the International Organization for Standardization (ISO) of a standard
296 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS basic character set (called ASCII in the United States), along with standard mechanisms for extending this scheme to represent virtually any number of additional characters, makes possible the representation and exchange of bib- liographic information using as many different characters as needed.2 Communications By almost any measuring scale, the data communications industry is large and growing rapidly. Most data communications systems extant today use "voice grade" telephone lines, typically at transmission rates up to 2.4 kilobits per second, although rates up to 9.6 kilobits per second are now available. The higher transmission speeds, 19.2 kilobits per second and higher, are be- coming somewhat more available, but they require special communication channel arrangements and are still relatively expensive. Communications costs have been declining and are expected to continue declining, but at a slow rate [26]. The so-called value added networks (VANs), like TELENET and TYM- NET, along with other specialized common carriers made possible by a 1971 FCC ruling, have made long-distance communication charges more nearly proportional to actual use [27]. This has helped make possible the rapidly growing use of on-line retrieval services, such as those operated by Lockheed Information Systems and Systems Development Corporation, and this in turn has spurred the building of a growing number of bibliographic data bases by abstracting and indexing services for computer searching. For the novice or infrequent user, there are difficulties in learning the query language and indexing schemes implemented by the providers of search service and in understanding the widely differing structure and content of the many data bases now available. A number of technological innovations are under development which hold promise for continued lower data communication costs and improved service alternatives. Optical fibers, through which data can be transmitted by light pulses, are being tested in telephone circuits by the Bell System and others [28]. They offer the advantage of being much smaller and lighter than tradi- tional copper cables, as well as eliminating interference with adjacent lines because of the induction properties of conventional circuits. Because of their transmission characteristics, they offer a wider bandwidth than conventional telephone circuits and thus will permit faster data communications over the standard voice grade channels [29]. American Telephone and Telegraph is also gradually shifting to digital transmission and switching equipment, 2. For a description of these standards, see the following American National Standards: X3.4- 1968, Code for Information Interchange; X3.41-1974, Code Extension Techniques for Use with the 7-Bit Coded Character Set of American National Standard Code for Information Interchange (ASCII), X3.4- 1968. The corresponding international standards, published by the International Organiza- tion for Standardization (ISO), are ISO 646-1973 and ISO 2022-1973, respectively.
TECHNOLOGICAL FOUNDATIONS 297 which will provide faster, more accurate, and less expensive means for data communications than the existing analog circuits. The use of satellites for data communications offers more accurate and cheaper long haul communications, especially for overseas hookups [30]. A number of communications satellites are already in use [311. The cost for the ground stations necessary for satellite data communications is still high, around $100,000 or higher, but is expected to decline to about $10,000 within five years [26]. The extremely wide bandwidth of cable television, along with its potential of being connected to every individual home, have long held the promise of providing a low cost means for data communications for homes and businesses. But cable television has yet to fulfill this promise, partly because of the enormous investment required and partly because demand for it has been disappointingly low, especially in the cities where normal TV reception is of good quality. Its actual degree of use for data communications in the future remains a question at this point. A significant trend is the gradual blurring of functions between computers and data communications. The use of computers to regulate and direct data flow in value added networks has helped reduce communication costs, as noted above. The increasing use of microprocessors to handle communica- tions functions in terminals, front-end processors, data concentrators, and satellites are all signals of this blurring of function and thus of the merging of these two once separate industries. Reprography While technological innovations have revolutionized the computer industry as a whole, improvements in impact printers have been only evolutionary in scope; improvements will continue to be made, but printing has become an increasing bottleneck as computing speed has increased. Advances in nonim- pact printers within the last decade have provided a good alternative [32]. Nonimpact printers can be much faster than impact printers because they are not limited by the operation of electromechanical hammers, although paper movement mechanisms can be a problem. The nonimpact printing technologies with the most promise at this point are ink jet systems on one hand, and electrophotographic and electrostatic systems on the other [25]. A number of systems utilizing these technologies are commercially available and work well, although some problems of reliability have been encountered with ink jet systems, and the cost of paper for electrophotographic and elect- rostatic systems is still relatively high. Among the electrophotographic print- ers, the laser printers introduced within the last few years are particularly promising, including such systems as the Xerox 1200, the IBM 3800, and the Canon LBP-4000 [33]. In addition to their high speed, these nonimpact printers offer a much wider range of functional capabilities than impact printers, including an essentially unlimited character set and the ability to compose intermixed text and graphics. Although the market for impact
298 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS printers is far from dead, use of nonimpact printers is expected to grow rap- idly. According to a recent report by Frost and Sullivan, Inc., of New York [34], the market share for nonimpact printers will increase from 7 percent of the total printer market in 1974 to 20 percent by 1985. In the last fifteen to twenty years, the text composition industry has changed drastically as a result of the introduction of computer-based tech- niques for page composition. The use of computer-driven photocomposition machines is widespread and has provided significant economies for printing with no degradation in quality. More recent developments in hardware and in software support make possible the automatic composition of intermixed text and graphics on a page. These machines are generally cathode ray tube phototypesetters, although some laser beam and fiber optics devices are being built. The output of these systems is a photographic image of the page which is then transferred to a plate for printing. The major function in the composition industry which is still largely unau- tomated is the task of creating the printing plate itself. For most printing companies, the composed pages must be manually arranged on a multipage "flat," and this flat is then used to create a multipage printing plate for the printing presses. This process is time consuming, expensive, and error prone. The purpose of a new generation of platemakers is to eliminate this series of steps. Development of two types of such automated platemakers, microform- to-plate and laser devices, is receiving particular emphasis. The microform- to-plate machines use microfilm images of the pages as input, automatically arrange these images in the proper sequence and orientation, and enlarge the images for burning a plate. The laser devices scan composed pages with a low-wattage laser beam, and use that beam to control the movements of a separate, high-powered laser which burns the printing plate directly. Since two separate lasers are involved, they need not necessarily be located at the same site; some of these machines have been set up with the two lasers widely separated in distance, linked together via data communications facilities. Laser platemakers are almost exclusively used by newspaper publishers at this time, because their high volume justifies the high equipment costs. Both microform and laser technologies offer significant cost advantages for high- volume publishers, and the equipment cost will decline as sales increase. The predictions for growth of the microforms industry have consistently been much higher than actual growth, and growth forecasts are now much more conservative and probably more realistic than in past years [35]. The expanded use of microforms is still limited by their many well-known disad- vantages, which include too many formats, reduction ratios, and retrieval coding schemes; poor quality film images at the reader and on hard copy prints; readers which are expensive, hard to use, and bulky; inadequate user environments in libraries; and difficulty in obtaining hard copy prints. As Bagg [36] points out, these are the same disadvantages that have plagued microforms for many years, and libraries alone cannot afford to fund the
TECHNOLOGICAL FOUNDATIONS 299 developments needed to overcome these problems; they must be able to take advantage of developments generated by demand from a much larger mar- ket. For large files that are characterized by primarily serial access, roll micro- film will continue to be an effective storage medium. For smaller amounts of information, like technical reports and even full journals, microfiche is a more hospitable medium and is being increasingly used. In spite of the con- tinuing lack of standard formats and reduction ratios, which is more of a problem for fiche than roll film, the use of fiche will probably continue to grow, although slowly. The rising cost of silver halide films, which is the only type suitable at this point for archival storage, makes broad growth based on these films unlikely. The newer vesicular and diazo films are lower in cost, and perfectly acceptable for many uses, but there has not as yet been suffi- cient testing to guarantee their use for archival storage. Increased use of these films for document storage in libraries is thus retarded [37]. The introduction of "ultrafiche," which is essentially microfiche prepared with high reduction ratios (in the 90X to 250X range), has further confound- ed the problems with microforms. Much of the development of ultrafiche has been outside the mainstream of micrographics, since it typically uses nontra- ditional technologies for part of the processing. The formats and reduction ratios employed vary even more widely for ultrafiche than microfiche, and they show no signs of converging on even a few standards. The use of ultra- fiche has primarily been to fulfill special system requirements such as credit card verification, bank signature card verification, large catalogs of industrial parts, and telephone directory assistance lookups; these are likely to continue to be the primary areas of future use. The special library collections which have been distributed on ultrafiche, like the Encyclopaedia Britannica's Li- braty of American Civilization, are expensive and have not been marketing suc- cesses [38]. Aside from their importance to libraries for document collections, micro- forms are finding more frequent applications for internal library processes as well. The development of computer output microfilm (COM) has provided a very convenient way to generate some large internal library files on roll microfilm or on microfiche from a computer-based master file. Files such as card catalogs and union lists are being found more frequently on COM- produced microforms because frequently updated files such as these can be maintained by a computer system and periodically regenerated in their en- tirety for use by library patrons and librarians. As libraries move to more computer-based techniques for internal processing and control, the use of COM files to replace conventional paper and card files will increase. According to a recent report on the microfilm industry by Frost and Sulli- van, "the future of the microfilm industry will depend largely on how well COM (computer output microfilm) penetrates the marketplace" [351. The COM proportion of the total microfilm industry will reach 37 percent by
300 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS 1985, according to the report. There are now an estimated 2,500 to 2,800 COM installations worldwide, at an average cost of $80,000 per installation, and the market is expected to grow at 15 to 30 percent per year [39]. Advanc- es in computer technology have provided increased flexibility in COM de- vices for varied formats and reduction ratios and for improved control. While the microfilm industry, and especially the COM-related segment, is expected to continue growing, microfilm has become less attractive for storage of large dynamic files because the cost of electronic storage media continues to plum- met. A glimmer of hope for the use of microfilm for certain kinds of dynamic files is represented by the development of updatable microfilm systems. Three companies have recently introduced updatable microfilm systems: 3M will market the MicrOvonic File developed by Energy Conversion Devices, Inc.; A. B. Dick and Scott Graphics, Inc., have formed a joint venture called A. B. Dick/Scott to develop and market a system; and General Electric and Bell and Howell have formed a company called Microx Corporation to develop and market yet a third system. At least two other companies, Xerox Corpora- tion and Eastman Kodak Corporation, are also experimenting with these techniques [401. Rather than actually changing images on the microfiche, these systems permit the later addition of new images to the same microfiche in frames left blank when the fiche was first exposed. A. B. Dick/Scott has already sold several machines to the U.S. Army for the creation and mainte- nance of personnel records [41]. The technology looks promising, but it is too early yet to predict its success for widespread implementation. In addition to its possible use for digital storage, videodisk can be used for storage and retrieval of documents in image form. The resolution of videodisk images will not be adequate for displaying a full normal-sized printed page on one frame because of the limitations of standard TV circuitry, but por- tions of a page could be displayed with acceptable quality. The spread of this technology for document image distribution depends completely on the suc- cess of videodisks in the home entertainment market, since their success there would lower costs by providing a large volume market. Software As the hardware components of information systems become cheaper and a wider variety becomes available, and as system complexity increases to serve a wider range of functions, the software component becomes more important. The software component of computer systems has become the dominant cost factor in system construction. One fairly recent estimate [42] is that software will be 85 percent of the total system development costs by 1985. This cost is exceeded only by the much larger expense of information analysis and data entry for building the files of a large scale information system. In the past, there have been four factors included in the "software cost." These are: (1) the overall information system design; (2) the design of the
TECHNOLOGICAL FOUNDATIONS 301 data structures and files; (3) the design of the detailed information flow; and (4) the design, coding, and testing of computer programs for the processing functions and control implied by the other three. It is gradually being recog- nized that the fourth, computer program implementation, while still critical- ly important, is only the last step in the process and cannot be done well unless the other three have been done well and unless the detailed task of computer programming has not been started prematurely. New techniques for organizing the programming task, such as structured programmning [431, are improving our ability to produce relatively error-free programs and test them effectively. Extension of this strategy to overall structured system design is also gaining acceptance [44]. However, as the system design must deal increasingly with human interfaces and interorganizational problems, and not merely hardware and software components, the complexity and difficul- ties compound. Much of the cost usually attributed to "software" is due to these complexities and difficulties. Although many programming languages are available for computer pro- gram implementation, basic assembly level programming continues to be advantageous, especially where programs are run repeatedly and efficiency is important. The increasing use of minicomputers also tends to require that the appropriate machine language be used since the language facilities available for minicomputers are more limited than for large computers. Compiler-level programming languages continue to be important for ease of implementation and change, but assembly language programming is far from dead and is now used more than had previously been predicted. Also related to software are the requirements for extended character sets. Ideally, we would wish not to be limited by the character set that can be input, processed, and displayed. But in the past practical considerations have constricted the range of choice, particularly for output terminals and the software that controls them. These limitations are decreasing and more of the potential that had been built into the input and data base segments of some information systems can now be more fully used for output. For new major systems of any permanence a design having the potential for a very large character set (such as the extended ISO sets) should be used. For information systems, the most important software component is data management. The requirements for data forms, file designs, and access paths to the information content for bibliographic systems are the most demanding requirements of all computerized information systems now being implement- ed. Only the handling of the full graphics of primary publications is more complex, and library and information systems in the future will have to handle graphics as well. There are many data elements which are highly variable in size and which are not present in all records. The files of a major system can become very large and, typically, multiple access routes are need- ed to support various functions. Besides the basic software, means are also needed to support the administration of the files; these include standardiza-
302 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS tion of definitions, reorganization when required, protection from loss or damage, and recovery from a variety of problem situations. For most bibliographic systems now operating, the software for the data management and access functions has been specially designed for the purpose because nothing adequate was available when the systems were designed. In the last few years quite a number of so-called data base management software packages have been marketed, but only a very few of them are even close to what is needed.3 Most of them were intended for application to various business problems and for much smaller files than in bibliographic systems. Anyone initiating the design of bibliographic systems now should examine what is available, but should not expect to find a complete solution. Major data base management software packages now cost in excess of $100,000 and run on large computers. Operating systems for large computers have improved greatly over the past several years. The trend toward virtual memory machines has considerably simplified the design and operation of complex information systems enabling a mix of on-line and batch processing operations to be run concurrently. They still have their limits, however, and care must be used to keep the paging to and from real memory within reasonable bounds and to stay below saturation points on the CPU and channels if good responsiveness for on-line operations is to be maintained. In contrast to 1970-71, basic operating systems for minicomputers are now available and it is no longer necessary to start entirely from scratch in build- ing the software for them. However, producing that software for minicomput- ers still requires considerable specialization. Software for the microcomputers in intelligent terminals is about at the place software for minicomputers was several years ago and only rudimentary development support is available. As various local systems and cooperative networks seek to establish inter- communication in order to operate on a national, multidiscipline, multicom- munity basis, the network software necessary to make the interconnection is essential. Fundamental for the design of this software are network protocols for routing, billing, and target facility action. The National Commission on Library and Information Science (NCLIS) has appointed a task force to develop a protocol for computer-to-computer transmission of library-related data among various "bibliographic utilities" [45]. Software for searching bibliographic files, either in libraries or through various information service centers, has been available for many years. The basic techniques now used originated in the early 1960s, although there has 3. More than twenty such systems were surveyed at the National Comparative Data Base Management Systems Conference in Los Angeles, May 19-21, 1976, but few of them were even close to being applicable to bibliographic systems and none was fully adequate for all such purposes.
TECHNOLOGICAL FOUNDATIONS 303 been extensive development to turn them into effective service systems. As the work in libraries turns more to the searching of computer files for subject access, search software will become of much more importance for the library community. However, present search systems grew from relatively simple beginnings, and extensions or new approaches are needed to improve the user interface in support of question coding and evaluation of retrievals. For good retrieval performance, extensive knowledge of the subject matter and the practices used in building the information file are now necessary. In the last five to six years, the software package industry has grown rap- idly. Unfortunately, for bibliographic systems, most of these are aimed at the general business market and only a few are directly applicable. System Environment: Intra- and Intercommunity Considerations Most of this assessment so far has dealt with the "nuts and bolts" of informa- tion technology from which bibliographic control systems can be put togeth- er. In determining how to assemble those parts to gain the maximum benefit for all parties, one must consider the interrelationships among the several communities concerned with bibliographic control. The forces for change in bibliographic control and in the mechanisms by which bibliographic control is carried out are those of economics and timeli- ness in organizing the large volume of recorded knowledge in the modern world. To be successful in meeting the challenges presented, repetitive and redundant human effort must be eliminated and mechanized "bridges" must be built for transfer of information within and among the several commu- nities involved: publishers, abstracting and indexing (A and I) services, li- braries, other information service activities, and information users. There has been a tendency for each of these functional communities to act indepen- dently and sometimes in conflict with the others. Considerable progress has been made within each of these communities over the last ten years in ap- plying information technology. For example, in the library community there are several large local systems, several networks, and several cooperative ven- tures of various types in effective operation. A recent report lists twenty-five networks using varying degrees of automation or preparing for future auto- mation. Several of them use OCLC facilities 146]. However, the interfaces among the several communities are far from being as effective as they could be and as they must be for effective service to information users. There is little mechanization of basic data capture in pub- lication processes that can be passed on to libraries and abstracting and indexing services. Item identification and cataloging practices have not been sufficiently harmonized among these communities. With a few exceptions, there has been little exploitation of search techniques for subject access such
304 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS as developed in the information service community to replace certain catalog use functions in libraries. Effective arrangements have not been made to assure primary document availability to users for all documents abstracted and indexed by the A and I community. These intercommunity interfaces currently present the greatest challenge to those building bibliographic control systems. In order to achieve useful mechanized linkages among the various activities, three areas of agreement must be achieved: (1) appropriate standards for the mechanics of information interchange, (2) appropriate standards for bibliographic analysis and index- ing, and (3) viable administrative agreements for exchanging data, docu- ments, and services of mutual value on a quid pro quo basis or by market mechanisms. Design and Implementation Considerations The designer of bibliographic control systems faces an open-ended problem in two respects. Designs must be adaptable if they are to operate successfully across institutional and community boundaries, and they must be readily extendable for future purposes. In short, systems must be designed for change. It would be nice to be able to "start over" without the limitations that are imposed by history or by the shortcomings of the technology that existed when the current generation of information systems were conceived. Howev- er, that is not realistic now, nor will it ever be realistic. The designer cannot abandon the past, but rather must interface effectively with it while handling the present and working toward the future. A key aspect of design in a multiinstitutional, multicommunity environ- ment is the set of standards and common practices that provide the guidance for achieving intersystem compatibility [47]. While it would be inappropriate to treat that subject in depth in this discussion, the following key design principles related to standards and common practices from the perspective of information technology are noted. Standards, whether formal or de facto, must deal with information con- tent-its control by authority files, its representation, and its structure-in order for automatic processes to interpret properly the stream of information they handle. Dealing with the mechanical externals alone is not sufficient. In fact, there may be a variety of detailed ways of coding information that provide equivalent representations of information content and lead to no ambiguity. Automatic processes can be used for transformations among those forms, preserving compatibility at required interfaces and permitting the evo- lution of various systems or parts of systems with fewer constraints on interre- lated sequences of changes. Of course, the greater the homogeneity in me- chanical details, the simpler the intersystem operation, while the consequent avoidance of transformation costs makes it more economical. However, as
TECHNOLOGICAL FOUNDATIONS 305 existing systems evolve and grow together, mechanical transformation for compatibility will be necessary until the appropriate standards are uniformly observed. Moreover, as technology improves, the cost of compatibility-pre- serving conversions becomes less. Because the technology is so powerful now and is still improving, designers should not skimp on machine processes; they will get cheaper. Human func- tions, however, will get more expensive. System designs should be based on available levels of detail for information content, not specific current levels of technological development for input and output. Implementation must occur gradually while maintaining current opera- tions, and system implementors must expect to modify system details as oper- ational experience is gained. The total collection of systems that must work together is too large, interrelationships are too complex, and rates of progress too dissimilar for major changes in basic techniques and facilities to occur universally at the same time. Conclusions and Outlook As in earlier assessments, the components are available for building a strong technological foundation for bibliographic control systems. As contrasted to the 1970-71 period, there are now many examples which show that effective use of information technology has matured in library systems. These include, among many others, the achievements at the Ohio College Library Center, the Library of Congress, Stanford (BALLOTS), the University of Chicago, and the Ohio State University. Much challenge remains to bring these many separate operations and local networks together and to extend cooperative interfaces outside the bounds of the library community alone. The improved availability and costs of these information technology com- ponents will be due more to concurrent developments in general computeri- zation in society at large, such as office automation, electronic consumer products, banking, and electronic funds transfer, than to any attempt of the bibliographic community to influence hardware or software suppliers. The bibliographic community should anticipate the technology likely to become available and be prepared to exploit it. Basic technological areas in need of continued research and development are: (1) mass storage devices; (2) data access and management software, especially for operation on minicomputers and in networks; (3) handling of preparation, display, and search of intermixed text and graphics; (4) parti- tioning of functions in a distributed system among levels of processors and files; (5) internetwork operations; and (6) user-system interfaces, such as search software and aids.
306 RONALD L. WIGINGTON AND CHARLES N. COSTAKOS REFERENCES 1. National Academy of Sciences, Computer Science and Enginecring Board, Information Systems Panel. Libraries and Information Technology: A National System Challenge; A Report to the Council on Library Resources. Washington, D.C.: National Academy of Sciences, 1972. 2. Endicott, Lucian J., Jr., and Huyck, Peter H. "De Ludi Natura Liber Secundus." Datama- tion 17 (December 1, 1971): 32-36. 3. "New Computer 5-10 Times Faster." Columbus Dispatch (September 23, 1976). 4. Allan, Roger. "Components: Microprocessors Galore." IEEE Spectrum 13 January 1976): 50-56. 5. Mennie, Don. "Consumer Electronics: Fun and Function." IEEE Spectrum 13 (January 1976): 84-86. 6. Kaplan, Gadi. "Industrial Electronics: To Boost Productivity." IEEE Spectrum 13 (January 1976): 87-90. 7. "Minicomputers Challenge the Big Machines." Business Week (April 26, 1976), pp. 58-63. 8. Denning, Peter J. "Third Generation Computer Systems." Computing Surveys 3 (December 1971): 175-216. 9. Parmelee, R. P.; Peterson, T. I.; Tillman, C. C.; and Hatfield, D. J. "Virtual Storage and Virtual Machine Concepts." IBM Systems Journal 11 (1972): 99-130. 10. "Advances in Storage Technology for Micros and Minis." Computer 9 (March 1976): 6-15. 11. Salzer, John M. "Bubble Memories: Where Do We Stand?" Computer 9 (March 1976): 36- 41. 12. Allan, Roger. "Semiconductor Memories." IEEE Spectrum 12 (August 1975): 40-45. 13. Mattson, Donald. "Understanding Media." Computer Decisions 8 (March 1976): 44-46. 14. Boer, Garret L. A Look at Mass Storage. Livermore, Calif.: Lawrence Livermore Laboratory, University of California, 1975. 15. Mennie, Don. "Television on a Silver Platter." IEEE Spectrum 12 (August 1975): 34-39. 16. Broadbent, Kent D. "A Review of the MCA Disco-Vision System." Paper presented at the 115th SMPTE Technical Conference and Equipment Exhibit, Los Angeles, April 26, 1974. Available from MCA Laboratories. 17. Salzman, Roy D. "The Computer Terminal Industry: A Forecast." Datamation 21 (Novem- ber 1975): 46-50. 18. Reagan, Fonnic H., Jr. "Datapro Explodes Some Data Entry Myths." Computerworld (Octo- ber 30, 1974), pp. 23-24. 19. Griffith, Arnold K. "From Gutenberg to GRAFIX-I: New Directions in OCR." Journal of Micrographics 9 (November 1975): 81-89. 20. Fenaughty, Alfred L. "Demand Printing: A Revolution in Publishing." Journal of Micro- graphics 8 (March 1976): 201-6. 21. Kurzweil, Raymond. "The Kurzweil Reading Machine: A Technical Overview." Paper presented at the Annual Meeting of the American Association for the Advancement of Science Symposium on Science, Technology, and the Handicapped, February 19, 1976. 22. Yasaki, Edward K., Sr. "Bar Codes for Data Entry." Datamation 21 (May 1975): 63-68. 23. McEnroe, P. V.; Huth, H. T.; Moore, E. A.; and Morris, W. W., III. "Overview of the Supermarket System and the Retail Store System." IBM SystemsJournal 14, no. 1 (1975): 3- 15. 24. "The Office of the Future." Business Week (June 30, 1975), pp. 48 ff. 25. Arthur D. Little, Inc. A Survey of Technologies Applicable to Electronic Mail Handling Systems. Cambridge, Mass.: Arthur D. Little, Inc., 1974. NTIS report no. PB 252 821. 26. Frank, Ronald A. "Users Told to Expect Few Cost/Performance Benefits." Computerworld (March 29, 1976), p. 25. 27. Moulton, Peter D. "Datran's Datadial and Telenet's VAN Service: A Cost Comparison." Modern Data 9 (March 1976): 32-35.
TECHNOLOGICAL FOUNDATIONS 307 28. Murray, Chris. "Tests Aimed at Fiber Optics System." Chemical and Engineenrng News 54 (July 26, 1976): 17-18. 29. Mennic, Don. "Communications: Electronics and Optics, Too." IEEE Spectrum 13 (January 1976): 57-61. 30. Hartwig, Glenn. "What the New Data Satellite Will Offcr." Data Communications 5 (March! April 1976): 45-48. 31. Abramson, Norman, and Cacciamani, Eugene R., Jr. "Satellites: Not Just a Big Cable in the Sky." IEEE Spectrum 12 (September 1975): 36-40. 32. Lennemann, Eckart, and Sakmann, Walter. "Impact Line Printing: Improvemcnt of a Proven Technology." Computer 8 (Scptember 1975): 16-27. 33. Falk, Howard. "Computers: Poised for Progress." IEEE Spedrum 13 (January 1976): 4449. 34. "More Printers in Use." Data Communications (March/April 1976), p. 16. 35. "Outlook Hazy as Memory Prices Drop." Datamation 22 (June 1976): 148. 36. Bagg, Thomas C. "A Technological Review: The Future of Microimagery in the Library." Drexel Libraty Quarterly 11 (October 1976): 66-74. 37. Dranov, Paula. Microfilm The Librarians' View, 1976-77. White Plains, N.Y.: Knowledge Industry Publications, 1976. 38. Grieder, E. M. "Ultrafiche Libraries: A Librarian's View." Microform Review 1 (April 1972): 85-100. 39. Axner, David H. "COM and Knowledgc." Computer Decisions 8 (May 1976): 68. 40. "The Race for a New Market in Microfilm." Business Week (April 5, 1976), pp. 44 ff. 41. Lee, L. S.; Dexter, C. E.; and Legg, R. D. Proposed Microform System for Administration of HQDA Mditaly Personnel Records: Final Report. Alexandria, Va.: Army Task Force, 1975. NTIS report no. AD-A012 340. 42. Boehm, Barry W. "Software and Its Impact: A Quantitative Assessment." Datamation 19 (May 1973): 48-59. 43. Baker, F. T. "Chief Programmer Team Management of Production Programming." IBM Systems Journal 11 (January 1972): 56-73. 44. Yourdon, Edward, and Constantine, Larry L. Structured Design. New York: Yourdon, Inc., 1975. 45. Libraty of Congress Bulletin 35 (August 1976): 453. 46. Martin, Susan K. Library Networks, 1976-77. White Plains, N.Y.: Knowledge Industries Pub- lications, 1976. 47. Wigington, Ronald L., and Wood, James L. "Standardization Requirements of a National Program for Information Transfer." Library Trends 18 (April 1970): 432-37.