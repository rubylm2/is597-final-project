The models, manuals and published methods of evaluating library collections have both grown and proliferated in the last fifteen years. From the Research Libraries Group (RLG) Conspectus methodology to the Pacific Northwest methodology to the amalgam that is now the OCLC/WLN Conspectus methodology, librarians have been given copious guidance as to possible methods for looking at collections and a new and much more illustrative set of definitions of collection levels. However, once the data are collected, and sometimes even while the collection of data is going on, many librarians express uncertainty about the validity of the data, the accuracy of their assignment of levels, and even the basis upon which the levels are assigned. Am I considering the universe of publication, the research and teaching needs of my institution, or my collection as compared to institutions of a similar size or rank? As benchmarking and outcomes assessment become standard requirements for program reviews in academic institutions, library collection evaluation must assist our institutions by using methods as they are intended and by understanding the processes involved in arriving at decisions about the collection. When we have a better understanding of the premises, methods, and limitations of the conspectus method, and of the inevitable role of uncertainty and the search for confidence always present in decision making, not only will our assessments be better informed and our colleagues more confident, but more and more successful collection partnerships and alliances, within and between institutions, may result. Research Methodology The researchers posted a call on listservs such as COLLDV‐L, ACQNET, and AUTOCAT for volunteers and asked for recommendations from people who were actively performing evaluations in order to assemble a cohort of persons who could speak knowledgeably on the topic of the decision‐making process in conspectus work. The universe of people who have done extensive work in this area, the researchers found, is quite small. In the initial request to the listservs, the researchers sought only those with experience doing a conspectus and urged respondents who had trained conspectus users to respond. The results of these requests were not satisfying, and the researchers consulted members of the Western Library Network (WLN) users group and those who were prominent in the literature to identify appropriate subjects for the research. Ultimately, the researchers interviewed thirteen people who were experts on leading conspectus evaluations and, in particular, on the decision‐making processes involved in these evaluations (the survey questions are included in the appendix). The respondents came from a variety of types of schools—large state academic institutions, large private universities, two public libraries, and medium‐size academic libraries. There were no respondents from small private academic libraries. Three of the respondents had been instructors in conspectus methodologies and therefore had experience from several types of conspectus projects. All had done at least one conspectus project and most had done several. For purposes of the interview, respondents were asked to concentrate on one project in their answers. This article presents the results of these qualitative interviews and advice for practicing librarians who are evaluating or considering evaluating their collections. The study asked how decisions were made in the assignment of collecting levels by librarians using a variety of conspectus methodologies and asked these librarians to evaluate both the worth of the methodologies to the users of these methodologies and the difficulties encountered in using those methodologies to rank collections. In characterizing the decision‐making methods used by the respondents, the researchers reviewed the decision literature to find decision theories that might illuminate the actual use of the conspectus methodologies. For the purposes of this study, the researchers used the definition of decision making used by Henry Mintzberg, Duru Raisinghani, and Andre Theoret: a decision is a specific commitment to action, and the decision process a set of actions and dynamic factors [1]. Literature Review Introduction/Background In the WLN Collection Assessment Manual (4th ed.), Nancy Powell and Mary Bushing define “conspectus” as “an organizing process of systematically analyzing and describing a library collection using standardized definitions” [2, p. 5]. Others call conspectus a set of standard codes, a tool, a means to an end, a survey, overview, or outline to use for systematic assessment. All agree that a conspectus methodology or tool attempts both to be systematic and provide standard definitions [2, 3, 4, 5]. The development of conspectus measures rests on the realities of the late twentieth century: the publishing explosion, pricing trends, declining levels of financial support for higher education, the “race for space” in libraries, the problem of preservation, and the proliferation of formats. The charge to the RLG, over twenty years ago, was to create a tool by which libraries could evaluate their own collections and communicate those findings to aid in resource‐sharing activities. One of the conspectus method's strengths as an evaluation tool was its methodological flexibility. Another was the tool’s acknowledgement of the worth of librarians’ experiential knowledge of their own collections [5]. Paradoxically, these strengths have been the basis of some of the criticisms of conspectus methodology as it was judged using tests of statistical soundness rather than measures of the worth of the experience itself, that is, its usefulness. The conspectus methodology was intended to allow evaluations of the collection’s suitability for the mission of the institution. In so doing, it cannot truly be subjected to tests of validity or reliability, as the environment in which it is framed informs the conspectus process. The initial intended use of the conspectus was as an aid to cooperative collection [6], but the conspectus methodology has expanded to serve as, among other things, a tool for interlibrary loan planning, a public document for the use of the institution, a budgetary or public relations tool, a framework upon which to structure collection development policy, a spur for fundraising, a training tool [7], a source of accreditation information, a means of building librarian confidence [6], and the basis for distributing preservation responsibility [8]. Anthony Ferguson and Ross Atkinson both note that simply doing the conspectus is useful; the process of making those assessments has intrinsic worth to the institution [8, 9]. Others have disputed these uses, although, as Atkinson points out, the decision to use collaborative data and to assess a collection using a conspectus is itself the start of cooperation [8]. Library conspectors seem generally to see conspectus work as a source of management information for further decision making. The RLG framework, the National Collection Inventory Project (NCIP, often called the National Title Count), the Pacific Northwest Conspectus Project/WLN, and other variations on conspectus methods have been successfully used in academic libraries of varying size and in public libraries as well, as reported by both Mary Bushing [4] and Sally Loken [6], who have written a great deal about the best ways to approach this daunting task, and the elements that might make such a project a success, such as training, communication, leadership elements, and so on. This use for purposes not originally intended, and by public and school libraries, is seen by some as evidence of the hardiness of the conspectus and its flexibility [3], while others feel that such use is not intended or well supported, with tools that adapted the Library of Congress (LC) classes in the RLG conspectus to Dewey categories being particularly suspect [10]. Criticism of the Model While much literature reports on the uses of conspectus and its effects on libraries’ collections and staff, since shortly after the first publication of conspectus manuals there has been a vocal group of critics of the conspectus tools and their uses. Even Bushing, instrumental in the development of conspectus methods, concurs that the success of a conspectus project hinges on the essentially indefinable concept of the “informed and experienced librarian” [4], and Virgil Blake and Renee Tjoumas agree that the validity of conclusions drawn depends almost precariously on the subject expertise of librarians, who may or may not receive some kind of conspectus training. A standard of what constitutes an informed and experienced librarian is impossible to derive, as is a standard way to compare a given collection to the theoretical universe of publication. These concepts and comparisons imply some acknowledged standards that are not explicitly defined, or definable, although recent revisions to the definitions have solidified attempts to operationalize these standards. As Blake and Tjoumas note, even if standardized conspectus training were made widely available (and were used) in academic libraries, that is still not a guarantee that ratings would be consistently applied by those trained [7]. Critics of the conspectus methods have used criteria of scientific validity as a basis for criticism of the conspectus method. However, the usual test of scientific method, replicability, is not available to a conspectus study since it results in a snapshot in time. Therefore, questions of scientific method as applied to such studies are by definition inapplicable. The methodology, or more specifically, the exact meaning of the numeric designators assigned, and the operational definitions of the numbers assigned, have been widely discussed. David Henige [11, 12], argued that, as the numeric designations were not ordinal numbers or even interval data, any research which purported to compare institutions would be spurious, and no correlations could be possible. If all that is true, and the tool is not useful as a way to make quantifiable judgments about collections, then it would follow that the evaluation of a collection can only be a qualitative judgment, perhaps not even measurable, much less amenable to statistical concepts of reliability, internal or external validity, or correlation [11]. Effort and statistical uses were, said Henige, fatuous [12]. Donald G. Davis and E. Stewart Saunders agree that librarians may have a rather ill‐supported faith in numbers, no matter how poorly used. They agree with Henige that the collection level numeric designations cannot be correctly used to generate meaningful statistics because the levels do not represent levels of magnitude. A research‐level collection cannot be thought to be equal, for example, to an instruction‐level collection times four. Or, a level‐3 collection in one subject area may not be equivalent to a level‐3 collection in another subject. If the unit of measurement is not based on sheer quantity intervals, then any correlations would be spurious, according to Davis and Saunders. At the same time, comparisons between and within institutions are also problematic [13]. A level‐3 collection at one institution may not be similar to a level‐3 collection at another institution, as Scot Siverson’s [14] and Daniel Dorner’s [15] research has found. The levels, as stated in many manuals, overlap and are defined somewhat differently from collection to collection and from rater to rater. However, Davis and Saunder contend that simply because the levels assigned in using conspectus methods cannot be used to generate statistics does not mean that the conspectus methods and levels have no worth at all. Because the judgment of collection level is consensual and culturally and experientially defined, a conspectus accrues meaning (thus, worth) through cumulative use. The tool gains meaning as a community uses it. The numeric values are not interval data, indeed, perhaps not measurable at all, at least not as set values indicating lesser and greater. Rather, the numeric values are predictive or are statements of probability. Therefore, the fact that a level‐3 collection has no measurable relationship to a level‐2 collection is less important than the fact that a level‐3 collection is likely to be rated a three across raters as those raters gain experience [14]. Lee Roy Beach views these consensual meaning‐making processes using what he terms the lens model of decision making [16, p. 36] and describes a method, which can be statistically represented, of using the characteristics of that being decided upon (in this case the definitions) to create inferences or predictors of results (in this case levels of collection). Though there appears to be no research applying the lens model to decisions made in determining collection levels, further research doing so could help conspectors understand the conspectus process. Siverson contends that the question of external validity is moot; the conspectus offers descriptive analysis, which by definition cannot have (and does not need) external validity. Deriving collection levels is not a way to amass data for quantitative analysis but rather is a means by which one organizes and presents information about the collection [14]. Dorner considers the assigned conspectus levels to be standardized expressions of informed opinion [15], and Peter Hernon, responding to Henige’s charges, sees as irrelevant the evaluation of conspectus methods using the criteria one uses in evaluating a research tool [17]. It is a comparison, in sum, of apples and textbooks. Indeed, this view is consistent with the uses that have been made of the conspectus to good effect throughout the twenty or more years of its existence as a methodology. Other fundamental questions regarding the use and interpretation of conspectus levels have been raised in regard to the relationship between number of volumes held and the quality of a collection; the conspectus values seem to imply that, for libraries, more is always better. Siverson calls this an orthodoxy for librarianship and a flawed assumption, seeming to indicate that all items contribute equally to the overall quality of a collection [14]. The conspectus definitions, with their emphasis on the quantifiable, have undeniably added to this quantitative emphasis. However, the revision of the definitions, beginning with the Pacific Northwest Conspectus and most recently with the revisions of the definitions sponsored by the Association of Research Libraries (ARL), RLG, and WLN, in 1996–97, have made the tools less quantitative [10]. Dorner also noted a biasing effect of the numbers, resulting in a tendency for most collections to be assigned values falling in the middle of the range. Interestingly, Dorner’s study correlating Current Collecting Intensity (CCI) rankings and number of holdings in psychology in Canadian academic libraries found a much stronger, positive significant relationship between budget and CCI than between number of holdings and CCI [15]. Henige also sees the conspectus methods to be particularly susceptible to deceptive use by conspectors, with local political pressures possibly resulting in assigning inflated values [12]. In general, the criticisms of the methods and structures of conspectus work do seem to presume that the data gained in conspectus work is research data and that the methods used should be evaluated as though they were the methods of a statistical researcher. If, in fact, the data produced is not amenable to more traditional research, is producing management data (data to inform management decisions) or action research data (data which produces actions that enhance the collection), or is not so much producing data as stimulating relationships, describing, communicating, or any other of the number of uses one might see for the resulting values and descriptions, then the librarian will ultimately decide whether the effort produces anything of value to the institution, be it data, relationships, communication, knowledge, or wisdom. As Atkinson wrote, “bibliography has always been a mutable, obscure, ambiguous messy pursuit, and those of us who practice it must accept that the instruments we create to help us manage our work will necessarily reflect those qualities” [9, p. 354]. In short, Atkinson sees the conspectus as an art rather than a science, and it should be evaluated as such. Decision Literature A look at the decision literature offers some enlightenment on the issues raised by the critics of conspectus decision making. According to Henry Mintzberg, Duru Raisinghani, and Andre Theoret, a decision is a specific commitment to action, and the decision process a set of actions and dynamic factors. Cognitive and social psychologists, and management and political scientists have all addressed the processes of decision making, going back to John Dewey and before. Mintzberg’s research in the 1970s examined instances of what he termed “strategic decisions,” defined as such for their novelty, the sheer amount of resources and number of people involved, the complexity of the processes, and the ambiguous paths decision makers might take to reach a decision. Examining these decisions, Mintzberg and others describe a decision‐making process that involves extensive searching for information. Organizational or strategic decisions such as those made in the course of assessing a collection or collections using the methods and matrices of a conspectus are examples of such complex and murky decisions [1]. Discussions of decision making use a variety of decision models, including rule‐based theories and models, but the most frequently used is a variety of rational choice theory. Rational choice theory, as defined by Beach, presumes that the decision making has only one goal, that there is unlimited information, that the decision makers are able to use the information, and that the opportunities and consequences are known. In Beach’s opinion, “like normative theory in general, this description is so patently wrong, both conceptually and empirically, that it often serves as a foil for contrasting descriptions” [16, p. 126]. Pure rationality would result in the perfect decision because all information and all consequences would be known to the decision makers, thus eliminating all risk [18]. Decisions made in the course of a conspectus project could also be viewed using a choice‐based model of decision making, that of limited rationality, governed by a logic of consequence. Decision makers make choices among alternatives by evaluating the consequences of likely choices. In this model, the future is assumed to be understood—within limits. Instead of calculating the best possible action out of every alternative (which implies complete rationality—unheard of in any organizational setting) decision makers seek to make a decision that is “good enough.” While rationality is an ideal, inconsistent goals within the group, divided attentions, and varying qualities and amounts of information (incomplete shelf lists, the inability to know well the universe of publication) constrain decision makers from being completely rational, despite the tendency to strive to be completely rational by gathering more and more data. The model of limited rationality emphasizes that decision making involves the estimation of risk and the limits of the environment that impact how accurate estimation of risk can be in a given setting. Decisions made in the course of assessing the collection using conspectus can also be conceptualized as either rule based or choice based, according to James G. March’s synthesis of decision research. The decision‐making process as a rule‐based process is grounded in a logic of appropriateness. Rules follow from one’s role, and one’s role as decision maker springs from one’s job identity, age cohort, and so on. That one person may have several roles is not surprising. That one is several decision makers in conspectus work, and thus must balance out several identities and rules when making a decision, makes the decisions much more difficult. The body of socially constructed rules, whether explicit or not, can seem overwhelming [18]. If one sees decision making as governed by rules (thus roles), decision making in conspectus projects is made difficult in two ways, the contractual nature of ones identity as a librarian and conspector, and the multiple possible identities that battle within us. Who are we when we assess the collections, and which rules do we follow? These conflicting identities (e.g., advocate for the growth of the collection, liaison to teaching faculty, subject expert with a reputation to uphold, guardian of the library and the university’s reputation, fund‐raiser) are further complicated by the effect of competence, or what March calls the “competency trap,” namely, that decision makers tend to adopt the role that has been most profitable in the past or the role they are most comfortable playing [18]. To paraphrase Newton’s Law of Motion, a body in success tends to seek success, that is, to apply the same logic, use the same methods to gather information, form similar coalitions. In fact, this inflexibility of role (and thus, reluctance to try on another role with its rules) can even be seen in decision makers after the decision is made. March notes, “this resilience of belief in the face of experience is an obvious feature of … political and religious faiths and equipment purchases” [18, p. 84]. Librarians need not seek to remove all inconsistencies in decision making. Indeed, these inconsistencies are part of the reality of professional decision making in a complex environment. Decision makers interpret experience in such a way as to conserve belief and tend to espouse simple theories of causation despite experiences which indicate the opposite. Many decision makers seek to remove all inconsistencies in the environment, in roles, and in decisions. However, inconsistent preferences, notes March, “are not pathologies. [Inconsistent data] are … facts of social, economic and political life” [18, p. 106]. In addition to the effect of conflicting roles, rules, and a general discomfort with ambiguity, conspectus decision making is further complicated by the fact that a conspectus project does not occur in solitude. There tends to be a leader; there is typically a team, even many teams. Most organizational decisions fall in the intersections between multiple decision makers, each pursuing his or her own concepts of his or her roles and rules, his or her understanding of the optimal course to take. There may even be disagreement about the goals of the assessment project and the ultimate point of using conspectus methodology and frameworks to assess the collections. Mintzberg’s discussion of organizational decision making describes the role of the manager in making complex decisions as that of the decision champion, controlling the timing of events and the perception of the group held by outside forces. The leader of the team must be willing to manipulate the group’s visibility and flexibility, monitoring the flow of information within the group and to the group from outside factions, protecting freedom of action. The most important role of a decision team leader is not to lead the decision making per se, but to manipulate the larger environment to give the team members freedom to act on its charge [1]. This may be doubly important in conspectus work, because, as March notes, group decision making relies on gathering information that is rarely innocent, that is, the decision relies more on the parties involved and methods used than the data itself [18]. Beach proposes a variant on the rules and roles approach to decision making that he calls naturalistic decision making. In naturalistic theory, decisions are descriptive rather than normative and include such models as the recognition model, which revolves around recognition of the situation, understanding a situation, serial evaluation of sets of solutions, and mental simulation. Narrative‐based decision models involve both a scenario model, in which scenarios are used to aid in decision making, and a story model, which is based on an “implicit theory of human behavior” [16, p. 130]. These models are less tied to statistical tools and are more reality based. Thus, in either the rule‐based view of decision making, the model of limited rationality, or the varieties of naturalistic theory, all of which might be employed to examine the decision making of conspectors, the central fact of the models is the discomfort with and need to control for uncertainty. March concludes that information tends to be gathered to sustain decisions already made rather than to inform future decisions, indeed, that the goal of organizational decision making is not to gain knowledge but to gain confidence. Mintzberg concurs, but does not see this as a negative attribute of decision making. Discussing the development of criteria used to make decisions, he reports that, in nearly all organizational decisions, criteria are developed implicitly as choices are made. Even if stated criteria are at hand prior to the decision‐making process, the confirmation process, during which decision makers take ownership of the decision, is a process of rationalizing the choice made: “the determination of criteria, in effect, follows the making of the choice” [1, p. 256]. As Patricia Phillips notes in her study of the decision‐making processes of Tennessee academic libraries selecting automation systems, “given some uncertainty, [we] seek more information than is required … beyond a certain point, more information does not improve the accuracy of decisions, but it does increase the decision‐maker’s confidence” [19, p. 19]. March also comments that “confidence increases with the amount of information processed, even though accuracy typically does not” [18, p. 40]. Other studies of decision making stress some of the same principles and precepts, leading to the conclusion that conspector decision making does not differ widely from other forms of organizational decision making [20, 21, 22, 23, 24]. Results In conspectus work, as in organizational decision making addressed in the literature, the researchers observed the effect of the organizational environment on the decision‐making ability of the evaluating librarian, the amount of risk taking involved, particularly in the assignment of levels, the librarians' confidence in their ability to make subject‐based decisions, and the effect of group decision making on librarians’ experience of conspectus decision making. Planning The respondents gave a number of pieces of advice for the librarian making collection decisions, no matter which methods are used. All thirteen advised librarians to determine what information, or end result, they want from the study before they begin. Most advised that a series of questions to be answered be set up before the study is designed. If the key question to be answered is the age of the collection, one method might be advisable; if the key question is serials, another conspectus method or tool might work better. When used with other measures, raw numbers of volumes are useful for evaluating strength in monographs in a subject and for comparing the collection against that of other institutions of similar size and mission. If the questions to be addressed are few, and the scope of the subject area or call number range is small, a conspectus study such as is discussed here might not be useful, and a more informal evaluation, stopping short of assigning collection levels, would be better suited to the task at hand. Respondents suggested knowing what the range of questions and answers are likely to be before designing the study. Another concern in planning collection analysis activities is the level of specificity at which the collection is to be evaluated. Both the WLN and the RLG conspectus levels are hierarchical, with studies available at the division level (e.g., language and literature), category level (American literature), and descriptor level (American postcolonial literature). Usually, the larger the collection examined, the more specific the study should be. The reason for this level of specificity is that a smaller collection will not have as many titles in very specific call number ranges. A college library, for example, might not have many titles in Icelandic literature and would be better served to study that collection at a more general level, perhaps Scandinavian literature or, even broader, Western European literature. One respondent advises librarians not working with a consortium (which may require study at the descriptor level) to start analysis at the broader category level. As the respondent noted, “just remember that the moment the decision is made to go to the descriptor level, the work is multiplied.” Most studies discussed in the interviews (nine out of the thirteen reported) seemed to concentrate effort on determining the current collection level, the strength of the present collection. If the studies conducted formed part of a full conspectus study, the collections acquisition commitment (AC) and a collection goal (GL) were also assessed. Acquisition commitment is determined by the budgetary commitment to the subject and the approval plans that cover the area. The collection goal is determined most often by the level of the courses taught in the subject and the depth and intensity of research being done in the area. Most (twelve of the thirteen) of the evaluation studies reported used the conspectus levels, usually zero to five with subdivisions a, b, c, and so on, to indicate the strength of a collection, with zero being out of scope and five being a comprehensive collection (see the levels in the app.). One respondent suggested using only the whole numbers, without numerical subdivisions, unless the collection is large or the librarian is convinced that the subject level will be useful. This respondent urged librarians to think about the most efficient use of their time. Another respondent commented, “Attitude is important. If you don’t see the use [of greater specificity], don’t bother. If you see potential, go ahead.” Decision Making Who makes the decisions in conspectus work? The respondents used a variety of individual and group decision‐making models. All stated that, in their experience, librarians find this decision making very difficult, either because of a lack of confidence in the numbers or a perceived lack of knowledge of what makes a good collection. To counteract this trend, one respondent used a hierarchical method in which an individual librarian did the study and assigned the levels, which were then checked by a more experienced collection development librarian. Many respondents (seven out of thirteen reporting) used a team approach to lessen the collective lack of confidence in findings. Some used a method in which several librarians did the study and a group evaluated the result, while others used an individual librarian doing the study and a group evaluated the results. Sometimes the group assigns the levels. Most of the group studies used ground rules laid down initially by the librarians who designed the study. All of the respondents emphasized that levels are not set in stone. One respondent noted, “Make a decision and live with it. Then change it if necessary. This isn’t brain surgery.” Methods used for decision making, either by groups or individuals, were equally varied. One respondent suggested that librarians use a work sheet and standardize the type of information and format of information entered on the work sheets by all conspectors. She also suggested that the librarians begin the analysis assuming the collection to be at the minimal level and that they revise the collection level upward as the data indicate. Conspectors should be free to revise the level assigned throughout the process. “Do it quick and dirty, write it down, then test your impressions,” she advises. Another suggests that the most important information is the information in the note field, the observations that librarians make when doing the study. These can be a key indicator as to the level, which should be applied. The definitions provided by the formal conspectus methodologies, particularly the ones recently revised by RLG, ARL, and WLN (see the app.), were key to decision making, according to all of the respondents. One respondent suggests that, after the librarian has made a decision about whether to use categories or descriptors (to evaluate at the more general or more specific levels [a, b, c]), he should then use the definitions as “chain saw definitions.” He notes that “the levels are the lingua franca of collection development. For that, the language works. Carry around a pencil. Don’t wait for values. Do it in pencil! On the basis of this evidence, it’s a 2, 3, etc. In periodicals you can tell—you have just a few (that’s a 2), have a lot, that’s a 3 or 4. Most of us don’t have a 5 (comprehensive). We don’t often have a zero. You usually can lop off these two.” Another respondent asks that the librarian doing the study write three sentences that characterize the collection (two of the respondents used this method). Participants are limited to three descriptive statements. Then they eliminate the levels that do not apply and go back to the description of levels given in the conspectus tools to find a good match between data and level (most of the respondents advised that such elimination is the best method for getting to an accurate level). In sum, collection work is a process, not a product, notes one respondent who has been using conspectus methods to evaluate the same institution’s collection for ten years and has not yet finished. Another respondent sees conspectus methodology as one set of methods she can pick and choose from to make her case for resources. She wants to do this analysis as “quick and dirty” as possible because, she notes, like most written materials, the assessment is out‐of‐date as soon as it is finished. Confidence Lack of confidence, in participating librarians’ judgment, is a pervasive concern. The statistical concept of confidence is here closely related to an individual’s confidence. For example, an individual’s reliance on the assigned levels as true and accurate indications of the strength of the collection is closely related to that individual’s beliefs about the utility of quantitative data or measures to assess a collection. One respondent noted, “The collection levels are the best tools we have for understanding the collection, although they are not perfect.” One of the chief problems inherent in conspectus decision making is the reluctance of librarians to weed, exclude, or say no in collection activities. One respondent noted, “We librarians talk about teaching critical judgment all the time, but I can’t get librarians to exercise it. They are never willing to say no to anything. Collection assessment teaches many things, but critical judgment is probably the most important.” Advice from respondents who have managed large studies include such comments as: 1. Keep it simple and hopeful 2. Try to keep the anxiety levels as low as possible 3. To get buy‐in and make librarians confident in their decisions, there needs to be an understanding of what is wanted and what it will be used for 4. Keep lots of people involved and insure consistency in the application of the levels Most of the respondents advised librarians to trust their judgments. “You know more than you think you do,” one noted. Others advised bringing in faculty experts to offer their own assessments and thus give the evaluators confidence in their own assessments. One respondent told of a Springer Verlag math editor who came to campus to appraise the math collection. Librarians taught him the levels and gave him the conspectus work sheet. He determined that the collection was unexpectedly high—a four (research‐level) collection. Coincidentally, this is the level the librarian had assigned, but she assumed that she must have been wrong. Balance between Judgment and Quantitative Data Respondents emphasized repeatedly (five out of thirteen respondents) that librarian’s educated judgments tend to be valid. Notes one, “We are pretty accurate—if you have a liberal education, you ought to know what ought to be in a collection.” Often the qualitative hunch and the quantitative assessment tend to validate one another, one respondent observed. Another flatly stated that often the librarian does not need quantitative data, that observation and experience with the use of the collection are often enough. Indeed, one respondent advises librarians to begin conspectus work with a subjective judgment of the collection in mind: start with the conspectus’s definitions of collection levels, and often there will be no need for quantitative data. The respondent noted that this is particularly true with an assessment of journals in a collection because the dates held are not easily available from bibliographic records and because journals are often not classed or even cataloged. He cautions, however, not to “shoot from the hip.” If the conspector has a sense of the universe of publication, the decision to assign collection levels is made more quickly, and he or she needs less quantitative data. If the selector has little collection development background or experience in the field, he or she will need to do more quantitative study in order to become familiar with the field. Many librarians simply are uncomfortable with the collection assessment process, even with clear guidelines to follow. Though all of the respondents remarked that librarians basically know their collections, librarians express a degree of discomfort, which may come from lack of confidence in their judgment. However, librarians are frequently right, even when they do not check extensively. One respondent notes, “I have never changed a level after intensive checking,” as he finds that further assessment just verifies the level originally assigned. Most (ten out of twelve) respondents agreed that a combination of personal judgment and more empirical methods is best, and that, as one respondent observed, “You can spend the rest of your life doing this stuff.” One respondent advised setting the collection and goal level before examining the collection, to set the context of the study, and adding information on these goals as they entered the results of the quantitative studies. The goal and acquisition data served as a reference and explanation for the numbers. Others noted that the balance happens to the degree that the bibliographer is secure and recommended that librarians not strive for 100 percent confidence in their numbers. “If the measure is 1% off, the world will not stop,” observed one respondent. “To fuss and re‐verify is silly—you can always go back and change your mind.” Again and again respondents advised librarians to remember the purpose of the exercise. As one respondent put it, “It is not to make us look good. It is to describe and ultimately to justify asking for resources. It is important to put some reality into the exercise.” Conspectus Methodology Respondents were asked to talk about which methods worked and which did not in their conspectus or evaluation project. Their replies varied with their philosophies about conspectus work—some conduct exact and clearly conspectus‐based studies, and others combine assessment methods and conspectus methodologies to do studies that are conspectus based but that do not follow the methodology strictly. However, most had interesting things to say about the value and usefulness of each methodology—with widely differing and firmly held opinions. Formal or informal methodology.—Respondents used both informal and formal methodologies for their studies and reported widely differing purposes for their studies. Some assessed the entire collection; others assessed small areas. All of the librarians surveyed have many years of experience in collection assessment and have taught librarians to use the methods mentioned here. For the purpose of this study, the researchers asked the respondents to respond to our questions based on the one study they had most recently completed. There are three major formal conspectus methodologies in use today—the Pacific Northwest Project/WLN conspectus methodology (not used extensively at present), a more recent version of the Pacific Northwest, now called the WLN methodology, and the RLG methodology. The three major methodologies differ primarily in the numbering scheme that they use to determine levels and in the definitions used for each level. The RLG methodology uses a schema of A through E, with A as the highest level. The Pacific Northwest changed the levels to zero through five (with five as the highest level) and added subdivisions of a through c in the middle ranges. The WLN methodology grew out of the Pacific Northwest methodology and is notable primarily for the extensive revision of the definitions and the attention to such special areas as children’s literature, fiction, and preservation. Out of the thirteen respondents, seven had used the Pacific Northwest/WLN conspectus methodology, four had used the WLN methodology, four used the RLG methodology, and one used the National Title Count.3 Some created databases with acquisition, collection strength, and collection goal levels, and descriptions on work sheets for use with consortia. Others were more eclectic in their choice of methodologies and picked from each of the established conspectus methodologies. No matter what choice they made, most adapted the methodology to their local needs and mission. For example, one respondent assessed a science‐technology collection in a science academic library. For her study, list checking was the method of choice, and the study emphasizes currency in date ranges. The journal collection was a major focus. Another studied a land grant university and used the entire gamut of methods based on the WLN methodology and entered the data into the WLN conspectus software to be used by a consortium. Shelf scanning.—For both monographs and serials, respondents recommended going to the shelf and looking at the collection (also called shelf reading and shelf walking). Ten of the thirteen respondents used the method, and one described it as the “meat of the assessment process.” In a given call number range, shelf scanning can correct misconceptions about how many books were actually on the shelf, or their condition. Even the amount of dust on the tops of books can be illustrative. Age of the collection.—There was more division among respondents regarding the utility of gauging a collection’s age. Collection age was determined for a variety of reasons—some used it more for weeding than for assessment. Two respondents pointed out that the ideal collection would be of different ages for different subject areas—the average age of a useful humanities collection, for example, should logically be much older than that of a physics collection. One respondent pointed out that computing the average age is particularly important if you have a marginal or uneven collection. Others saw median age as a more important measure. Using the median age in assessment decisions prevented some of the statistical anomalies that can result from using the average age if the collection has a skewed distribution of publication or purchase dates. Also, modal age can more accurately reflect the state of a collection in certain areas such as computer science, where looking at clusters is useful. Several noted that the average age of most U.S. collections is thirty to forty years old, reflecting the buying patterns of the 1960s and 1970s, when money was plentiful and budgets at most academic institutions were growing. Another measure of collection age, currency, defined in the WLN conspectus methodology as the percent of the collection added in the last ten years, can be an important measure in assessing certain collections, such as computer science. Also, respondents who work in small libraries or with institutions founded more recently find age measures less helpful in collection assessment, particularly if they have used reprints or more current editions to supply some of their need for historical material. Core lists.—Several of the respondents used Books for College Libraries, both because it is a qualitative list and because it is automated and can be run against an online public access catalog. Some relied heavily on bibliographic lists, both serial and monographic. One respondent noted that bibliographic lists are especially important in science where serials are extremely important. Six respondents favored checking the collection against monographic lists as a more qualitative, less number‐based assessment method. Another respondent disagreed, especially when using the brief collection strength method, which uses OCLC holdings of a particular volume and expert opinion to establish a list of “best” books. He called it “the blind leading the blind.” Experts.—Most respondents were reluctant to use experts outside the library to assess collections. If they used experts, they used their own faculty most often (nine out of twelve respondents). Some said they used expert opinion only for verification after they finished their own project. Others advised using experts only when the picture is so unclear that the librarians really need help. One respondent warned that using faculty outside the library could turn into an off‐topic discussion of the library budget or the shortcomings of the LC system if the librarian is not careful to choose expert faculty well and describe the task clearly. Publishing data.—Opinions differed on the efficacy of using the universe of publication as a benchmark for assessment or conspectus work. Many respondents advised that librarians look at the universe of publications as expressed in published lists, such as Bowker’s Annual, Books in Print, Publishers Weekly, or vendor’s statistics. One respondent felt that looking at the universe of publication is “pie in the sky and not worth the time to presume your collection will in any way mirror the universe of publication.” Another felt that looking at the universe of publication is only useful when some attempt at comprehensiveness is part of the level’s definition. Others feel that some idea of the universe of publication is necessary to evaluate the relative size and comprehensiveness of the collection. One respondent warns against using only one indicator to measure the universe of publication, as it may not truly reflect different collections. Shelf list counts.—Nearly all of the respondents used shelf list counts. If it were possible in any way, they advised using automated shelf list counts, as manual counts are less exact. One problem is that shelf list counts are not very helpful with regard to serials, especially if titles are being counted. On the other hand, one respondent stated that she would never use shelf list counts, as her area is science and she feels that shelf list counts tell none of the story when it comes to scientific journal literature. However, others felt that shelf list counts in call number ranges, either the National Title Count or more specific ranges, can tell a story about the relative strengths and weaknesses within a collection and in comparison to other collections. Serial titles can be measured and compared, but cataloging practices in serials make it difficult to identify the active serials in the collection through a shelf list count. Volumes added, circulation data, ILL data.—Volumes added data are frequently used to determine the AC, and circulation data and interlibrary loan (ILL) data are used to determine user needs. Several respondents used this kind of data to determine a measure of collection growth. Others felt that a collection should be measured against national publishing data, not user needs. Much depends on the mission of the library being evaluated. A research library might, on the one hand, be more interested in measuring itself against the teaching and research activity in its own institution. On the other hand, some research libraries, particularly if they are the libraries of record for a region or state, have as part of their mission the responsibility for being the library of record in that area. Other respondents noted the need to consider user needs as a tool to adjust GL levels. One respondent advised looking at the red flags—small collections that are very heavily used, large collections that are never used. Several respondents mentioned the problem of browsing use of the collection—materials that are used and replaced on the shelf in the library. One or two respondents have used focus groups or informal user studies to examine use that is not easily measured by circulation or ILL data. Budgetary commitment.—The respondents examined a library’s collection budget only to assess the AC. Collection goals should be commensurate with acquisition commitment, and this requires some assessment and adjustment of both levels. If the goal of a collection is to collect at the research level and the acquisition commitment is at the minimal information level, there is a definite disconnect. Often the result of a conspectus study is a formal request to adjust the budgetary commitment for a particular subject or collection area. Local conditions.—Most of the respondents considered local conditions—courses taught, credit hours, college catalog listings, degrees awarded, and demographics—to establish goals for collections. Again, five respondents emphasized that national data should be used for collection strength goals. One respondent noted, “The basis for evaluating a collection is a comparison against the rest of the world, not a comparison of the collection against user’s needs. This comes later, and is a necessary use of the conspectus, but it does not affect [collection levels assigned]. Even if it [the collection] fulfills all of the expressed needs of the faculty, a 3a collection is still a 3a collection. It does not become a 4 just because it fills the expressed needs of a graduate program.” However, a 3a collection might well be the collection goal of that particular library collection. Some respondents excluded 100‐level courses when they assessed curricular needs, thinking that general education courses, such as those in math and English, could be supplied from the core collection. Most tended to look more closely at upper division and graduate courses, numbers of faculty, and so on. Benchmarking.—Again there was disagreement among the respondents about whether to compare library collections to other libraries’ collections. One respondent urged, “Describe, not compare.” Another noted that he was unable to find a school that he thought comparable to his. One respondent felt that he would only use benchmarking after he had finished his study so that the comparison would not influence his assignment of collection levels. Respondents who used benchmark comparisons in assigning collection levels felt that they are useful as a reality check. One respondent thought the reality check is needed; “Librarians always think the collection is worse than it is.” When using benchmarking, some respondents use the National Title Count for comparison; others use proprietary software data, searches in other library databases, consortium information, or peer groups established by their universities. Consortium conspectus projects in particular used comparisons to look at the metacollection of a state or consortium, to see where the pockets of excellence and deficit were, and, after collection responsibilities were assigned, to check on the progress of institution’s particular collection responsibilities. Verification.—Only three respondents used verification studies as a formal procedure. Most saw conspectus work as a snapshot in time. Some exceptions were when red flags appear about the growth of an important collection or when there is a change in management or a change in library personnel. Three respondents talked about adjusting institutional values as collection priorities change. Only two mentioned redoing an entire study, but most added to and modified the original one. One respondent noted, “If you need to justify, verify.” 3. The National Title Count was created by the Collection Management and Development Section of the Association of Library Collections and Technical Services Division of the American Library Association. It is sometimes called the North American Collection Inventory Project. The National Title Count uses call number ranges to compare collections from a wide variety of libraries. Surprises The respondents were asked to list the surprises that they found as a result of their conspectus and evaluation studies. They did find surprises, and some of them were startling. Here are just a few. 1. Material in classical languages—Greek and Latin—in an Alaskan public library. 2. Most of the respondents report being surprised at the strength of their collections. However, there are some exceptions. “I was surprised at how poor a collection we have. We don’t attempt much research support. The entire library needs to be involved [in rectifying the situation].” 3. One respondent was surprised that “we could do it!” 4. Several respondents found rare books that they had not realized were lurking in the collection. 5. “How difficult LC or any classification scheme makes the use of the collection. How imprecise any classification scheme is.” 6.  “We had materials we didn’t know we had or how they got here; obsolete business textbooks, inherited collections. … We uncovered treasures and occasioned weeding.” 7.  An Icelandic literature collection in a public university library in Georgia (United States). Conclusions A careful analysis of the responses to questions in the interviews reveals decision traps that impede decision making in a conspectus project and may explain some, though not all, of the difficulties librarians face in making collection decisions. First, most studies of decision making emphasize the importance of carefully framing the question for study. Beach calls this process “diagnosis,” which he defines as “policies that decision makers use to evaluate the characteristics of decision situations prior to making decisions about what to do in these situations” [16, p. 7]. The respondents emphasized how important these policies are to framing the question. Indeed, many of the respondents’ concerns—the level of specificity at which a collection should be analyzed, whether the collection should be compared to other collections or the universe of information—are framing questions, and a major critical observation made about conspectus methodology is that its units of measurement are not incremental and that conspectus projects do not form a coherent body of knowledge. Perhaps this inconsistency of reported conspectus levels is more a problem of comparing projects with different frames than of comparing projects that address the same question. Perhaps the profession needs to agree on the questions, and then we might have more agreement on the answers, particularly when cooperative collection analysis projects are undertaken. In addition, the interpersonal, collective, group nature of our projects needs to be considered when the efficacy of conspectus methods is discussed. The respondents noted again and again the problem of confidence in conspectus decision making. As Beach again notes, “Even when we make decisions alone, we take into account the views and potential reactions of others” [16, p. 110]. Clearly, when approaching a collection assessment, librarians think of their roles in the library and with the patrons with whom they work. They also consider their reputations as selectors and librarians and as specialists in subject fields, and may tend to exaggerate the consequences that may befall their institution due to a mistake in judgment. Paul Mosher notes that librarianship is a social enterprise based in a specific culture, and that collection development is an enterprise motivated by beliefs rather than outwardly imposed structures [3]. It can be hypothesized that one reason for the hesitance, dread, and scorn with which librarians may approach the conspectus is a culture that does not have a long history of collaborative work or of freewheeling ease with ambiguity. The rules and roles of librarianship, our beliefs that override the encouragements of any manual, compel us to hesitate, collect more data, hedge, and cease to use the conspectus as it might best be used. Beach observes that decision makers tend to underestimate deontology, the influence of moral obligation and commitment on human behavior [16]. He notes that obligation, commitment, and duty are strong motivations. These influences may provide some of the basis on which librarians, whose culture is individualistic, inclusive, and nonjudgmental, have some difficulty with the value judgment aspects of critical thinking. This influence may lie behind the advice several respondents gave: to describe the collection rather than to compare. Clearly, a tenuous balance exists between the conspectus’s failings and strengths, and there is, running parallel to its successful adoption and use, what James R. Coffey characterizes as unenthusiastic, qualified acceptance [25]. Its successful adaptations in Alaska, the United Kingdom, Australia, and by public libraries indicate that conspectus work has some worth to librarians. Coffey, however, sees failure to use the tools consistently to be the reason for the lack of meaningful resource sharing initially envisioned by the RLG [25]. Ferguson says that perhaps the conspectus has provided a kind of red herring, a means for those involved not to talk about the difficult issues surrounding true resource sharing and collaborative collection management, and instead to focus on one small aspect of a larger and more difficult problem. However, those difficult issues, which include local autonomy, competition among institutions, and pressures from outside the library to provide material locally, are much more likely to be the sources of the failure of resource sharing, rather than any limitations of conspectus methodology [9]. Lack of “frame control,” as J. Edward Russo phrases it in his book on decision traps [26], is another concern. The fact that classification schemes (Dewey, LC, etc.) are not particularly helpful in evaluation of subject coverage, particularly in interdisciplinary topics; that the tools we use, though they have improved with access to computerized tools, are sometimes partial and incomplete; that looking at material on the shelf ignores material that is checked out; and that methods for studying use are inconsistent often leads librarians either to overanalyze their collections or to shoot from the hip. The respondents generally asked librarians to be sensible about what information is needed to answer the question of the worth of a collection (back to diagnosis again). As Beach expresses it, conspectors should gather “knowable facts and reasonable estimates of ‘unknowables’” [16, p. 2]. Even given all this, this study of the decision literature and analysis of the uses of the conspectus suggest some directions that may be useful for further study. Use of the lens model and inferential statistics may ease some librarians’ discomfort with the lack of numerical specificity in the collection‐level definitions, and this is an avenue for further study for researchers who wish to make the conspectus methodology more rational. Another avenue of study that may be profitable is an analysis of the application of what Beach calls naturalistic decision theory [16] to the use of the conspectus. He describes this theory as driven by “verbal models and real‐world situations” [16, p. 197]. Naturalistic theory sees the behavior of decision makers in organizations as constrained by their position in the organization and thus not in possession of the full array of options that an outsider might be able to consider. Naturalistic theory views decision makers as individuals who make decisions for the organization while operating within constraints. Certainly, the librarians in our study talked about the constraints of conspectus study—inadequate time, inadequate resources, and inadequate tools. Their solutions used real‐world techniques and encouraged sensible alternatives. Though naturalistic theories do not provide results that satisfy the analytic paradigm of decision work in which investigators “want their science to appear rigorous and orderly” [16, p. 197], study in this vein may be more appropriate to Atkinson’s “messy pursuit” [9, p. 354]. It is an interesting framework and merits further study. In Beatrice Kovacs’s extensive descriptive study of the formal components of acquisitions decisions, she discusses librarians’ preference for numeric formulae, stating, “rather than defend their allocation decisions with elaborate formulas … librarians … need to be able to defend their decisions with their understandings of all the … ambiguous realities of the college library” [27, p. 17]. Despite the agony and angst respondents to the survey expressed, they agreed that working with the conspectus has given them confidence in describing their collections; tools for requesting increased resources; solid information for collection management, particularly in preservation and in making decisions about storage; and evidence to be used in making decisions about resource allocation. Given the joys, limitations, and uses of conspectus work, this need to admit our own limitations, and the limits of our collections and the conspectus methods and tools, may paradoxically be the best way to improve our efforts to understand our own collections. To be comfortable with ambiguity may be the beginning of true wisdom.