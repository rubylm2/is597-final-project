The Impact of Multimedia Course Enhancements on Student Learning Outcomes Katherine Schilline School of Library and Information Science, Indiana University-Purdue University University Library 3 WOE, 755 West Michigan Street, Indianapolis, IN 46202. E-mail: katschil@iupui.edu An online course was redesigned to use multimedia applications to improve student learning and promote meaningful engagement among classmates and course activi- ties and materials. Consumer Health Informatics (CHI) is a masters-level course that examines Internet-based and telehealth models for delivering health information to consumers. When first offered in 2007, the delivery format was primarily textual. Fol- lowing the course redesign in 2008, the course materials, including weekly topical discussion forums, projects, and course evaluations, were compared. Qualitative content analyses and statistical comparison of quantitative data demonstrated signifi- cant improvements in the level of students' engagement in course materials and with peers. Students' attitudes and perceptions recorded in course evaluations also showed significant changes. Keywords: virtual learning, multimodal learning, student learning outcomes, content analysis, comparative data analysis project compared two sections of the same online course taught by the same instructor in 2007 and 2008, to de- termine whether the changes resulted in tangible improvements in student perfor- mance or attitude. Consumer Health In- formatics (CHI) is a masters-level, fully web-based course. Students from the In- diana University Schools of Library & Information Science, Nursing, and Infor- matics examine Internet-based and telehealth models for delivering health information to consumers. After being taught once, the CHI course was up- graded with support from an internal uni- versity course development grant. The grant sought to integrate multimedia ap- plications and multimodal instruction (text, visuals, audio, etc.) into the course to improve student learning and promote more meaningful engagement with class- mates, and with course activities and ma- terials. After delivering the course in spring 2007 and the revised course in spring 2008, the author undertook quali- tative and quantitative analyses of online course materials to determine whether student performance and attitudes im- proved between the two generations, and if so, to what extent. Multiple data sources were used, including: (1) grades on three major assignments, (2) course participation scores, (3) course evalua- tion scores, and (4) a content analysis of randomly selected weekly discussion fo- rum conversations. Together, these data provided a snapshot of students' engage- ment and course learning outcomes. Qualitative content analysis and statisti- cal comparisons of quantitative data showed significant improvements from 2007 to 2008 in the level of students' en- gagement in course materials and with peers in discussion forums. Students' at- titudes and perceptions recorded in end-of-the-semester evaluations also J. of Education for Library and Information Science, Vol. 50, No. A - Fall 2009 214 ISSN: 0748-5786 Â©2009 Association for Library and Information Science Education
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 215 showed significant improvement. These results illustrate the efficacy of multimodal learning techniques in vir- tual education. Trends in the Distance and Online Learning Literature The early literature on higher educa- tion online distance learning compared learning outcomes and learners' atti- tudes about their course experiences in traditional versus web-based or distance environments. From this literature, the no significant difference phenomenon emerged, providing evidence that tech- nology neither improved nor retarded educational quality. Largely based on the analyses of more than 350 research articles and papers published from 1928 to 1999, the no significant difference phenomenon referred to the fact that the majority of studies identified no signifi- cant differences between student learn- ing outcomes and overall course grades in distance environments versus tradi- tional classroom settings (Russell, 1999). As distance education became a main- stream instructional mode the focus shifted to the attributes of students who performed well in online environments (Rovai, 2002b, 2003). Issues and con- cerns addressed in this literature included the cost effectiveness of web-based in- struction, advantages and disadvantages of synchronous and asynchronous com- munication, evaluation methods for both students and instructors, time manage- ment issues, instructional design, faculty and student computing skills and train- ing, infrastructure and administrative support, and faculty reward systems (Carr-Chellman & Duchastel, 2000; Garrett, Lundgren, & Nantz, 2000; Kochtanek & Hein, 2000). Research has also dealt with issues such as what kinds of courses and content to mount online, how to best implement the most appropri- ate applications for the most meaningful learning experiences, how to address faculty and student adaptation to web en- vironments, and what elements of the learning experience to preserve as 'tradi- tional' (Carty, Stark, van der Zwan, & Whitsed, 1996; Edwards & Harden-Fritz, 1997; Harmon & Jones, 1999; A. Lee, 1999; M.-G. Lee, 2001; Wegner, Holloway, & Wegner, 1999). The literature recognizes that students' persistence in distance education courses and programs may be lower than in tradi- tional settings (Rovai, 2002a). In fact, at- trition rates of up to 50% have been reported in online courses. Developing a sense of community becomes increas- ingly important, for relationships exist between community and student persis- tence in these settings (Rovai & Wighting, 2005). Without a sense of community, students may feel isolated, lonely, and separate, wondering if any- one is paying attention. An effective on- line community is characterized by sharing, support, and encouragement be- tween students, and between students and faculty (Palloff & Pratt, 2001). Commu- nity also involves shared learning goals, and mutual duties and obligations. It fos- ters feelings of belonging, wellbeing, and connections as part of a defined group (Rovai & Wighting, 2005). As reflected by the latest, smaller body of literature on multimodal instruction and learning in online environments, stu- dent-centered learning maximizes course engagement and is vital for student reten- tion, positive learning outcomes, and sat- isfaction (Bernard et al., 2004; Greene, Dillon, & Crynes, 2003; Palloff & Pratt, 2007). Research has also illustrated the ef- ficacy of multimedia instruction in online environments, showing that meaningful learning increases when visual and ver- bal/visual materials are presented simulta- neously (Doering, Beach, & O'Brien, 2007; Gunel, Hand, & Gunduz, 2006; Lindsay, 2004; Tempelman-Kluit, 2006). When compared with text-only instruc- tion, for example, multimodal instruction
216 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE including text and pictures, has been found to be a more powerful learning tool in terms of learner outcomes (Choi & Johnson, 2005). Another study identified significant differences in learner motiva- tion and attention between students who viewed video-based instruction on using a complex software tool as compared to stu- dents who were provided with traditional text-based instruction. The video-based instruction led to better performance than unimodal instruction. The multimodal training exercise led to a stronger mental model of the computer program and im- proved identification of window elements and objects. Learners also reported that the video-based instruction was more memorable (Gellevij, Meij, Jong, & Pieters, 2002). Engaging online learners in time well spent is a priority of effective online courses. Appropriate online teaching techniques involve eliminating informa- tion not directly related to the learning task, chunking content, and using both audio and visual processing channels (Tempelman-Kluit, 2006). Interesting and motivating activities include early icebreaking exercises, peer partnerships and team activities, reflective activities, games, and simulations (Bernard et al., 2004; Palloff & Pratt, 2007). Although discussion boards and electronic mail eliminate nonverbal cues like facial ex- pressions and voice inflections, these continue to be powerful tools for group communication and cooperative learn- ing, as are synchronous electronic meet- ing spaces such as video conferencing and chat rooms (Bunn, 2004; Conrad & Donaldson, 2004). Multimodal Instructional Development In 2007, the author was awarded a uni- versity-sponsored 'Jump Start' grant to work with the Office of Professional De- velopment Center for Teaching and Learning (CTL) to redesign a Consumer Health Informatics (CHI) course that had been developed in 2006, and taught for the first time in spring 2007. CHI is open to masters students in health sci- ences librarianship, health informatics or nursing informatics, and is cross- posted through the Schools of Library & Information Science, Nursing, and In- formatics. The course typically enrolls geographically dispersed students, li- brarians, and healthcare professionals. CHI focuses on Internet-based and telehealth models for delivering healthcare and health information to the public. Topics include the design, devel- opment, and usability of electronic con- sumer health information resources; consumer access to clinical information and current research; health literacy and health information literacy; electronic personal health records; privacy and con- fidentiality issues in e-health; and others. The course is grounded in learning pedagogies that combine multiple ele- ments for effective learning. Course pro- jects allow students to compare and evaluate consumer health information technologies, including web-based deliv- ery, social networking sites, e-mail, tele- phone, and virtual spaces. Students also evaluate the quality of web-based con- sumer health information and recom- mend strategies for ensuring information quality, examine the impact of technol- ogy-based information on consumer healthcare decision-making, and analyze the changing relationships between healthcare consumers and healthcare professionals as a result of available consumer information technologies. After being delivered online in 2007, significant redesigns were planned for the 2008 course. For example, the 2007 course relied solely on text, requiring an unusually large amount of reading. Feed- back indicated that some students, partic- ularly those who were fairly new to online learning, experienced problems navigating through materials in the course web site.
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 217 The instructor proposed to integrate relevant multimedia tools and resources to: (1) improve the instructor's ability to effectively address different learning styles, (2) provide multiple paths for communication and interaction among geographically dispersed participants, and (3) increase opportunities for stu- dents to interact meaningfully with con- tent delivered in a variety of formats. Specific redesign activities included cre- ating a logo and graphical interface, or course 'wrap,' that streamlined access to all materials and provided a professional and appealing 'look' or identity. Course content, structure, and assignments were not changed in any way through the redesign process. In order to reduce the amount of read- ing, and to appeal to multiple learning styles, content was developed in non-text formats. When available, existing online learning products, including select YouTube presentations, were used to il- lustrate key points. Adobe Presenter's PowerPoint plug-in was used to narrate presentations of introductory materials, including expectations and syllabus, each of the assignments, and several weekly topical mini-lectures. An online tour of the course's organization and structure within Oncourse was recorded using Captivate. The course had been or- ganized into weekly topical categories previously, but was further restructured into broad content modules that pulled together those weeks during which similar content was studied. Methodology This research was designed to deter- mine whether and to what extent multime- dia and structural enhancements to a Consumer Health Informatics course re- sulted in tangible differences in student performance and attitude. A comparison of 2007 and 2008 courses was conducted, with all data gathered from the two se- quential sections of the course (2007 n = 21; 2008 n = 28). Quantitative data were derived from multiple sources including three major course assignments, cumula- tive discussion forum participation scores, and end-of-the semester course evaluations scores. Student learning was evaluated through the three major course assignments: (1) a formal, detailed and systematic evaluation of three web-based consumer health information resources (200 pts), (2) a 6-8 page written research paper on important issues in consumer health informatics (200 pts), and (3) a small team project through which students developed mock-ups of products, re- sources, or applications for the delivery of consumer health information (300 pts). Students' perceptions and feedback about the course were gathered at the end of the semester using the School's stan- dardized course evaluation forms that were sent to each student via postal mail and tallied by administrative staff. Course evaluations from the 2007 section were compared to those from 2008. Re- sponses were indicated on a 1 to 5 Likert scale, with 1 = highly effective/excellent, and 5 = very ineffective/very inadequate. The course evaluation questions assessed instructor preparedness, knowledge of subject matter, time management skills, and other qualities and characteristics. For the purposes of statistical analysis, a variety of simple comparative tests in- cluding ANOVA were performed using SPSS. An alpha level of P < 0.01 was used as the criterion of statistical signifi- cance for comparative tests. Content Analysis of Weekly Discussion Forums Each week, students participated in a discussion forum discussing readings and activities, and exploring important issues. The goal was to generate mean- ingful dialog among classmates; there- fore, students were encouraged to post comments early in the week, and respond to others' posts, sharing ideas, percep-
218 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE tions, and experiences. Participation was recorded weekly. Grades reflected the extent to which students participated meaningfully by contributing new ideas, drawing-out important issues and con- cepts, reframing current perspectives, or advancing the conversation. A qualitative comparison of the discus- sion forums was conducted for weeks 2, 4, 6, 8, 10, and 12. These were randomly selected from among the 13 individual weekly forums to be representative of the overall course. Content analysis was con- ducted to compare the forums for each generation of the course based on stu- dents' online conversations around im- portant course topics. Discussion topics were derived from the course goals and weekly learning outcomes. In 2007, all students (n = 21) partici- pated in one large forum throughout the semester. In 2008 (n = 28), knowing that a forum of 28 participants had the poten- tial to generate unmanageable numbers of messages each week, the class was subdivided into four separate discussion groups of seven to nine students each. For the purposes of analysis, the 2007 forum was compared independently to each of the four 2008 forums. A codebook was developed based on the identification of key terms and con- cepts from the course's major learning goals and the weekly learning objectives from the selected discussion forums. This resulted in an analysis of several major conceptual themes within each week. The instructor and a reference librarian independently coded the discussion fo- rums for specific sets of words and phrases. Inter-rater reliability was as- sessed on three sample coding run-throughs. The resulting correlation coefficients ranged from 0.856 to 0.907. This showed a good relationship. Coding was based on the frequency of relevant words and phrases appearing in any of the forums' messages. The level of generalization of key words and phrases was fairly broad to take into account dif- ferences in syntax and writing style. More than 250 potential key words, phrases, and synonyms were initially re- corded in the codebook. Coders color-coded discussion forum text based on the associated learning objective. Uncoded text was rescanned for recur- ring themes that had not been identified earlier, and other potentially relevant terms, phrases, and concepts were gener- ated for addition to the codebook. Results Data from multiple sources were ana- lyzed to determine whether and to what extent the revised, media-enhanced course resulted in measurable improve- ments in student performance or attitudes. Data were drawn from three major course assignments, discussion forum participa- tion grades, end-of-the-semester course evaluations, and content analysis of six randomly selected weekly discussion fo- rums. Results from course evaluations and discussion forum participation scores dis- played significant improvements from the 2007 to the 2008 course. Data related to the demographic com- position of the two course sections were compared. There were no statistically significant differences (P > 0.01) be- tween the groups for demographic vari- ables including age, gender, race or ethnicity. However, the 2008 enrollment was significantly larger than in 2007 (28 students in 2008 vs. 21 in 2007). Quantitative data were collected. To identify any statistically significant dif- ferences in student achievement in the two course sections, scores from the three major course assignments were com- pared. Table 1 shows average scores for each assignment in each section, and the results of statistical analyses. These re- sults failed to achieve statistical signifi- cance in two of three cases, with only the research paper (P = 0.016) showing sta- tistically significant higher grades in 2008. Overall, these findings showed that
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 219 the enhancements made did not result in students systematically achieving higher grades on assignments. Standardized course evaluations were completed by students in both sections of the course. Questions were coded for sta- tistical analysis with the lowest rating as 5 (ineffective/inadequate), and the highest rating coded as 1 (highly effective/excel- lent). The mean course evaluation score for the 2007 course (n = 1 8) was 2.40. The mean for the 2008 course (n = 18) was 1 .40. Statistical assessment demonstrated a significant difference (P = 0.000) be- tween students' attitudes. Improvements were seen in the 2008 evaluations. These findings are illustrated in Table 2. Participation was also assessed both quantitatively and qualitatively. Each week, students' participation in the weekly discussion forum was scored; at the end of the semester, these weekly scores were translated into a final discus- sion forum participation grade. The mean overall course participation score was 177 out of a possible 200 points in 2007 (n = 20), and 1 83 in 2008 (n = 27). When these participation scores were com- pared, statistically significant changes were seen from 2007 to 2008 (P = 0.006). These findings showed that enhance- ments to the course improved the level of student engagement in course materials with peers around important course topics in the weekly discussion forums. Content analysis was used to determine how many messages each student posted each week, with results showing that the 2008 course participants were more ac- tively engaged, in terms of quality of posts to the forums. There was an average of 133 communications/messages each week in the 2007 section (M = 7.39 mes- sages per student, per week), and an aver- age of 145 messages each week in 2008 (M = 5.18 messages per student, per week). The range among students' level of participation was considerable, with some students consistently posting doz- ens of messages each week, while others posted only two or three. Time-on-task, including how much time students spent in the forum each week, was not calcu- lated since the 2007 version of the course management system did not make these data available. The content analysis also addressed the broader issues of whether 2008 section students were more or less likely to: â¢ Interject inappropriate comments or take the conversation off-topic; â¢ Appropriately advance the conversation with (a) framing questions, (b) supplemental literature or materials, or (c) contradictory opinions or facts. Table 1 : Course Grades Comparisons. Possible 5^ Data Source Points 2007 2008 P Value Consumer Health Application 200 pts n = 21 n = 29 0.059 Critical Analysis 185 189 Team Informatics Project 300 pts n = 21 n = 29 0.077 276 281 Written Research Paper 200 pts n = 1 9 n = 27 * 0.01 6 175 181 statistically significant Discussion Forum Participation 200 pts n = 20 n = 27 * 0.006 177 183 statistically significant
220 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE Table 2: Course Evaluation Questions and Results. Mean in Mean in 2007 2008 Question (n = 18) (n = 18) 1 . At the beginning of the semester I was told what was 2.00 1 .14 expected of me in this course (i.e., assignments, texts, papers, projects, etc.) and the criteria for grading. 2. The objectives of the course were clearly stated by the 2.50 1.14 instructor. 3. The course content was related to the objectives as 2.50 1 .14 established by the instructor. 4. Teaching method(s) were well suited to the course. 2.50 1 .57 5. The exams, tests, and quizzes were relevant to the course 2.00 1 .71 objectives. 6. The instructor utilized class time effectively. 2.50 1 .71 7. The material was presented in a well-organized fashion. 2.50 1 .29 8. The amount of material covered in the course was 3.00 1 .71 reasonable. 9. The assignments, including readings, exercises, etc., were 2.50 1 .43 useful and helpful in learning the subject matter. 10. The projects/papers) were a good learning experience. 2.50 1 .43 1 1 . The instructor appeared sensitive to attitudes and reaction 2.50 1 .57 of the class. 1 2. The instructor made students feel free to ask questions, 2.00 1.14 disagree, express their ideas, etc. 1 3. I felt personally respected as an individual in the class. 2.00 1 .00 14. The instructor was available for consultation. 3.00 1 .29 1 5. The instructor was helpful when students had difficulty 2.00 1.14 with course work. 1 6. The instructor was able to clarify the subject matter. 2.00 1 .43 1 7. The instructor stimulated thinking and encouraged further 2.50 1 .14 independent study. 1 8. It is assumed that, for a graduate course, a student will 2.00 2.29 spend a minimum of three hours in preparation for every hour in class. Taking the expectation into account, how appropriate was the amount of work in this course for the credit received? Forum analyses showed that 2008 stu- dents were more focused on the task at-hand than were their 2007 counter- parts. Coders found 43 examples of in- appropriate interjections into the conversation in the six weekly forums from the 2007 course, including mes- sages that should have been private, ran- dom off-topic comments, or random off- topic complaints. As part of the rede-
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 221 sign, the instructor added detailed in- structions warning students against posting inappropriate interjections into the discussion forums. These details were provided both as part of the written instructions, and in multimedia format as a web-based audio-narrated PowerPoint demo (Adobe Presenter PowerPoint plug-in). As a result, in 2008 only 16 instances of inappropriate interjections were identified in the fo- rums analyzed. Students were expected to engage meaningfully in the discussion forums by generating a dialog: sharing their ideas and thoughts, drawing-out important is- sues and concepts, reframing current per- spectives, or otherwise advancing the conversation. In the 2008 section, stu- dents were four times as likely to advance the forum discussions by posing framing questions (2008 weekly M = 12) than they were in 2007 (weekly M = 3). Stu- dents were also more than twice as likely to advance the discussion by pointing out, describing, or recommending sup- plemental materials, readings, and useful URLs (2008 weekly M = 7; 2007 weekly M = 2). Interestingly, 2008 section partic- ipants were not necessarily more likely to advance the discussion by presenting a dissenting or contradictory opinion or fact (2008 weekly M = 1 ; 2007 weekly M = 2). Finally, content analysis of the six ran- domly selected weekly discussion fo- rums (weeks 2, 4, 6, 8, 10, 12) also showed that 2008 students appeared to be more likely to generate meaningful dia- log around important themes. The num- bers in Table 3 represent 'hits' - or the numbers of times that individual students discussed the concepts drawn from the course goals and weekly learning objec- tives. Higher numbers of hits were not al- ways desirable. For example, the social networking scores (I on Table 3) in the 2007 course section were largely attrib- uted to the fact that many students spoke very personally and in length about their relatives' illnesses and experiences, rather than talking about the substantive issues at-hand. The coders observed that in 2008, students were more likely to speak appropriately about weekly topics, so there were fewer cases in which stu- dents personalized the content and spoke only in terms of their own personal range of experiences. This was seen again with F (Table 3), which addressed changing healthcare relationships. In the 2007 course section, coders agreed, the major- ity of discussion was highly personal- ized, with students talking mostly about their own relatives' healthcare relation- ships; in 2008, the conversation appeared to be more substantive. For letter L in Ta- ble 3, the coders weeded out obvious cases in which students used terms inap- propriately. However, additional analy- sis is necessary in order to better understand and distinguish between ap- propriate or factually correct use of terms and concepts versus inappropriate or fac- tually incorrect usage; and to determine how students' usage of important terminology and concepts progressed over the semester. Discussion Although results were mixed, data analyses illustrated that the revised, 2008 course was statistically significantly im- proved in two important areas: student participation and course evaluation scores. Participation in the 2008 discus- sion forums was notably more substan- tive, as assessed through students' use of terms and phrases that represented their engagement with course goals and weekly learning objectives. Students in the 2008 course section also posted more frequently to the discussion forums, and achieved higher overall participation scores (P = 0.006). Additionally, the semester's end course evaluation scores were markedly higher for the 2008 section (P = 0.000). In 2008,
222 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE Table 3: Discussion Forum Content Analysis Results. Content Analysis: Area from which Terms, Phrases 2007 # of 2008 # of were Drawn Hits Hits A. Identify technology-related challenges (course goal and 42 67 week 1 0) B. Identify trends or future developments, and resulting 52 74 challenges of CHI (course goal) C. Roles of students' professions in supporting or driving CHI 24 37 (course goal, and weeks 2 and 6) D. Discuss the roles of private, regional, state or national 1 8 29 organizations or government agencies in CHI (course goal and weeks 2, 4, 12) E. Discuss information security-related issues (technical 19 37 perspective) (course goal and week 12) F. Discuss changing relationships between patients-patients, 147 206 patients-providers, providers-providers, and other changing relationships; subsequent impact on decision-making (course goal) G. Evaluate the quality of the information delivered on the 49 84 Internet; recommend strategies for information quality in relation to usability, creation, design (course goal, and weeks 10 and 8) H. Identify, compare or evaluate available CHI applications 26 49 or technologies (course goal) I. Discuss the pros and cons of health-related social 68 67 networking; share examples of social networking opportunities (course goal and week 6) J. Discuss, describe, evaluate user-centered design 16 41 (throughout and week 8) K. Identify privacy and confidentiality issues (social 111 129 perspective) (course goal and week 12) L. Make use of specific terminology including telehealth, 257 349 telemedicine, telenursing, health informatics, health policy, social networking, usability, electronic medical record, personal health record, provider, usability, digital divide, health literacy, health information literacy, and others . . . the course was rated higher on all evalua- tion questions except question #1 8, which asked whether the workload was light, av- erage, or heavy. In 2008, participants indi- cated that the load was heavier, which was not surprising since additional non-print materials had been added. However, 35% of the readings were cut in order to com- pensate for the extra workload anticipated by having to read or use materials and as-
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 223 signment instructions in multiple formats. Although the instructor originally be- lieved that content could be duplicated in print and multimedia formats so that stu- dents could work with both formats at once, the literature recommends that du- plication be eliminated in order to de- crease unnecessary cognitive processing (Tempelman-Kluit, 2006). Areas that did not show statistically significant changes after the redesign in- cluded students' grades on two of the three major assignments, and final course grades (P = 0.066). These results showed that, although 2008 section students did achieve higher scores on the research pa- per assignment (P = 0.016), this was not the case for all assignments. Therefore, multimedia enhancements to assignment instructions neither improved nor re- tarded students' performances on graded activities. Limitations There were several limitations to this research. The number of participants was small (N< 30), so results cannot be gener- alized. There were also more students in 2008 (n = 28) than in 2007 (/i = 21). Fur- thermore, levels of interpretation and generalizability of the content analysis are limited. Because a variety of media enhancements were made to the course at one time, there is no way of knowing ex- actly which specific change or new media application was responsible for which specific course improvement. It is impor- tant to note that course instructions, goals and objectives, and assignments did not change at all, but of course the instructor may have been more comfortable teach- ing the course the second time. Additionally, many variables can af- fect the evaluation of a training program or course, some of which have little to do with actual content, and may interfere with students' abilities to objectively evaluate the course or their own learning. Although some of these issues are elimi- nated in an online environment, potential confounding factors could include the students' or instructors' ages, gender, race or ethnicity, and other issues such as the instructor's sense of humor or tone. Confounding factors that specifically im- pact student evaluations of online courses, include the visual appeal and us- ability of multimedia applications, or whether the course has an attractive and friendly online personality (Centra & Gaubatz, 2000; Cramer & Alexitch, 2000; Shevlin, Banyard, Davies, & Griffiths, 2000). Considerations for Future Research Online teaching and learning require some unique skills and considerations, and may not be appropriate for all faculty and students. Understanding and making accommodations for the required time commitments, technical skills, and self-motivation are vital (Palloff & Pratt, 2001). Integrating multimedia enhance- ments into a course involves a set of po- tential drawbacks including start-up and long-term maintenance time, costs, and customization issues. The costs associ- ated with designing effective multimedia course applications are not insignificant, especially when the assistance of graphic designers or instructional designers is needed. Another important consideration is the fact that effective electronic learn- ing products require consistent monitor- ing and maintenance, and function best if customized to the precise needs of spe- cific user audiences. This often necessi- tates a commitment to long-term and regular upkeep, involving considerable time and expertise. In this course, the in- structor personalized some of the au- dio-narrated PowerPoint presentations to the current semester, but in the future, all presentations will be generalized so that they can be used across multiple semesters. Research findings can have significant
224 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE ramifications for administrative and fi- nancial issues surrounding the creation and maintenance of multimedia learning products. Further research is warranted to continue examining the correlation be- tween students' attitudes about elec- tronic learning products and their actual academic achievement. Investigating the effectiveness of two or more products claiming to teach the same subject con- tent, or of second-, third-, or fourth-gen- eration tools is also useful for assessing whether later-generation learning re- sources actually improve learning. Those who invest their time, energy, and money creating and upgrading online learning products believe that later-generation tools are more effective. As instructors continue to use multimodal learning tools to promote educational experiences that are active and interactive, stu- dent-driven, and student-centered, the potential to deliver instruction using next-generation technologies such as vir- tual reality or other interactive media should be seriously investigated. Re- search provides information that instruc- tors can use to examine the potential benefits and drawbacks of, and selectively implement, various formats according to their efficacy in particular situations or circumstances. Conclusion This study is useful for identifying spe- cific areas of the consumer health infor- matics course that were and were not impacted as a result of multimedia and structural enhancements to the 2008 course. Findings also provide informa- tion that the instructor can use to continue improving this and other courses. For ex- ample, the instructor is currently review- ing data from course assignment grades to reexamine more closely exactly what specific skills sets, course goals, and ob- jectives each assignment should emphasize. Regardless of which media or methods are used to deliver course content, the goal remains for instructors to develop relevant activities that engage students in the course, and with one another, in an ac- tive learning community. Ultimately, ed- ucators will continue to take advantage of the wide variety of multimedia learning tools and online course options to im- prove the quality of the educational expe- rience, striving to prepare students for rigorous critical thinking, problem-solv- ing, and lifelong learning in their professional careers. Acknowledgements The author would like to thank Erich Bauer and Lynn Ward of the IUPUI Cen- ter for Teaching and Learning for their participation in the course redesign, and the anonymous reviewers for their comments. References Bernard, R. M., Abrami, P. C, Lou, Y., Borokhovski, E., Wade, A., Wozney, L., et al. (2004). How does distance education compare with classroom instruction? A meta-analysis of the empirical literature. Review of Educational Research, 740), 379^39. Bunn, J. (2004). Student persistence in a LIS dis- tance education program. Australian Academic Research Libraries, 350), 253-269. Carr-Chellman, A., & Duchastel, P. (2000). The ideal online course. British Journal of Educa- tional Technology, 31OX 229-241. Carty, J., Stark, I., van der Zwan, R. M., & Whitsed, N. (1996). Towards a strategy for supporting dis- tance-learning students through networked access to information: Issues and challenges in preparing to support the doctorate in education. Education for Information, 14(4), 305-316. Centra, J. A., & Gaubatz, N. B. (2000). Is there gen- der bias in student evaluations of teaching? Jour- nal of Higher Education, 7/(1), 17-33. Choi, H. J., & Johnson, S. D. (2005). The effect of context-based video instruction on learning and motivation in online courses. American Journal of Distance Education, 79(4), 215-227. Conrad, R.-M, & Donaldson, J. A. (2004). Engag- ing the Online Learner. San Francisco: Jossey-Bass.
The Impact of Multimedia Course Enhancements on Student Learning Outcomes 225 Cramer, K. M., & Alexitch, L. R. (2000). Student evaluations of college professors: Identifying sources of bias. Canadian Journal of Higher Edu- cation, 20(2), 143-164. Doering, A., Beach, R., & O'Brien, D. (2007). Infus- ing multimodal tools and digital literacies into an English education program preview. English Edu- cation, 40(') ,41-60. Edwards, C, & Harden-Fritz, J. (1997). Evaluation of three educational online delivery approaches. Proceedings of the Second Annual M id- So ut h In- structional Technology Conference, USA. Re- trieved September 2, 2008, from http://www. mtsu.edu/~itconf/proceed97/online_3.html. Garrett, N. A., Lundgren, T. D., & Nantz, K. S. (2000). Faculty course use of the Internet. Journal of Computer Information Systems, 4/(1), 79-83. Gellevij, M., Meij, H. v. d., Jong, T. d., & Pieters, J. (2002). Multimodal versus unimodal instruction in a complex learning context. Journal of Experi- mental Education, 70(3), 215-239. Greene, B. A., Dillon, C, & Crynes, B. (2003). Dis- tributive learning in introductory chemical engi- neering: University students' learning, motivation, and attitudes using a CD-ROM. Jour- nal of Educational Computing Research, 29(2), 189-207. Gunel, M., Hand, B., & Gunduz, S. (2006). Compar- ing student understanding of Quantum Physics when embedding multimodal representations into two different writing formats: Presentation format versus summary report format preview. Science Education 90(6). 1092-1 112. Harmon, S. W., & Jones, M. G. (1 999). Planning and implementing web-based instruction: Tools for decision analysis. Proceedings of Selected Re- search and Development Papers Presented at the Second National Convention of the Association for Educational Communications and Technol- ogy, USA, 423-427. Kochtanek, T. R., & Hein, K. K. (2000). Creating and nurturing distributed asynchronous learning environments. Online Information Review, 24(4), 280-293. Lee, A. (1999). Delivering library services at a dis- tance: A case study at the University of Washing- ton. Journal of Library Services for Distance Education, 2(1). Lee, M.-G. (2001). Profiling students' adaptation styles in web-based learning. Computers & Edu- cation, 36(2), 121-132. Lindsay, E. B. (2004). Distance teaching: Compar- ing two online information literacy courses. Journal of Academic Librarianship, 30(6), 482-487. Palloff, R. M., & Pratt, K. (2001). Lessons from the cyberspace classroom: The realities of online teaching. San Francisco: Jossey-Bass. Palloff, R. M., & Pratt, K. (2007). Building online learning communities: Effective strategies for the virtual classroom. San Francisco: Jossey-Bass. Rovai, A. P. (2002a). Building sense of community at a distance. The International Review of Re- search in Open and Distance Learning, 3('). Re- trieved August 26, 2009, from http://www.irrodl. org/index.php/irrodl/article/view/79/l 52 Ac- cessed 08-26-09. Rovai, A. P. (2002b). Sense of community, per- ceived cognitive learning, and persistence in asyn- chronous learning networks. The Internet and Higher Education, 5(4), 319-332. Rovai, A. P. (2003). A practical framework for evaluating online distance education programs. The internet and Higher Education, 6(2), 109-124. Rovai, A. P., & Wighting, M. J. (2005). Feelings of alienation and community among higher educa- tion students in a virtual classroom. The Internet and Higher Education, 8(2), 97-1 10. Russell, T. L. (1999). The no significant difference phenomenon. Raleigh, NC: North Carolina State University. Shevlin, M., Banyard, P., Davies, M., & Griffiths, M. (2000). The validity of student evaluation of teaching in higher education: Love me, love my lectures? Assessment and Evaluation in Higher Education, 25(4), 397^05. Tempelman-Kluit, N. (2006). Multimedia learning theories and online instruction. College and Re- search Libraries, 67(4), 364-369. Wegner, S. B., Holloway, K. C, & Wegner, S. K. ( 1 999). Realizing the potential of web-based instruc- tion: Lessons learned. Paper presented at the Annual Conference of the Association for Supervision and Curriculum Development, San Francisco, CA.