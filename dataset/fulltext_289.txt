Assessing ¡Schools Richard J. Cox Professor , School of Information Studies , University of Pittsburgh, Pittsburgh , PA. Email: rcox@sis.pitt. edu Eleanor Mattern, Lindsay Mattock, Raquel Rodriguez and Tonia Sutherland Doctoral students , School of Information Studies , University of Pittsburgh , Pittsburgh , PA Over the past decade, ¡Schools have emerged to educate the next generation of infor- mation professionals and scholars. Claiming to be edgy and innovative, how can and should these schools function in the spirit of assessment that now drives so much in the university? This essay, which explores how well we can assess ¡Schools, emerged from a doctoral seminar, Academic Culture and Practice, taught by Richard Cox and includ- ing four doctoral student participants and the Dean of School of Information Studies at the University of Pittsburgh, Ronald Larsen. The doctoral students, among other activi- ties, were required to work on assignments to support a self-study for the University of Pittsburgh's reaccreditation by the Middle States Association. As we proceeded through the course, we found ourselves increasingly drawn to questions about how ¡Schools, in their nascent state, can assess themselves. Four major areas - reputation, evaluating productivity in scholarly publishing, student evaluation of teaching, and student satis- faction with their academic programs - that emerged based on student interest as the seminar proceeded are discussed. Keywords: ¡Schools, LIS education, assessment, accreditation, essay Introduction Just troversy a couple in the of education decades ago, of librarians one con- Just troversy in the education of librarians and other information professionals was the loss of "library" in the name of some schools, beginning a conversation that links to the present ¡School movement. Half-a-dozen years ago the ¡School Cau- cus was formed, annual conferences start- ed, and schools that were not former LIS schools began to join. Today, the focus of discussion about LIS education resides with these ¡Schools. Ischools "address the relationship between information, technology, and people," elevating information and its management to a critical role in soci- ety (Larsen, 2010, p. 3018). While some wonder why older notions of L-Schools or LIS Schools do not fit within this defi- nition, ¡Schools have a more complicated vision. Larsen adds, "an iSchool provides the venue that enables scholars from a va- riety of contributing disciplines to lever- age their individual insights, perspectives, and interests, informed by a rich, 'trans- disciplinary' community" (p. 3021). The heart of the notion of "trans-disciplinarity" is creating new knowledge, but as Larsen points out, such collaboration is "not a natural act" and needs to be fostered delib- erately (p. 3021). Change occurs slowly in universi- ties, so how does this work for ¡Schools? Claiming to be edgy and innovative, how can and should these schools function in the spirit of assessment that now drives so much in the university? (Olson and Gru- din, 2009). This essay explores how we can assess the recent ¡Schools, emerging from a doctoral seminar, Academic Cul- ture and Practice, taught by Richard Cox and including four doctoral students. This J. of Education for Library and Information Science , Vol. 53, No. 4 - (Fall) October 2012 ISSN: 0748-5786 © 2012 Association for Library and Information Science Education 303
304 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE seminar addresses a particular problem identified by Jonathan Cole in his impor- tant study about American universities, namely their lack of attention to preparing new faculty and leaders (Cole, 2009). This course immerses students into the history and culture of higher education, the con- text for the development of LIS education, and prepares doctoral students for aca- demic careers. We did not examine all ¡Schools, but focused on ones descending from older schools educating librarians and archi- vists, representing our immediate context (namely the school at the University of Pittsburgh tracing its origins back more than a century to the training of school li- brarians). We recognize that there are now iSchools lacking this lineage, emerging from other domains such as communica- tions or computer science. Given that our school was involved in preparing a self-study for the University of Pittsburgh's reaccreditation by the Mid- dle States Association, doctoral students, among other activities, worked on assign- ments to support this self-study. As we proceeded through this course, we found ourselves increasingly drawn to ques- tions about how iSchools, in their nascent state, assess themselves. Four major ar- eas - reputation, evaluating productivity in scholarly publishing, student evaluation of teaching, and student satisfaction with their academic programs - emerged as the seminar proceeded based on student inter- est (comprehensive coverage was not the aim). Reputation Reputation has become synonymous with quality in higher education, empha- sizing the prestige of the students, faculty, programs of study, or the school itself. Of- ten the reputation of a school is reflected through the results of a ranking system, where higher ranked schools are perceived as more prestigious or reputable. ISchools appear to have a similar reliance on rank- ings; of the 21 U.S. based iSchools, 17 post on their website the results from at least one of the available ranking systems. How do these rankings relate to the school's reputation? What qualities or characteris- tics are key to defining the reputation of an institution of higher education? In a two-year study of higher education, 26 institutions were visited and surveyed to analyze the industry of higher education, focusing on the competition for reputation and prestige (Brewer, Gates, & Golman, 202). The authors define three classes of universities: prestigious, prestige seek- ing, and reputation-based. Prestigious and prestige-seeking organizations compete in four markets: student enrollments, re- search funding, public fiscal support, and private giving. Such factors are often mea- sured in ranking systems for higher educa- tion, such as the Carnegie Classification and the U.S. News and World Report. Many methodologies have emerged for ranking higher education institutions. Some include a subjective reputational score calculated through surveying deans and other officials, while others have based their calculations exclusively on objective measures such as research expenditures. While colleges and universities attempt to discredit specific methodologies, they also legitimize the rankings through using high-ranking scores (when they receive them) in promoting their schools. The competition for higher rankings leads ad- ministrators to shape policy with the rank- ings in mind. However, ranking systems are not new, dating back to before the turn of the 20th century (Stuart, 1 995; Webster, 1986). One of these first attempts at repu- tational rankings, based on a survey of fac- ulty members at thirty-six different institu- tions, occurred in 1925 with Raymond M. Hughes' A Study of the Graduate Schools in America , making Hughes the "inventor of the reputational ranking" (Stuart, 1995, p. 237). Hughes did a second, although un- published, study in 1934. After these first attempts, very few rankings would be pub- lished for some years.
Assessing iSchools 305 Contemporary reputational rankings continue to follow the example of these early systems. The U.S. News & World Re- port survey, first published in 1983, relied solely on a reputational score until 1987 (Webster, 1992). Today, the U.S. News & World Report compares similar schools, departments, and programs, with 30% of the score based on a reputational survey. The "Best Colleges" are listed according to six different categories based on the Carnegie Classification. Graduate schools are listed according to 1 1 different disci- plines that can be further ranked according to specialty. The Carnegie Classification relies on more objective data, and classi- fies universities based on statistical data, rather than providing a list of top schools. The Top American Research Universi- ties appeared in 2000 to counter these oth- er ranking systems. Published through The Center for Measuring University Perfor- mance, this ranking attempts to provide an objective analysis of American re- search universities, relying on 9 differ- ent measures supported by data reported by universities to third parties such as the National Science Foundation (Lom- bardi et al., July 2000). Schools are then "ranked" according to how many factors fall into the top 25. Unlike other systems, the result is not a "top 10" list, but rather a comparative look at the "top" schools in the U.S. As competition in higher educa- tion is increasingly global, attempts also have been made to rank the top schools in the world, including The World Uni- versity Rankings, which ranks the top 200 universities based on 1 3 indicators in five categories (teaching, research, citations, industry income, and international mix), and the Academic Ranking of World Uni- versities, first published in 2003 by the Center for World-Class Universities and Education of Shanghai Jiao Tong Univer- sity, China. Attempting to make sense of the multi- tude of ranking methodologies, the Insti- tute for Higher Education Polity (IHEP) emerged in 1993 to "provide a road map of this complex rankings landscape," through the Ranking Systems Clearing- house. The website offers resources for both national and international ranking systems, attempting to provide an unbi- ased look at the myriad of ranking sys- tems appearing world-wide. Working in conjunction with UNESCO-European Centre for Higher Education and the Inter- national Rankings Expert Group (IREG), the IHEP has assisted in the formulation of principles for higher education rank- ings reflecting the academic literature cri- tiquing ranking systems. Rankings based on a reputational factor, such as the U.S. News and World Report, are specifically criticized for the bias of the reputational score (Webster, 1992). Other studies have assessed the quality of the "objective" sta- tistical data used by other rating method- ologies (Schmitz, 1993; Kerr, 1991). Such classification becomes a major issue when ranking iSchools. Seventeen US based iSchools report the result of at least one ranking scale on their websites. Of these, 15 list their U.S. News and World Report ranking, with little con- textual information about the nature of the ranking. As few iSchools share a common origin, with many emerging from LIS and others from computer science and tech- nology oriented programs, it is difficult to determine how the iSchools should be cat- egorized. The U.S. News is one of the few systems that report the rankings specifi- cally for LIS. The U.S. News further com- plicates the classification of LIS programs by reporting the rankings for specialties offered within LIS, including archives and preservation, digital librarianship, health librarianship, law librarianship, school library media, services for children and youth, and information systems. The Top Research Universities also fails to capture the intricacies of the ¡School movement. While iSchools may be situated within large research universities, few of the fac- tors capture data related to the programs within the iSchools. Many of the data points for this system are gathered from the
306 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE National Science Foundation. While the NSF does report data for computer science programs, library science is categorized with journalism and communications. It is not clear where information technology or telecommunications programs fall within the NSF classification. The ¡Schools represent a range of un- dergraduate and graduate degrees and cer- tificates in a variety of programs. While the schools may be united under the iS- chools banner, the differences outnumber the similarities. What factors are crucial in assessing the quality of ¡Schools? Dur- ing the discussions at the first ¡School conference, held in 2006, the "essential attributes" of ¡Schools were defined as fo- cusing on information, its connection with people and technology, multidisciplinary approaches to research, academic inde- pendence, and an active research agenda with a doctoral program (Bruce, Rich- ardson, and Eisenberg, 2006). A notable similarity between the ¡Schools is their location within research institutions grant- ing doctoral degrees, in the prestigious or prestige-seeking category where they compete in student enrollments, research funding, public fiscal support, and private giving (Brewer, et al.). Prestige is consid- ered a "rival-good" (as prestige is gained by one institution, it is lost by another) also reflecting the nature of the rankings. Since much of the rhetoric of the iSchool movement suggests cooperation and col- laboration, perhaps the iSchool movement has a unique opportunity to break away from the reputation-based rankings and consider other methods for assessing qual- ity, such as what has occurred in Austra- lian universities focusing on establishing a system of institutions with a common mission competing in the global market as a whole (Sheil, 2010; Bradley, 2008). If ¡Schools can see themselves as a net- work advancing the goals of the informa- tion professions, there is an opportunity to move beyond the competitive nature of higher education reflected in ranking schemes. Through the governance of the ¡Caucus, ¡Schools could also adopt a simi- lar networked approach, promoting the ¡Caucus standards and benchmarks for the iSchool programs. This is not to say that all of the ¡Schools should offer the same programs; rather, by embracing the inter- disciplinary nature of the ¡Schools, stan- dards of quality should be established to ensure that the network of the ¡Schools is furthering the ¡Field. By focusing less on artificially established rankings and more on the ultimate goals and objectives of the movement, schools can aspire to a qual- ity education that will prepare graduates to become ¡Professionals. As the idea of ¡Schools evolves, so should quality standards. To provide consistency for longitudinal comparison, ranking systems have remained relatively stagnant in their methodology. The ¡Cau- cus can provide a set of quality standards that can grow with the new field. Should these standards be expressed in an iS- chool's specific ranking? This question remains to be answered by the ¡Schools. Clearly, the rankings fail to capture the interdisciplinary characteristics of the iS- chools; if the primary goal of the move- ment is to be recognized, an iSchools cat- egory in the U.S. News and World Report rankings would be one way to gain that recognition. However, a continued adher- ence to the rankings systems would only perpetuate the known problems with the rankings and competition among institu- tions of higher education (Kerr, 1991). Assessments of quality are a necessity for higher education to be accountable to students, and as the iSchools exemplify, one system doesn't fit all. The myriad of ranking systems only confuses potential students and makes it difficult for admin- istrators to set quality benchmarks. Re- cently, some commentators have suggest- ed that universities should move beyond the ranking systems and provide relevant information to students, faculty, and the public directly (Parker, 2010). iSchools can embrace this opportunity to look past the traditional measures of quality, pro-
Assessing iSchools 307 viding an example for other disciplines to consider how they would rate their own programs. Moving away from reputational rankings, the question then becomes what measures are to be used? Scholarly Publishing and Research As the number of iSchools has increased over the past decade, university adminis- trators, presented with benchmarking tasks such as tenure process reviews, long-term planning, and reaccreditation have uncov- ered a need to redefine success in research and scholarship. Both faculty and doctoral students present unique challenges in de- termining the shifting research impact of iSchool scholars. Recognizing the increas- ingly collaborative and trans-disciplinary scholarship emerging from iSchools raises the question of whether there has been an impact on scholarly publication (including doctoral dissertations) that can be linked to the iSchool in any tangible fashion? Attempting to quality iSchool scholar- ship presents several issues, a result of the difficulty of defining discrete subject areas across iSchools. Publishing activity in a wide range of journals and on a variety of topics adds to the complexity of iSchool scholarship. However, when attempting to quantify the impact of iSchools on schol- arly research several traditional measures and methods can be applied. iSchool ad- ministrators can employ traditional meth- ods such as citation analysis and research impact. Post-doctoral hiring into tenure track or other desirable positions presents another way to track success in scholar- ship. iSchools have, however, changed the paradigm. While newly minted Ph.D.s from LIS programs have taken positions in American Library Association (ALA) ac- credited library schools, the LIS students in an iSchool environment have more di- verse interests spanning multiple disci- plines. Now these LIS Ph.D.s might take positions in areas such as values in design, information visualization or digital hu- manities. An exploratory study examines the scholarly impact of iSchools based on ar- ticles and reviews indexed in the Web of Knowledge under the subject "information and library science" (Bar-Ilan, 2010). The study measures the number of publications and citations, the Hirsch h-index (a quan- tifiable index based on a scholar's number of citations and most cited publications) of the set of retrieved items, the most highly cited item, the most frequently ap- pearing document type, and the journal in which the highest number of items were published by the ¡School's home institu- tion during the period 2000-2009. The limitations of this method are immediately clear - defining subject areas across an iS- chool, publishing activity in a wide range of journals and topics, and, given that the study looked at the home institution's pub- lications rather than the iSchools' publica- tions, the possibility that articles indexed under "information and library science" did not originate in the iSchool but rather in some other department. The study finds that the leading publishers were Univer- sity of Illinois (largest number) and the University of Maryland (highest rate of citations). These two ¡Schools also had the highest h-indices. A closer look at this study, however, raises more questions and concerns than those identified by the au- thor, since, for example, publication num- bers by University of Pittsburgh faculty alone were double those indicated in Bar- Ilan's study. Information scholars, like many oth- ers, have been using citation analysis as a means of measuring the impact of their work. The earliest uses of citation analysis date back to the 1920s, and it has mostly been used in the science, technology, en- gineering and mathematics (STEM) dis- ciplines. Methods of citation analysis that have long been employed include: deter- mining journal impact factor (that is, the average number of citations a journal re- ceives compared to the number of articles published); assessing a scholar's number of citations and publications; determin-
308 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE ing the Hirsch h-index of a given author's citations; identifying scholars' most fre- quently cited items and most frequently appearing document types; and identify- ing the top tier journals in which faculty and doctoral students' citations appear. The use of citation analysis in any form has limitations: variations in citing prac- tices among researchers can produce dis- torted results; every piece of literature that is used in research is not necessarily cited or, conversely, literature may be cited but not used directly; and, finally, the immea- surable effect of cultural and language influences on citation choices (Laborie and Halpern, 1976). Various studies have been done on the viability and desirabil- ity of utilizing citation analysis tools such as journal impact factor and the h-index to validate academic success. Most disci- plines agree that the notion of journal im- pact is here to stay; most also agree that it is currently the best means of citation analysis. If ¡Schools seek to continue using cita- tion analysis to assess the quantity of fac- ulty publications and doctoral dissertations at an ¡School, administrators and scholars might consider performing their citation analysis using the University of Indiana's program, Scholarometer. Scholarometer allows users to query publication informa- tion and provides a statistical analysis of citations. As a social, or crowdsourcing, application, Scholarometer requires us- ers to tag their queries using a controlled vocabulary of disciplines. Scholarometer, however, still relies on the h-index to de- liver a quantifiable result. Concerns arise when using the h-index in an ¡School envi- ronment, however, because the h-index is traditionally used for the "hard" sciences. While a "g-index" has been created, nor- malized for humanities and social sci- ences, there is still some question about its reliability. Other concerns with using the h-index - such as context - should also be noted. For example, scholars may be cited unfavorably or they may cite themselves. The h-index, regardless of whether it is de- termined by conventional methods or via a more interactive social tool like Schola- rometer, provides quantitative information only and does not speak directly to the quality of a scholar's publications. How, then, should ¡Schools proceed? A brief review of publishing trends in 28 ¡Schools shows an increase in collabor- ative works and multi-author publications. There are fewer monographs and more trans-, inter-, and multidisciplinary works. Methodologies have also shifted to a more technological focus. There has been, for example, an increase in network methods such as social network theory and actor network theory, suggesting an increased awareness of the importance of connecting and collaborating across disciplines. As with monographs, there are fewer human- istic methods being employed (historiog- raphy, for example) which are often lone endeavors. As research foci and methodol- ogies have shifted, so too have publication venues. Unlike monographs, LIS scholars have not yet seen a decrease in publishing in discipline-specific journals, but many question whether the current print journal is a sustainable model, with increasing publishing costs and a rise in online pub- lishing venues. Scholarly publishing in LIS programs (as well as Information Systems and Tele- communications programs) generally em- ploys a "siloed" approach, in which each discipline-based department acts indepen- dently. Attempting to qualify research and scholarship in the trans-disciplinary model of the iSchool is made increasingly diffi- cult because in the past ten years we have seen both the emergence of the iSchool as well as a transition to electronic formats. This means we may have pertinent elec- tronic data for the past five years, but likely not the preceding five. The limited extant literature on ¡Schools suggests a research and scholarship agenda as diverse as the ¡Schools themselves; some foreground arts education, sociology or anthropology and others emphasize Human Computer Interaction or systems design. While these
Assessing i Schools 309 disparate notions may seem to be in con- flict with one another, in an iSchool model they represent the assorted configurations of potential collaboration. Scholarship originating in ¡Schools ranges from user studies on children in narrative spaces to data mining and bioinformatics. For this reason, iSchool scholars might consider new ways of assessing the impact of pub- lications that are more in keeping with the values and goals of these diverse institu- tions. Examining the contributions of an individual over time and assigning new values to existing rubrics for areas such as collaborative scholarship and the evolving set of journals in which iSchool faculty publish could provide a clearer picture of both the impact of ¡Schools on scholarly publishing and any emerging trends in iS- chool scholarship. Another area that ¡Schools can explore is the current lack of iSchool-specific jour- nals or other publications. To date, no one has completed a dissertation that focuses on the concept of the iSchool itself. The lack of existing scholarship about ¡Schools points to a need for further study about them. The dearth of existing journals ad- dressing the trans-disciplinary nature of iSchool research and scholarship presents a unique opportunity for today's ¡Schools and the ¡Caucus. An ¡Journal, created by ¡Schools, would provide a venue for their emergent collaborative scholarship. An ¡Journal might employ both traditional print and online components, allowing for an exploration of more visual or auditory elements of scholarship, like those being developed in disciplines such as cyber- scholarship and the digital humanities. Fi- nally, the annual ¡Conference presents an ongoing opportunity for iSchool-specific scholarship. T o better address issues of measuring iS- chool impact on research and scholarship, ¡Schools may also want to improve their tracking of iSchool faculty and graduates, offering insight into how faculty scholar- ship has changed and whether or not there has been a shift in dissertation topics or the kinds of employment common to iSchool Ph.D.s. These kinds of data analyses may provide insight into whether this change in scholarship precipitated - or is the result of - the appearance of the first iSchools in the early part of the 2 1 st century. Students and the Evaluation of Teaching Student evaluation of teaching (SET) is used widely in higher education and gen- erally understood as a means of measuring the efficacy of teachers and the extent of student learning. Despite questions raised by faculty and scholars as to their validity, SET results are commonly used in various faculty reviews. Although not the sole de- termining factor in promotion and tenure, measuring teaching and learning helps in faculty evaluation. Such evaluation also falls in line with the growing demand for accountability in academia, demonstrated in part by documents like the Miller Com- mission's 2006 report, A Test of Leader- ship: Charting the Future of U.S. Higher Education. One of the most remarkable and con- troversial assertions of the Miller Com- mission's report concerns a lack of ac- countability in educating students and measuring students' achievements (such as using the Collegiate Learning Assess- ment). The commission states that the pur- pose of measuring and gathering data on, among other things, "successful educa- tion" and "student learning" is for higher education institutions to demonstrate their "contribution[s] to the public good" (U.S. Department of Education, 2006, p. 4). This type of measurement and disclosure is de- scribed as part of the need for educational institutions maintaining public trust. SET plays a role not only in informing person- nel decisions, but also as an input for a system claiming to have the public interest at heart and aiming to translate "success- ful education" into "solid evidence, com- parable across institutions, of how much students learn in colleges or whether they
31 0 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE learn more at one college than another" (U.S. Department of Education, 2006, p. 14). The implementation of SET in higher education goes back almost a century. Hermann Remmers, a professor of educa- tion and psychology, pioneered the col- lection of student ratings at Purdue Uni- versity in 1927 (Algozzine et al. , 2004). Remmers and his colleague, G.C Branden- burg, designed the Purdue Rating Scale for Instructors , presenting students with 10 qualities, including: interest in subject, sympathetic attitude toward students, fair- ness in grading, liberal and progressive attitude, presentation of subject matter, sense of proportion and humor, self-reli- ance and confidence, personal peculiari- ties, personal appearance, and stimulating intellectual curiosity (Brookover, 1940) for rating their instructors (Knudsen and Stephens, 1931). At the time, Remmers designed the rating scale for teachers to use only for voluntary self-improvement. Teaching evaluation slowly grew into a mandatory process. According to Haskell, the inclusion of SET results for the pur- poses of tenure and promotion review increased during the 1960s, but was still largely voluntary as noted by Centra in his review of the development of SET re- search (Centra, 1993). However, a 1993 survey suggests that nearly 90% of US campuses required teaching evaluations (Trout, 2000). The reasons for this change from voluntary to mandatory are various, but can be related to the continuing devel- opment of tenure and the desire to quan- tify learning for accountability purposes. Yet tenure existed for decades without the formal inclusion of student evaluations. The development of academic tenure in the U.S. can be traced back to 1915 with the establishment of the American Asso- ciation of University Professors (AAUP), although tenure predates this in America's oldest universities. Academic freedom was the central is- sue in arguing for tenure. By the early twentieth century the ideals of academic freedom and tenure were converging in the hiring practices of institutions (Cameron, 2010). In 1915 the AAUP outlined regu- lations and principles for the formal ap- pointment of tenured faculty and the right to academic freedom in their report, Dec- laration of Principles on Academic Free- dom and Academic Tenure. Tenure and academic freedom were at the forefront of the American Association of Univer- sity Professors' (AAUP) concerns due to cases of faculty being unjustly dismissed, but as the AAUP report explains, clar- ity and understanding of these issues was for the benefit of faculty and universities alike, protecting institutions' reputations and potential societal influence (Ameri- can Association of University Professors, 1915). With more faculty and administra- tions buying into the benefits of tenure and the AAUP continuing to argue for its implementation, academic tenure became pervasive throughout the US by the 1960s, coinciding with mandatory SET (Metzger, 1973). The overlap between the rise of academ- ic tenure and mandatory student evalua- tions is significant, with the links between them easily taken for granted. Assuming that SET is a requisite piece of the tenure process rests in the belief that the practice of SET is fair, accurate, and reliable and does not impinge on academic freedom. Perhaps the most basic assumption in this scenario is that classroom teaching can be quantified in such a way that a number or rating can describe to what extent a teach- er qualifies as "good," "effective," "bad," or "ineffective," or any other vaguely pre- scribed measure that lies in between. These assumptions have been ques- tioned by researchers. Educators them- selves do not agree on what defines the concept of "good " or "effective" teach- ing (Trout, 2000). Are the parameters for good teaching universal across the higher education landscape? Critics point to the difficulty in answering these questions in the affirmative as a significant reason to stop administering SET as a part of tenure
A ssessing iSchools 311 review or considering it as a minor fac- tor. Even if the definition of good teach- ing could be clarified and agreed on, the task of creating an instrument that allows students to evaluate accurately teaching in light of that definition is no small undertak- ing. Marsh and Roche (1997) believe that teaching is a multidimensional activity and that SET instruments should accurately re- flect this characteristic, but downplaying the assessment of a faculty member's rat- ing as good or bad or anything that could be used to determine promotion or tenure. Others criticize SET methods. Paul Trout, an English faculty member, even ques- tions their ultimate purpose (Trout, 2000; Clayson, 2009). The issues around SET can be viewed as an opportunity for reimagining and im- provement, especially for iSchools, in the definition and practice of student evalua- tions. A significant part of charting the fu- ture of SET has to do with determining its purpose; is it a tool for voluntary self-im- provement, or a mandatory exercise that will impact universities in making person- nel and budget decisions? This is precisely where iSchools could enter the discussion. In a sense, iSchools define themselves by the boundaries they cross and blur. Their purpose is characterized through engaging and exploring the connections between in- formation, people and technology, rather than prescribed by a particular discipline (Olson and Grudin, 2009). Given the in- terdisciplinary nature of iSchools, there is potential for innovation, since SET can be understood as falling within the inter- sections of information, technology, and people. These three areas are not only sub- stantial parts of what SET is and how it is implemented, but they are also points of entry for iSchools, suggesting reimagining student evaluations on two levels, defini- tion and practice. Rather than search for the global definition of "good" teacher, teaching evaluation could be defined in re- lation to the goals for a class. Another op- tion, as Marsh and Roche (1997) suggest, is that SET could be designed to build off the many dimensions that are brought together in the act of teaching. Already aware of and familiar with multiple disci- plines, iSchools could observe instructors or survey institutions to determine what dimensions are appropriate for particular disciplines. For SET to become a fully supported and understood tool, a new direction is needed. It is possible that through its pro- cess of development, the ¡School demon- strates some potential innovative ways of thinking about how SET is defined and imagined. Rather than search for a single definition of "good" teaching and attempt to measure it, the purpose of SET can be explored and expanded. The ¡School ex- perience is showing that blurring the line between expected boundaries and posing new questions is not necessarily a bad thing. In terms of SET, this could lead to more useful measurements and genuine results. Student Satisfaction As iSchools mature, they would be prudent to ask whether their students are satisfied with this trans-disciplinary model of information science education. Today, American universities and colleges are accountable to accreditation agencies, boards of trustees and visitors, parents, and students. Student satisfaction assess- ments act as one measure that schools may use to demonstrate performance and ef- fectiveness to these stakeholders (Bryant, 2006). An interest in the assessment of student satisfaction emerged in the mid-20th cen- tury, as evidenced by Lora Robinson and Richard Seligman's 1968 efforts to devel- op a "student morale" measurement tool. Robinson and Seligman (1968) observe that, prior to their work, there had been little attention given to evaluating student morale on campus. In carrying out their own study, they employed the College and University Environment Scales (CUES), a tool created by C. Robert Pace of UCLA
312 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE in the 1960s to assess campus climate. As CUES was not designed just for mea- suring morale, the researchers identified the "morale-relevant items," including whether faculty show an interest in stu- dents, whether the quality of teaching is perceived as high, whether the expression of ideas is encouraged, and whether older students demonstrate a concern for new students (Robinson and Seligman, 1968). Drawing upon these, the authors develop what they refer to as a morale scale. Pace, whose metric served as a basis for the mo- rale scale, later developed the College Stu- dent Experiences Questionnaire, another standardized instrument including mea- sures to assess student morale or student satisfaction (Upcraft and Schuh, 1996). Today, higher education administrators gauge student satisfaction through a va- riety of tools. Upcraft and Schuh (1996) characterize some of these assessment measures as static, indicating that higher education administrators may gain insight about the student satisfaction climate on campus through information that does not involve "undertaking a complex research study" (Upcraft and Schuh, 1996, p. 1 54 - 1 55, 151). Sources may include anything from student newspaper articles to student retention rates. Administrators can build upon what they learn through their static measures by engaging in "active means" of assessment (Upcraft and Schuh, 1996, p. 157). These active measures may be qualitative, such as structured interviews with students, or quantitative, such as sur- veys designed to gauge student satisfac- tion with courses, programs, and general campus experiences. Colleges and univer- sities have the option of purchasing satis- faction surveys from companies such as Noel-Levitz (Bryant, 2006) or administer- ing surveys that are developed in-house. Student satisfaction is "the result of a complex set of factors" (Appleton-Knapp and Krentler, 2006, p. 254). Quality of in- struction, interaction with professors and colleagues, and class size are just a few of the determinants identified in the litera- ture. While there have been limited studies on the relationship between student expec- tations and student satisfaction, Appleton- Knapp and Krentler (2006) note that stud- ies outside of the educational literature support this relationship. A study in the health care field determines that an indi- vidual's satisfaction with health care has a direct relationship with the expectations that the individual holds (Murray, Kawa- bata, and Valentine, 2001). While many would argue against comparing students and consumers, literature that explores satisfaction is abundant in the business journals. Although multiple forces shape consumer satisfaction, researchers in the business field often use surveys to assess whether there is agreement between con- sumers' expectations and their opinions of the quality of experience or product re- ceived (Crisp, et al., 2009). Most everyone feels disappointment when expectations have been invalidated. Even if the experience of the individual is not outwardly negative, it is possible that an inconsistency between expectation and experience will lead to dissatisfaction. In considering the consequences of a rela- tionship between expectations and sat- isfaction, Appleton-Knapp and Krentler (2006) argue that it is important for educa- tors to understand that they have the abili- ty to inform and shape preconceptions that students possess. Others have confirmed this (Fishbein and Ajzen, 1975). Students entering an information science program will presumably form expectations about the education that they will receive based on encounters that they have with infor- mation professionals and the field prior to their enrollment. In order to be satisfied with the ¡School model, student expecta- tions should be in line with the goals of the ¡Schools and ¡Schools need to provide students with an understanding of the con- cepts behind these schools. In the library science field, education in the ¡Schools is almost unrecognizable to the training that was once the norm. Lynch (2008) discussed the evolution of pro-
Assessing i Schools 31 3 fessional library education in the United States, noting that it has its roots in "ap- prenticeship and in-service training ses- sions," a vocational and practicum-based model (p. 936-937). She notes, however, that in the 1980s the field recognized the need to "examine what the Information Age had brought" (p. 944). Some library schools transitioned to the ¡School model of interdisciplinary education. The transi- tion in educational goals was evident in the changing names of schools, with "library" schools becoming "information" schools (Olson and Grudin, 2009). What impact might this transition have on student sat- isfaction? In the case of students who plan to be librarians, some may be surprised to find a very different model in the iSchools. Given that the ¡School organization is a new development in information science education, it is likely that many students at iSchools are actually not aware of what the iSchools represent. In November 2010, the University of Pittsburgh's School of Information Sci- ences disseminated an electronic student satisfaction survey in which students' knowledge of the meaning of an ¡School was assessed. Current students in the un- dergraduate, master's, doctoral, and ad- vanced certificate programs were asked four short questions related to iSchools: Have you ever heard of the concept of an ¡School? If "Yes," from where? What is your definition of an "iSchool?" Was the fact that the School of Information Scienc- es is an iSchool a factor in your decision to attend the University of Pittsburgh? With only seven doctoral students and 1 1 under- graduate students choosing to participate in the survey, the 101 respondents were predominantly Master's degree students. The results indicate that there is not col- lective understanding of the meaning of an iSchool among the School of Information Sciences students. Of the respondents, 35 students reported that they never heard of the concept. The students who responded that they had heard of the concept cited a number of different sources for this infor- mation, including administration and fac- ulty, the School of Information Sciences website and the websites of other iSchools, literature that they read during and after the application process, and associations such as ALA and ASIST. Not all of the students who replied that they had heard of the concept of an iSchool demonstrated an actual understanding of it. Many stu- dents equated the idea of the iSchool with distance learning. One student response suggested that he or she thinks the idea of the iSchool is the same as the Web-based Information Science Education (WISE) consortium. The association students have with iSchools and web-based learning is strong and perhaps the greatest miscon- ception needing to be addressed. Some survey respondents maintain that the term is simply a label with little meaning, de- scribing it as "jargon" or an effort to be "trendy." Such students have not internal- ized the iSchool goals shaping the educa- tion they are receiving. The various defi- nitions offered by students indicate a very vague or simply inaccurate understanding about iSchools. What impact may this patchy under- standing of an iSchool have on student satisfaction? If expectations do indeed have a relationship to student satisfaction, students whose expectations are not in line with the goals of the iSchool may not be satisfied upon entering the program. With such a small number drawn to the school because of the goals it holds as an iSchool, a question arises: will these students be satisfied with an education that is formed with these goals in mind? Educators can shape student expec- tations. Appleton-Knapp and Krentler (2006) determine that one way to manage expectations is to make course or program expectations clear to students at the earli- est opportunity and to listen to their expec- tations on the first day. Clarification could be provided to the students with a weak or inaccurate understanding. The responsi- bility to remedy the lack of understanding of the meaning of the iSchool falls on the
314 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE larger ¡School community. More iSchool- specific scholarship may provide clarity to students, as may greater discussion of the iSchool concept on the individual school websites. Just as the iSchool is evolving as an idea, students' understanding of this new approach to information science edu- cation is taking shape. As ¡Schools mature and become a more visible and vocal pres- ence, students entering these programs will likely understand the iSchool goals and have expectations in line with them. If there is harmony between institutional goals and student expectations, it is likely administrators, faculty, and students alike will be satisfied with the learning commu- nity. Assessing ¡Schools While They Are in Motion We live, in our universities, in a new era of assessment, one that is rapidly chang- ing. There is another issue here, namely what might be lost in the evolution from one school, the library school, to a new one, the iSchool. The earlier school exist- ed to train librarians, and for some, espe- cially those dropping library in their name (Paris, 1988), the purpose expanded well beyond library education. This has been made more complicated by the corporate model of the university, threatening to shift the university from a public good to that of a business. The criti- cisms are familiar - universities are train- ing not educating; accountability, audit- ing, and reporting are overwhelming both faculty and administrators; productivity measures drive faculty while not necessar- ily supporting fundamental activities such as teaching and research; the university is no longer a social institution, now it is an industry; branding and marketing con- sume ever greater amounts of resources (time and money); credentials are products to sell not earn; and the priority is to get higher rankings, even if it is understood that such rankings are flawed (Tuchman, 2009). We have to address the earlier dis- cussion of assessing ¡Schools in this con- text. Although there is much work ahead in order for ¡Schools to establish themselves, there is no secret why ¡Schools appeared at this time. Olson and Grudin (2009), in their description of the iSchool movement, provide a telling comment about what is going on. Affirming that library science continues to have an important role in the new schools, they state, "they were pro- ducing librarians but failed to meet the academic standards of leading research universities" (p. 15-16). While there are exceptions to this assessment, in general this is worthy of additional discussion. Whether the research and scholarly goals of ¡Schools will chart the course for these new schools amidst constant and more de- tailed assessment and corporate-like agen- das in universities is open for speculation. Conclusion We can find evidence of healthy LIS and ¡Schools, but one might worry about how healthy any of these must be to func- tion in the corporate university that has subsumed the modern research univer- sity (Cox 2010). What might ¡Schools become? Some could argue that there is nothing new with the idea of the corpo- rate university, or the problems associated with it, since the university has always been part of the real world with financial and other infrastructure requirements. We must ask if any of the various means for assessing ¡Schools (such as those dis- cussed here), while acknowledging that we have no choice but to meet these de- mands, actually help us in formulating vi- sions and missions that nurture the iSchool to function as part of a research university and contribute to the public good by creat- ing new knowledge and educating the next generation of scholars and practitioners to be producers of knowledge. There are few faculty, for example, who would balk at the requirement to have their courses eval- uated, knowing that such evaluations can be helpful in making them better teachers
A ssessing i Schools 315 and that they need to be accountable for what they do in the classroom. However, when faculty are burdened with layer after layer of bureaucratic assessments that re- duce the time for them to keep up in their own field or to do research in it, then the natural consequence for many is to disen- gage from such endeavors and leave it to others to take on this labor. As ¡Schools strive to establish themselves as new aca- demic programs on their campuses, gener- ating new challenges for faculty identities and responsibilities, it remains to be seen whether they can meet these new require- ments along with the increasing requests, from both external accrediting agencies and their own campus administrations, for data, self-assessment, and new metrics. Academic assessment is a reality, a form of accountability that none of us can (or should want to) escape from, but the vi- sionary rhetoric that we hear from new ac- ademic programs such as iSchools suggest hope and promise for more engaging aca- demic endeavors; there is a difference, of course, between the reality and the hope. References Algozzine, B., Beattie, J., Bray, M., Flowers, C., Gretes, J., Howley, L., Mohanty, G. & Spooner, F. (2004). Student evaluation of college teaching: A practice in search of principles. College Teach- ing, 52, 134-141. American Association of University Professors, 1915 Declaration of Principles on Academic Freedom and Academic Tenure. Retrieved from http://www.aaup.org/AAUP/pubsres/policydocs/ contents/ 1 9 15.htm Appleton-Knapp, S. L. & Krentler, K. A. (2006). Measuring student expectations and their effects on satisfaction: The importance of managing stu- dent expectations. Journal of Marketing Educa- tion, 28(3), 254-264. Bar-Ilan, J. (2010). Measuring research impact: A first approximation of the achievement of the iSchools in ISI's Information and Library Sci- ence Category - An exploratory study, paper pre- sented at the annual ¡Conference, University of Illinois at Urbana-Champagne, Illinois, February 3-6. Bradley, D. (2008). Review of Australian Higher Education: Final Report. Canberra, A.C. T.: Dept. of Education, Employment and Workplace Relations. Brewer, D., Gates, S., & Goldman, C. (2002). In Pursuit of Prestige: Strategy and Competition in U.S. Higher Education. New Brunswick NJ: Transaction Publishers. Brookover, W. B. (1940). Person-Person Interaction Between Teachers and Pupils and Teaching Ef- fectiveness. The Journal of Education Research, 34(4), 277-278. Bruce, H., Richardson, D. J., & Eisenberg, M. (2006). The I-Conference: Gathering of the clans of information. Bulletin American Society for In- formation Science and Technology, 32(4), 11-12. Bryant, J. L., (2006). Assessing expectations and perceptions of the campus experience: The Noel- Levitz Student Satisfaction Inventory. New Di- rections for Community Colleges, 134, 25-35. Cameron, M. (2010). Faculty tenure in academe: The evolution, benefits and implications of an important tradition. Journal of Student Affairs, 6. Retrieved from http://steinhardt.nyu.edu/scms- Admin/media/users/lh62/CameronJoSA.pdf Centra, J. A. (1993). Reflective faculty evaluation: Enhancing teaching and determining faculty ef- fectiveness. San Francisco: Jossey-Bass Publish- ers. Clayson, D. E. (2009). Student evaluations of teach- ing: Are they related to what students learn? A meta-analysis and review of the literature. Jour- nal of Marketing Education, 3 1( 1 ), 1 6-30. Cole, J. R. (2009). The Great American Univer- sity: Its Rise to Preeminence; Its Indispensable National Role; Why It Myst Be Protected. New York: Public Affairs. Cox, R. J. (2010). The Demise of the Library School: Personal Reflections on Professional Education in the Modern Corporate University. Duluth, MN: Litwin Books. Crisp, G., Palmer, E., Turnbull, D., Nettelbeck, T., Ward, L., LeCouteur, A., Sarris, A., Strelan, P., & Schneider, L. (2009). First year student expecta- tions: Results from a university-wide student sur- vey. Journal of University Teaching and Learn- ing Practice 6(1). Retrieved from http://ro.uow. edu.au/jutlp/vol6/iss 1 /3 Fishbein, M. & Ajzen, I. (1975). Belief, Attitude, In- tention, and Behavior: An Introduction to Theory and Research. Reading, MA: Addison- Wesley Publishing Company. Haskell, R. E. (1997). Academic freedom, tenure, and student evaluation of faculty: Galloping polls in the 21st century. Education Policy Analysis Archives, 5(6). Retrieved from http://epaa.asu. edu/ojs/article/view/607
31 6 JOURNAL OF EDUCATION FOR LIBRARY AND INFORMATION SCIENCE Kerr, C. (1991). The new race to be Harvard or Berkeley or Stanford. Change, 23(3), 8-15. Knudsen, C. W. & Stephens, S. (1931). An analysis of fifty-seven devices for rating teaching. Pea- body Journal of Education, 9( 1 ), 1 5-24. Laborie, T. & Halpern, M. (1976). Citation patterns in library science dissertations, Journal of Educa- tion for Librarianship, I6( 4), 271-283. Larsen, R. L. (2010). i Schools . Encyclopedia of Library and Information Science, Third edition, Taylor and Francis, 3018-3023. Lombardi, J. V., Craig, D. D., Capaldi, E. D., & Gâter, D. S. (2000). The Top American Research Universities. The Lombardi Program on Measur- ing University Performance. Lynch, B. (2008). Library education: Its past, its present, its future. Library Trends, 56( 4), 931- 953. Marsh, H. & Roche, L. A. (1997). Making students' evaluations of teaching effectiveness effective: The critical issues of validity, bias, and utility. American Psychologist, 52(1 1), 1 187-1 197. Metzger, W. P. (1973). Academic tenure in Ameri- ca: A historical essay. Faculty Tenure, ed. Com- mission on Academic Tenure in Higher Educa- tion. San Francisco: Jossey-Bass Publishers. Murray, C., Kawabata, K. & Valentine, N. (2001). People's experience versus people's expecta- tions. Health Affairs, 20, 21-24. Olson, G. M. & Grudin, J. (2009). The information school phenomenon. Interactions, 16, 15-19. Parker, J. T. (2010). Let's make rankings that mat- ter. The Chronicle of Higher Education. Re- trieved from http://chronicle.com/article/Lets- Make-Rankings-That/1 24802/ Paris, M. (1988). Library School Closings: Four Case Studies. Metuchen, NJ: Scarecrow Press. Remmers, H. H. (1927). The Purdue Rating Scale for Instructors. Educational Administration and Supervision, 6, 399-406. Remmers, H. H. & Brandenberg, G. C. (1928). The Purdue Scale for Instructors. Lafayette, IN: La- fayette Printing Company. Robinson, L. & Seligman, R. (1968). Measurement of campus and student morale. Center for the Study of Evaluation Technical Report, 4. Univer- sity of California at Los Angeles. Sheil, T. (2010). Moving beyond university rank- ings: Developing a world class university system in Australia. Australian Universities Review, 52(1), 69-76. Schmitz, C. (1993). Assessing the validity of higher education indicators. Journal of Higher Educa- tion, 64(5), 503-521. Stuart, D. (1995). Reputational rankings: Back- ground and development. New Directions for In- stitutional Research, 88, 1 3-20. Trout, P. (2000). Flunking the test: The dismal record of student evaluations. Academe, Retrieved from, http://www.aaup.org/AAUP/pubsres/academe/ 2000/JA/Feat/trou.htm U.S. Department of Education. (2006). A Test of Leadership: Charting the Future of U.S. Higher Education, Washington, D.C. Tuchman, G. (2009). Wannabe U: Inside the Cor- porate University. Chicago: University of Chi- cago Press. Upcraft, M. L. & Schuh, J. H. (1996). Assessment in Student Affairs: A Guide for Practitioners. San Francisco: Jossey-Bass Publishers. Webster, D. S. (1984). A note on a very early aca- demic quality ranking by James McKeen Cattell. Journal of the History of the Behavioral Sciences, 20, 180-183. Webster, D. S. (1986). Academic Quality Rankings of American Colleges and Universities. Spring- field, 111: C.C. Thomas. Webster, D. S. (1992). Reputational rankings of col- leges, universities, and individual disciplines and fields of study, from their beginnings to the pres- ent, in Higher Education: Handbook of Theory and Research. New York: Agathon Press.